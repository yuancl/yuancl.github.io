<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css">


  <meta name="keywords" content="机器学习,强化学习,">








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2">






<meta name="description" content="强化学习路在何方 深度强化学习的泡沫背景 2015年，DeepMind的Volodymyr Mnih等研究员在《自然》杂志上发表论文Human-level control through deep reinforcement learning[1]，该论文提出了一个结合深度学习（DL）技术和强化学习（RL）思想的模型Deep Q-Network(DQN)，在Atari游戏平台上展示出超越人类水平的">
<meta name="keywords" content="机器学习,强化学习">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习现状与未来">
<meta property="og:url" content="http://yoursite.com/2019/02/22/rl/强化学习现状与未来/index.html">
<meta property="og:site_name" content="雷哥的博客">
<meta property="og:description" content="强化学习路在何方 深度强化学习的泡沫背景 2015年，DeepMind的Volodymyr Mnih等研究员在《自然》杂志上发表论文Human-level control through deep reinforcement learning[1]，该论文提出了一个结合深度学习（DL）技术和强化学习（RL）思想的模型Deep Q-Network(DQN)，在Atari游戏平台上展示出超越人类水平的">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2019/02/22/rl/强化学习现状与未来/resources/27517C5AFA3857CE6A330D86CB12A334.jpg">
<meta property="og:image" content="http://yoursite.com/2019/02/22/rl/强化学习现状与未来/resources/9D2F9556CB9C1DB7A2AEF9EA54261CDC.jpg">
<meta property="og:image" content="http://yoursite.com/2019/02/22/rl/强化学习现状与未来/resources/44D46B5545AB96FA9EE7D659C5008C69.jpg">
<meta property="og:image" content="http://yoursite.com/2019/02/22/rl/强化学习现状与未来/resources/62484FF7A8A452030E96A9841B776B72.jpg">
<meta property="og:updated_time" content="2019-03-13T10:31:44.806Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="强化学习现状与未来">
<meta name="twitter:description" content="强化学习路在何方 深度强化学习的泡沫背景 2015年，DeepMind的Volodymyr Mnih等研究员在《自然》杂志上发表论文Human-level control through deep reinforcement learning[1]，该论文提出了一个结合深度学习（DL）技术和强化学习（RL）思想的模型Deep Q-Network(DQN)，在Atari游戏平台上展示出超越人类水平的">
<meta name="twitter:image" content="http://yoursite.com/2019/02/22/rl/强化学习现状与未来/resources/27517C5AFA3857CE6A330D86CB12A334.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '雷哥'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/02/22/rl/强化学习现状与未来/">





  <title>强化学习现状与未来 | 雷哥的博客</title>
  














</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">雷哥的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/02/22/rl/强化学习现状与未来/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">强化学习现状与未来</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-22T07:11:13+08:00">
                2019-02-22
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/强化学习/" itemprop="url" rel="index">
                    <span itemprop="name">强化学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><a href="https://www.colabug.com/3621547.html" target="_blank" rel="noopener">强化学习路在何方</a></p>
<h3 id="深度强化学习的泡沫"><a href="#深度强化学习的泡沫" class="headerlink" title="深度强化学习的泡沫"></a>深度强化学习的泡沫</h3><h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><ul>
<li>2015年，DeepMind的Volodymyr Mnih等研究员在《自然》杂志上发表论文Human-level control through deep reinforcement learning[1]，该论文提出了一个结合深度学习（DL）技术和强化学习（RL）思想的模型Deep Q-Network(DQN)，在Atari游戏平台上展示出超越人类水平的表现。自此以后，结合DL与RL的深度强化学习（Deep Reinforcement Learning, DRL）迅速成为人工智能界的焦点</li>
<li>过去三年间，DRL算法在不同领域大显神通，DeepMind负责AlphaGo项目的研究员David Silver喊出“AI = RL + DL”，认为结合了DL的表示能力与RL的推理能力的DRL将会是人工智能的终极答案</li>
</ul>
<h4 id="DRL的可复现性危机"><a href="#DRL的可复现性危机" class="headerlink" title="DRL的可复现性危机"></a>DRL的可复现性危机</h4><ul>
<li>由于发表的文献中往往不提供重要参数设置和工程解决方案的细节，很多算法都难以复现，RL专家直指当前DRL领域论文数量多却水分大、实验难以复现等问题。该文在学术界和工业界引发热烈反响。很多人对此表示认同，并对DRL的实际能力产生强烈怀疑<ul>
<li>针对DRL领域，Pineau展示了该研究组对当前不同DRL算法的大量可复现性实验。实验结果表明，不同DRL算法在不同任务、不同超参数、不同随机种子下的效果大相径庭</li>
</ul>
</li>
</ul>
<h4 id="DRL研究存在多少坑"><a href="#DRL研究存在多少坑" class="headerlink" title="DRL研究存在多少坑"></a>DRL研究存在多少坑</h4><p>2018年的情人节当天，曾经就读于伯克利人工智能研究实验室的Alexirpan通过一篇博文Deep Reinforcement Learning Doesn’t Work Yet[13]给DRL圈送来了一份苦涩的礼物，从实验角度总结了DRL算法存在的几大问题：</p>
<ul>
<li>样本利用率非常低；</li>
<li>最终表现不够好，经常比不过基于模型的方法；</li>
<li>好的奖励函数难以设计；</li>
<li>难以平衡 “ 探索 ” 和 “ 利用 ” ,以致算法陷入局部极小；</li>
<li>对环境的过拟合；</li>
<li>灾难性的不稳定性…</li>
</ul>
<p>负面评论还将持续发酵。那么， DRL的问题根结在哪里？前景真的如此黯淡吗？如果不与深度学习结合，RL的出路又在哪里？</p>
<h3 id="免模型强化学习的本质缺陷"><a href="#免模型强化学习的本质缺陷" class="headerlink" title="免模型强化学习的本质缺陷"></a>免模型强化学习的本质缺陷</h3><h4 id="RL分类"><a href="#RL分类" class="headerlink" title="RL分类"></a>RL分类</h4><p>RL算法可以分为基于模型的方法（Model-based）与免模型的方法（Model-free）</p>
<ul>
<li><font color="blue">前者主要发展自最优控制领域</font>。通常先通过高斯过程（GP）或贝叶斯网络（BN）等工具针对具体问题建立模型，然后再通过机器学习的方法或最优控制的方法，如模型预测控制（MPC）、线性二次调节器（LQR）、线性二次高斯（LQG）、迭代学习控制（ICL）等进行求解</li>
<li><font color="blue">而后者更多地发展自机器学习领域</font>，属于数据驱动的方法。算法<font color="blue">通过大量采样，估计代理的状态、动作的值函数或回报函数，从而优化动作策略</font>

</li>
</ul>
<h4 id="免模型缺陷"><a href="#免模型缺陷" class="headerlink" title="免模型缺陷"></a>免模型缺陷</h4><ul>
<li>Ben Recht连发了13篇博文，从控制与优化的视角，重点探讨了RL中的免模型方法[18]。Recht指出免模型方法自身存在以下几大缺陷<ul>
<li>免模型方法无法从不带反馈信号的样本中学习，<font color="blue">而反馈本身就是稀疏的，因此免模型方向样本利用率很低，而数据驱动的方法则需要大量采样</font><ul>
<li>比如在Atari平台上的《Space Invader》和《Seaquest》游戏中，智能体所获得的分数会随训练数据增加而增加。利用免模型DRL方法可能需要 2 亿帧画面才能学到比较好的效果。AlphaGo 最早在 Nature 公布的版本也需要 3000 万个盘面进行训练。而但凡与机械控制相关的问题，训练数据远不如视频图像这样的数据容易获取，<font color="blue">因此只能在模拟器中进行训练</font>。而模拟器与现实世界间的Reality Gap，直接限制了训练自其中算法的泛化性能。另外，数据的稀缺性也影响了其与DL技术的结合</li>
</ul>
</li>
<li>免模型方法不对具体问题进行建模，而是尝试用一个通用的算法解决所有问题。而基于模型的方法则通过针对特定问题建立模型，充分利用了问题固有的信息。免模型方法在追求通用性的同时放弃这些富有价值的信息</li>
<li>基于模型的方法针对问题建立动力学模型，这个模型具有解释性。而免模型方法因为没有模型，解释性不强，调试困难</li>
<li>相比基于模型的方法，尤其是基于简单线性模型的方法，免模型方法不够稳定，在训练中极易发散</li>
</ul>
</li>
</ul>
<p>通过Recht的分析，我们似乎找到了DRL问题的根结。近三年在机器学习领域大火的DRL算法，多将免模型方法与DL结合，而免模型算法的天然缺陷，恰好与Alexirpan总结的DRL几大问题相对应</p>
<h4 id="为什么多数DRL的工作都是基于免模型方法呢"><a href="#为什么多数DRL的工作都是基于免模型方法呢" class="headerlink" title="为什么多数DRL的工作都是基于免模型方法呢"></a>为什么多数DRL的工作都是基于免模型方法呢</h4><ul>
<li>免模型的方法相对简单直观，开源实现丰富，比较容易上手</li>
<li>当前RL的发展还处于初级阶段，<font color="blue">学界的研究重点还是集中在环境是确定的、静态的，状态主要是离散的、静态的、完全可观察的，反馈也是确定的问题（如Atari游戏）上。针对这种相对“简单”、基础、通用的问题</font>，免模型方法本身很合适</li>
<li>绝大多数DRL方法是对DQN的扩展，属于免模型方法<img src="/2019/02/22/rl/强化学习现状与未来/resources/27517C5AFA3857CE6A330D86CB12A334.jpg">
</li>
</ul>
<h3 id="基于模型或免模型，问题没那么简单"><a href="#基于模型或免模型，问题没那么简单" class="headerlink" title="基于模型或免模型，问题没那么简单"></a>基于模型或免模型，问题没那么简单</h3><h4 id="基于模型的方法，未来潜力巨大"><a href="#基于模型的方法，未来潜力巨大" class="headerlink" title="基于模型的方法，未来潜力巨大"></a>基于模型的方法，未来潜力巨大</h4><p>基于模型的方法一般先从数据中学习模型，然后基于学到的模型对策略进行优化。学习模型的过程和控制论中的系统参数辨识类似</p>
<ul>
<li>因为模型的存在，基于模型的方法可以充分利用每一个样本来逼近模型，数据利用率极大提高</li>
<li>基于模型的方法则在一些控制问题中，相比于免模型方法，通常有10^2级的采样率提升。</li>
<li>此外，学到的模型往往对环境的变化鲁棒,当遇到新环境时，算法可以依靠已学到的模型做推理，具有很好的泛化性能</li>
<li>预测学习（Predictive Learning)关系<br>基于模型的方法还与潜力巨大的预测学习（Predictive Learning）紧密相关。由于建立了模型，本身就可以通过模型预测未来，这与Predictive Learning的需求不谋而合<ul>
<li>基于模型的RL方法可能是实现Predictive Learning的重要技术之一</li>
</ul>
</li>
</ul>
<h4 id="基于模型问题也较多，免模型方法，依旧是第一选择"><a href="#基于模型问题也较多，免模型方法，依旧是第一选择" class="headerlink" title="基于模型问题也较多，免模型方法，依旧是第一选择"></a>基于模型问题也较多，免模型方法，依旧是第一选择</h4><p>基于模型的DRL方法相对而言不那么简单直观，RL与DL的结合方式相对更复杂，设计难度更高。目前基于模型的DRL方法通常用高斯过程、贝叶斯网络或概率神经网络（PNN）来构建模型<br>基于模型的方法也还若干自身缺陷</p>
<ul>
<li>针对无法建模的问题束手无策</li>
<li>建模会带来误差，而且误差往往随着算法与环境的迭代交互越来越大，使得算法难以保证收敛到最优解</li>
<li>模型缺乏通用性，每次换一个问题，就要重新建模</li>
<li>可能的工作，笔者认为<ul>
<li>我们可以考虑多做一些基于模型的DRL方面的工作，克服当前DRL存在的诸多问题</li>
<li>此外，还可以多研究结合基于模型方法与免模型方法的<font color="blue">半模型方法</font>，兼具两种方法的优势<ul>
<li>这方面经典的工作有RL泰斗Rich Sutton提出的<font color="blue">Dyna框架和Dyna-2框架[28]</font></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h4><ul>
<li>模拟器存在非常大的问题，经过调试的线性策略就已经可以取得非常好的效果——这样的模拟器实在过于粗糙，难怪基于随机搜索的方法可以在同样的模拟器上战胜免模型方法<ul>
<li>可见目前RL领域的实验平台还非常不成熟，在这样的测试环境中的实验实验结果没有足够的说服力。很多研究结论都未必可信</li>
</ul>
</li>
<li>一些学者指出当前RL算法的性能评判准则也不科学</li>
</ul>
<h3 id="重新审视强化学习"><a href="#重新审视强化学习" class="headerlink" title="重新审视强化学习"></a>重新审视强化学习</h3><ul>
<li>DQN和AlphaGo系列工作给人留下深刻印象，但是这两种任务本质上其实相对“简单”。因为这些任务的环境是<font color="blue">确定的、静态的，状态主要是离散的、静态的、完全可观察的，反馈是确定的，代理也是单一的</font></li>
<li>目前DRL在解决<font color="blue">部分可见状态任务（如StarCraft），状态连续的任务（如机械控制任务），动态反馈任务和多代理任务中</font>还没取得令人惊叹的突破<img src="/2019/02/22/rl/强化学习现状与未来/resources/9D2F9556CB9C1DB7A2AEF9EA54261CDC.jpg">
</li>
</ul>
<h4 id="重新审视RL的研究-或不足-值得研究的方向"><a href="#重新审视RL的研究-或不足-值得研究的方向" class="headerlink" title="重新审视RL的研究(或不足)(值得研究的方向)"></a>重新审视RL的研究(或不足)(值得研究的方向)</h4><p>机器学习是个跨学科的研究领域，而RL则是其中跨学科性质非常显著的一个分支。RL理论的发展受到生理学、神经科学和最优控制等领域的启发，现在依旧在很多相关领域被研究<br><img src="/2019/02/22/rl/强化学习现状与未来/resources/44D46B5545AB96FA9EE7D659C5008C69.jpg"></p>
<ul>
<li>基于模型的方法<br>如上文所述，基于模型的方法不仅能大幅降低采样需求，还可以通过学习任务的动力学模型，为预测学习打下基础</li>
<li>提高免模型方法的数据利用率和扩展性<br>这是免模型学习的两处硬伤，也是Rich Sutton的终极研究目标。这个领域很艰难，但是任何有意义的突破也将带来极大价值</li>
<li>更高效的探索策略（Exploration Strategies）<br><font color="blue">平衡“探索”与“利用”是RL的本质问题</font>，这需要我们设计更加高效的探索策略。除了若干经典的算法如Softmax、ϵ-Greedy[1]、UCB[72]和Thompson Sampling[73]等，近期学界陆续提出了大批新算法，如Intrinsic Motivation [74]、Curiosity-driven Exploration[75]、Count-based Exploration [76]等。其实这些“新”算法的思想不少早在80年代就已出现[77]，而与DL的有机结合使它们重新得到重视。此外，OpenAI与DeepMind先后提出通过在策略参数[78]和神经网络权重[79]上引入噪声来提升探索策略, 开辟了一个新方向</li>
<li>与模仿学习（Imitation Learning, IL）结合<br>机器学习与自动驾驶领域最早的成功案例ALVINN[33]就是基于IL；当前RL领域最顶级的学者Pieter Abbeel在跟随Andrew Ng读博士时候,设计的通过IL控制直升机的算法[34]成为IL领域的代表性工作.IL介于RL与监督学习之间，兼具两者的优势，既能更快地得到反馈、更快地收敛，又有推理能力，很有研究价值</li>
<li>奖赏塑形（Reward Shaping）<br>奖赏即反馈，其对RL算法性能的影响是巨大的。<ul>
<li>Alexirpan的博文中已经展示了没有精心设计的反馈信号会让RL算法产生多么差的结果</li>
<li>设计好的反馈信号一直是RL领域的研究热点</li>
<li>近年来涌现出很多基于“好奇心”的RL算法和层级RL算法<ul>
<li>这两类算法的思路都是在模型训练的过程中插入反馈信号，从而部分地克服了反馈过于稀疏的问题。</li>
</ul>
</li>
<li>另一种思路是学习反馈函数，这是逆强化学习（Inverse RL, IRL）的主要方式之一。<ul>
<li>近些年大火的GAN也是基于这个思路来解决生成建模问题, GAN的提出者Ian Goodfellow也认为GAN就是RL的一种方式 [36]。而将GAN于传统IRL结合的GAIL[37]已经吸引了很多学者的注意</li>
</ul>
</li>
</ul>
</li>
<li>RL中的迁移学习与多任务学习<br>当前RL的采样效率极低，而且学到的知识不通用。迁移学习与多任务学习可以有效解决这些问题</li>
<li>提升RL的的泛化能力<ul>
<li>机器学习最重要的目标就是泛化能力, 而现有的RL方法大多在这一指标上表现糟糕[8]，无怪乎Jacob Andreas会批评RL的成功是来自“train on the test set”</li>
<li>研究者们试图通过学习环境的动力学模型[80]、降低模型复杂度[29]或模型无关学习[81]来提升泛化能力，这也促进了基于模型的方法与元学习（Meta-Learning）方法的发展</li>
</ul>
</li>
<li>层级RL（Hierarchical RL, HRL）</li>
<li>与序列预测（Sequence Prediction）结合<ul>
<li>Sequence Prediction与RL、IL解决的问题相似又不相同。三者间有很多思想可以互相借鉴</li>
</ul>
</li>
<li>免模型方法探索行为的安全性（Safe RL）<ul>
<li>相比于基于模型的方法，免模型方法缺乏预测能力，这使得其探索行为带有更多不稳定性。一种研究思路是结合贝叶斯方法为RL代理行为的不确定性建模，从而避免过于危险的探索行为</li>
</ul>
</li>
<li>关系RL<br>近期学习客体间关系从而进行推理与预测的“关系学习”受到了学界的广泛关注。关系学习往往在训练中构建的状态链，而中间状态与最终的反馈是脱节的</li>
<li>对抗样本RL</li>
<li>处理其他模态的输入</li>
</ul>
<h4 id="重新审视RL的应用"><a href="#重新审视RL的应用" class="headerlink" title="重新审视RL的应用"></a>重新审视RL的应用</h4><p>RL只能打游戏、下棋，其他的都做不了？</p>
<ul>
<li>我们不应对RL过于悲观。其实能在视频游戏与棋类游戏中超越人类，已经证明了RL推理能力的强大。通过合理改进后，有希望得到广泛应用</li>
<li>控制领域<br>这是RL思想的发源地之一，也是RL技术应用最成熟的领域。控制领域和机器学习领域各自发展了相似的思想、概念与技术，可以互相借鉴</li>
<li>自动驾驶领域<br>驾驶就是一<font color="blue">个序列决策过程</font>，因此天然适合用RL来处理</li>
<li>NLP领域<br>相比于计算机视觉领域的任务，NLP领域的很多<font color="blue">任务是多轮的</font>，即需<font color="blue">通过多次迭代交互来寻求最优解（如对话系统）</font>；而且任务的<font color="blue">反馈信号往往需要在一系列决策后才能获得（如机器写作）</font>。这样的问题的特<font color="purple">性自然适合用RL来解决</font><ul>
<li>因而近年来RL被应用于NLP领域中的诸多任务中，如文本生成、文本摘要、序列标注、对话机器人（文字/语音）、机器翻译、关系抽取和知识图谱推理等等</li>
</ul>
</li>
<li>推荐系统与检索系统领域<br>RL中的Bandits系列算法早已被广泛应用于商品推荐、新闻推荐和在线广告等领域。近年也有一系列的工作将RL应用于信息检索、排序的任务中</li>
<li>金融领域<br>RL强大的序列决策能力已经被金融系统所关注。无论是华尔街巨头摩根大通还是创业公司如Kensho，都在其交易系统中引入了RL技术</li>
<li>对数据的选择<br>在数据足够多的情况下，如何选择数据来实现“快、好、省”地学习，具有非常大的应用价值</li>
<li>通讯、生产调度、规划和资源访问控制等运筹领域<br>这些领域的任务往往<font color="blue">涉及“选择”动作的过程</font>，而且带标签数据难以取得，因此广泛使用RL进行求解</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li>虽然有上文列举的诸多成功应用，但我们依旧要认识到，当前RL的发展还处于初级阶段，不能包打天下。目前还没有一个通用的RL解决方案像DL一样成熟到成为一种即插即用的算法。RL算法的输出存在随机性，这是其“探索”哲学带来的本质问题，因此我们不能盲目 All in RL, 也不应该RL in All<img src="/2019/02/22/rl/强化学习现状与未来/resources/62484FF7A8A452030E96A9841B776B72.jpg">
</li>
</ul>
<h3 id="广义的RL——从反馈学习"><a href="#广义的RL——从反馈学习" class="headerlink" title="广义的RL——从反馈学习"></a>广义的RL——从反馈学习</h3><h4 id="定义与意义"><a href="#定义与意义" class="headerlink" title="定义与意义"></a>定义与意义</h4><ul>
<li>本节使用“广义的RL”一词指代针对“从反馈学习”的横跨多个学科的研究。与上文中介绍的来自机器学习、控制论、经济学等领域的RL不同，本节涉及的学科更宽泛，<font color="blue">一切涉及从反馈学习的系统，都暂且称为广义的RL</font></li>
<li><font color="blue">行为和反馈是智能形成的基石</font><ul>
<li>生成论（Enactivism）认为行为是认知的基础，行为与感知是互相促进的，智能体通过感知获得行为的反馈，而行为则带给智能体对环境的真实有意义的经验[65]。</li>
</ul>
</li>
</ul>
<h4 id="广义的RL，是未来一切机器学习系统的形式"><a href="#广义的RL，是未来一切机器学习系统的形式" class="headerlink" title="广义的RL，是未来一切机器学习系统的形式"></a>广义的RL，是未来一切机器学习系统的形式</h4><p>只要一个机器学习系统会通过接收外部的反馈进行改进，这个系统就不仅仅是一个机器学习系统，而且是一个RL系统。当前在互联网领域广为使用的A/B测试就是RL的一种最简单的形式。而未来的机器学习系统，都要处理分布动态变化的数据并从反馈中学习。因此可以说，我们即将处于一个“一切机器学习都是RL”的时代，学界和工业界都亟需加大对RL的研究力度</p>
<h4 id="广义的RL，是很多领域研究的共同目标"><a href="#广义的RL，是很多领域研究的共同目标" class="headerlink" title="广义的RL，是很多领域研究的共同目标"></a>广义的RL，是很多领域研究的共同目标</h4><p>节已经提到RL在机器学习相关的领域被分别发明与研究，其实这种从<font color="blue">反馈中学习的思想</font>，在很多其他领域也被不断地研究。仅举几例如下</p>
<ul>
<li>在心理学领域，经典条件反射与操作性条件反射的对比</li>
<li>在教育学领域，一直有关于“主动学习”与“被动学习”两种方式的对比与研究</li>
<li>在组织行为学领域，学者们探究“主动性人格”与“被动性人格”的不同以及对组织的影响</li>
<li>在企业管理学领域，企业的“探索式行为”和“利用式行为”一直是一个研究热点</li>
</ul>
<p>可以说，<font color="blue">一切涉及通过选择然后得到反馈，然后从反馈中学习的领域，几乎都有RL的思想以各种形式存在</font>，因此笔者称之为广义的RL</p>
<ul>
<li>这些学科为RL的发展提供了丰富的研究素材，积累了大量的思想与方法。同时，RL的发展不会仅仅对人工智能领域产生影响，也会推动广义的RL所包含的诸多学科共同前进。</li>
</ul>
<h3 id="束语"><a href="#束语" class="headerlink" title="束语"></a>束语</h3><ul>
<li>虽然RL领域目前还存在诸多待解决的问题，在DRL这一方向上也出现不少泡沫，但我们应该看到RL领域本身在研究和应用领域取得的长足进步</li>
<li>这一领域值得持续投入研究，但在应用时需保持理性</li>
<li>而对基于反馈的学习的研究，不仅有望实现人工智能的最终目标，也对机器学习领域和诸多其他领域的发展颇有意义</li>
<li>这确实是通向人工智能的最佳路径。这条路上布满荆棘，但曙光已现</li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
            <a href="/tags/强化学习/" rel="tag"># 强化学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/02/03/rl/基于模型的学习和规划/" rel="next" title="基于模型的学习和规划">
                <i class="fa fa-chevron-left"></i> 基于模型的学习和规划
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/02/23/rs/简单推荐模型之一 基于流行度的推荐模型/" rel="prev" title="简单推荐模型之一:基于流行度的推荐模型">
                简单推荐模型之一:基于流行度的推荐模型 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/images/favicon.ico" alt="雷哥">
          <p class="site-author-name" itemprop="name">雷哥</p>
           
              <p class="site-description motion-element" itemprop="description">不积跬步无以至千里</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">64</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">16</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">14</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/yuancl" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#深度强化学习的泡沫"><span class="nav-number">1.</span> <span class="nav-text">深度强化学习的泡沫</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#背景"><span class="nav-number">1.1.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DRL的可复现性危机"><span class="nav-number">1.2.</span> <span class="nav-text">DRL的可复现性危机</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DRL研究存在多少坑"><span class="nav-number">1.3.</span> <span class="nav-text">DRL研究存在多少坑</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#免模型强化学习的本质缺陷"><span class="nav-number">2.</span> <span class="nav-text">免模型强化学习的本质缺陷</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#RL分类"><span class="nav-number">2.1.</span> <span class="nav-text">RL分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#免模型缺陷"><span class="nav-number">2.2.</span> <span class="nav-text">免模型缺陷</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么多数DRL的工作都是基于免模型方法呢"><span class="nav-number">2.3.</span> <span class="nav-text">为什么多数DRL的工作都是基于免模型方法呢</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基于模型或免模型，问题没那么简单"><span class="nav-number">3.</span> <span class="nav-text">基于模型或免模型，问题没那么简单</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基于模型的方法，未来潜力巨大"><span class="nav-number">3.1.</span> <span class="nav-text">基于模型的方法，未来潜力巨大</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#基于模型问题也较多，免模型方法，依旧是第一选择"><span class="nav-number">3.2.</span> <span class="nav-text">基于模型问题也较多，免模型方法，依旧是第一选择</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#其他问题"><span class="nav-number">3.3.</span> <span class="nav-text">其他问题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#重新审视强化学习"><span class="nav-number">4.</span> <span class="nav-text">重新审视强化学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#重新审视RL的研究-或不足-值得研究的方向"><span class="nav-number">4.1.</span> <span class="nav-text">重新审视RL的研究(或不足)(值得研究的方向)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#重新审视RL的应用"><span class="nav-number">4.2.</span> <span class="nav-text">重新审视RL的应用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#总结"><span class="nav-number">4.3.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#广义的RL——从反馈学习"><span class="nav-number">5.</span> <span class="nav-text">广义的RL——从反馈学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#定义与意义"><span class="nav-number">5.1.</span> <span class="nav-text">定义与意义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#广义的RL，是未来一切机器学习系统的形式"><span class="nav-number">5.2.</span> <span class="nav-text">广义的RL，是未来一切机器学习系统的形式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#广义的RL，是很多领域研究的共同目标"><span class="nav-number">5.3.</span> <span class="nav-text">广义的RL，是很多领域研究的共同目标</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#束语"><span class="nav-number">6.</span> <span class="nav-text">束语</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-雷哥"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">雷哥</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Gemini
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  






  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
