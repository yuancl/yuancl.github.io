<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css">


  <meta name="keywords" content="Hexo, NexT">








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2">






<meta name="description" content="不积跬步无以至千里">
<meta property="og:type" content="website">
<meta property="og:title" content="雷哥的博客">
<meta property="og:url" content="http://yoursite.com/page/7/index.html">
<meta property="og:site_name" content="雷哥的博客">
<meta property="og:description" content="不积跬步无以至千里">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="雷哥的博客">
<meta name="twitter:description" content="不积跬步无以至千里">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '雷哥'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/7/">





  <title>雷哥的博客</title>
  














</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">雷哥的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/27/ml/GBDT模型/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/27/ml/GBDT模型/" itemprop="url">GBDT模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-27T07:19:11+08:00">
                2018-07-27
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/机器学习基础/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习基础</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://yuancl.github.io/2018/07/24/nlp/集成学习方法/" target="_blank" rel="noopener">集成学习方法</a></p>
<h4 id="GB-Gradient-Boosting-理解"><a href="#GB-Gradient-Boosting-理解" class="headerlink" title="GB(Gradient Boosting)理解"></a>GB(Gradient Boosting)理解</h4><ul>
<li><p>理解梯度下降</p>
<ul>
<li>求解目标函数$J(\theta)$的最优解，本质上是一轮一轮的求解w</li>
<li>对于最终的最后解W*,是由初始值$w_0$经过M次迭代得到的</li>
</ul>
</li>
<li><p>对比理解多轮迭代后的GB函数最优解</p>
<ul>
<li>$f(x)$经过M次迭代，得到$F(x)=\sum_{i=1}^m\theta _if_i(x)$</li>
<li>这里是反向思考：因为梯度下降是通过梯度下降的方法求出w参数值，最后就是经过了M轮迭代求出最优解w<em>.  这里的w是参数，同样的思想对比到函数F(x)上,<font color="blue">将F(x)视为整体类似于w</font>，最后通过多轮迭代求出F</em>(x)最优解</li>
</ul>
</li>
<li><p>可以看出上述是一个求解梯度的过程，因此也称为基于梯度的Boost方法(即GB:Gradient Boosting)</p>
</li>
<li><p>算法</p>
<ul>
<li>这里的m是m个机器学习模型，n应该是样本的数量。这里就能够看出，boost方法就是前一轮的结果会影响后一轮<img src="/2018/07/27/ml/GBDT模型/resources/297ED5E7B40AD7E5192DB0EE79E577F7.jpg"></li>
<li>一般定义不同的Loss函数，就能得到不同的算法，比如这里用二分类任务中常用的$L(y,F)=log(1+exp(-2yF)), y \in {(-1,1)}$,就能得到如下算法：<img src="/2018/07/27/ml/GBDT模型/resources/DB082CA541217CF13A4A4DC535C9D2F0.jpg">
</li>
</ul>
</li>
</ul>
<h4 id="DT理解"><a href="#DT理解" class="headerlink" title="DT理解"></a>DT理解</h4><p>参考<a href="https://yuancl.github.io/2018/08/06/ml/决策树/" target="_blank" rel="noopener">决策树</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/24/ml/集成学习方法/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/24/ml/集成学习方法/" itemprop="url">集成学习方法</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-24T23:09:10+08:00">
                2018-07-24
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/机器学习基础/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习基础</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="个体与集成"><a href="#个体与集成" class="headerlink" title="个体与集成"></a>个体与集成</h4><p>集成学习（ensemble learning）的一般结构：先产生一组“个体学习器”（individual learner），再用某种策略将他们结合起来<br><img src="/2018/07/24/ml/集成学习方法/resources/4B0EDAB496C74765B1C89797A91E5BB2.jpg"></p>
<ul>
<li><p>集成也可包含不同类型的个体学习器</p>
<ul>
<li>在一般的经验中，如果把好坏不等的东西掺到一起，那么通常结果会是比坏的好一些，比好的要坏一些。集成学习把多个学习器结合起来，如何能获得比最好的单一学习器更好的性能呢</li>
</ul>
</li>
<li><p>考虑一个简单的例子：在二分类任务中，假定三个分类器在三个测试样本的表现如下图所示</p>
<ul>
<li>集成学习的结果通过投票法（voting）产生，即“少数服从多数”。</li>
<li>这个简单的例子显示出：要获得好的集成，个体学习器应“好而不同”。<ul>
<li>个体学习器要有一定的<font color="blue">“准确性”</font>，即学习器不能太坏</li>
<li>而且要有<font color="blue">“多样性”（diversity）</font>，即学习器之间有差异。</li>
<li>事实上，如何产生并结合“好而不同”的个体学习器，恰是集成学习研究的核心<img src="/2018/07/24/ml/集成学习方法/resources/EC147BED87BB6C5FE7B43608C8F250F2.jpg">
</li>
</ul>
</li>
</ul>
</li>
<li><p>多学习器结合的好处</p>
<ul>
<li>从统计的方面看，由于学习任务的假设空间往往很大，可能有多个假设在训练集上达到同等性能，此时若使用单学习器可能因误选而导致泛化性能不佳，结合多个学习器减小这一风险；</li>
<li>从计算的方面来看，学习算法往往会陷入局部极小，有的局部极小点所对应的泛化性能可能很糟，而通过多次运行之后进行结合，可降低陷入糟糕局部极小点的风险；</li>
<li>从表示的方面来看，某些但学习器则肯定无效，而通过结合多个学习器，由于响应的假设空间有所扩大，有可能学得更好的近似<img src="/2018/07/24/ml/集成学习方法/resources/C648EEAAFD8FB8DF3F291B6B0A998214.jpg">
</li>
</ul>
</li>
<li><p>结合不同子模型的分类</p>
<img src="/2018/07/24/ml/集成学习方法/resources/BC8AEFE15619E77CB8F6BF1C2478097F.jpg">
</li>
</ul>
<h4 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h4><p>各个子模型训练的时候是随机在样本库中抽取部分样本，并行训练，最后的多个模型参数取平均值</p>
<ul>
<li>随机抽取样本(有放回的)</li>
<li>并行训练模型，各个模型之间无影响</li>
<li>最后样本合成时，以平均值参数进行合成$F(x)=\frac 1m\sum_{i=1}^mf_i(x)$<img src="/2018/07/24/ml/集成学习方法/resources/2C16474FB5F7CE8CDAFFAFA3A1E57777.jpg">
</li>
</ul>
<h4 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h4><p>Boosting是一族可将弱学习器提升为强学习器的算法。这族算法的工作机制类似：</p>
<ul>
<li>先从初始训练集训练出一个基学习器，再根据基学习器的表现对<font color="blue">训练样本分布进行调整</font>，使得先前基学习器做错的训练样本在后续收到更多的关注<font color="blue">(其实就是改变权重，对判断正确的样本进行降权，判断错误的进行加权)</font></li>
<li>然后基于调整后的样本分布来训练下一个基学习器；</li>
<li>如此重复进行，直到基学习器数目<font color="blue">达到事先指定的值T</font>，最终将这T个学习器进行加权结合<br>特点：</li>
<li>每个学习区直接相互影响，串行化学习，最终的模型是对各个学习器加权求和,$F(x)=\sum_{i=1}^m\theta _if_i(x)$<img src="/2018/07/24/ml/集成学习方法/resources/E593F3A40EE6A8026D8ECB26C37FA5D2.jpg"></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/22/ml/概率图/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/22/ml/概率图/" itemprop="url">概率图模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-22T22:19:00+08:00">
                2018-07-22
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/机器学习基础/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习基础</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>参考文章：<br><a href="https://www.jiqizhixin.com/articles/2017-11-29-3?from=synced&amp;keyword=概率图" target="_blank" rel="noopener">机器之心：读懂概率图</a></p>
<h4 id="图模型"><a href="#图模型" class="headerlink" title="图模型"></a>图模型</h4><ul>
<li>图模型为很多存在依赖关系的真实世界任务提供了可以解释的建模方式。图模型为我们提供了一种用有原则的方式解决这些任务的方法</li>
<li>概率图模型（PGM/probabilistic graphical model）是一种用于学习这些带有依赖（dependency）的模型的强大框架</li>
<li><p>图的每个节点（node）都关联了一个随机变量，而图的边（edge）则被用于编码这些随机变量之间的关系</p>
<ul>
<li>我们可以将图的模式分为两大类——贝叶斯网络（Bayesian network）和马尔可夫网络（Markov networks）<img src="/2018/07/22/ml/概率图/resources/5682C7A9FF3D94CD38864DBCE005C4AD.jpg">
</li>
</ul>
</li>
<li><p>“生成式”（generative）模型考虑联合分布P(Y,R,O)；<br>“判别式”（discriminative）模型考虑条件分布P(Y,R|O);</p>
</li>
<li><p>条件独立</p>
<ul>
<li>图结构实际上带有关于这些变量的重要信息。具体来说，它们定义了这些变量之间的一组条件独立（conditional independence），也就是这种形式的陈述——「如果观察到 A，那么 B 独立于 C。」</li>
</ul>
</li>
</ul>
<h4 id="有向图模型-贝叶斯网络"><a href="#有向图模型-贝叶斯网络" class="headerlink" title="有向图模型:贝叶斯网络"></a>有向图模型:贝叶斯网络</h4><p>“生成式”（generative）模型考虑联合分布P(Y,R,O),属于生成模型，借助有向无环图(DAG图)来刻画属性简的依赖关系，并<font color="blue">使用条件概率表来描述属性的联合概率分布</font>，这里重点是<font color="blue">计算联合概率分布</font></p>
<ul>
<li>例子<ul>
<li>让我们看看与每个节点关联的表格，它们的正式名称是条件概率分布（CPD/conditional probability distribution）<img src="/2018/07/22/ml/概率图/resources/D40035339F71FF67D1772BC364EC9F28.jpg">
</li>
</ul>
</li>
</ul>
<h4 id="隐马可科夫模型"><a href="#隐马可科夫模型" class="headerlink" title="隐马可科夫模型"></a>隐马可科夫模型</h4><p>隐马尔可夫模型（Hidden Markov Model，简称HMM）是结构最简单的动态贝叶斯网（dynamic Bayesian network），这是一种著名的有向图模型，主要用于时序数据建模，在语音识别、自然语言处理等领域有广泛应用</p>
<ul>
<li><p>其中几个重要的参数：</p>
<ul>
<li>a.状态转移概率，y1,y2….的转移概率。</li>
<li>b.输出观察概率y输出x的概率。</li>
<li>c.初始状态概率，y1<img src="/2018/07/22/ml/概率图/resources/7CFCA82225375CBBA196AB941002CB04.jpg">
</li>
</ul>
</li>
<li><p>HMM解决的问题</p>
<ul>
<li><font color="blue">如何评估模型与观测序列之间的匹配程度</font>，例如许多任务已有观察序列{x1,x2,x3…xn-1}求x(n)的最有可能值，就是转换为判定模型，<font color="blue">$P(x|\theta)$最大的匹配程度</font></li>
<li><font color="blue">根据观测序列推断出隐藏的模型状态</font>，已经{x1,x2,x3…x(n)},求{y1,y2,y3…y(n)}。如语音识别中，观测值为语音符号，隐藏状态为文字</li>
<li><font color="blue">如何训练模型，使其能最好的描述观测数据</font>，即调整模型参数[A,B,PI]，使得该观测序列出现的概率最大</li>
</ul>
</li>
</ul>
<h4 id="马尔可夫随机场"><a href="#马尔可夫随机场" class="headerlink" title="马尔可夫随机场"></a>马尔可夫随机场</h4><p>马尔科夫随机场是典型的马尔科夫网络，是一种著名的无向图模型，<font color="blue">多个变量之间的联合概率分布能够基于团分解为多个因子的乘积</font><br><img src="/2018/07/22/ml/概率图/resources/EEF542AE63098C2BA0D95E6D555110C2.jpg"></p>
<ul>
<li><p>团：<br>对于图中的任意两点都有线相连，则称该结点子集为一个”团”，若在一个团中加入另外的节点都不再形成团，那么陈该该结点子集为”极大团”</p>
</li>
<li><p>势函数：<br>亦称”因子”(factor)，这是定义在变量子集上的非负实函数，主要用于定义概率分布函数:</p>
</li>
<li><p>多个变量之间的联合概率分布能够基于团分解为多个因子的乘积</p>
<img src="/2018/07/22/ml/概率图/resources/E6DCEA98E35EF5CA0D9991847B6F7698.jpg">
<img src="/2018/07/22/ml/概率图/resources/DE12D0B50F145E19B33337D1C43F0839.jpg">
</li>
</ul>
<h4 id="条件随机场"><a href="#条件随机场" class="headerlink" title="条件随机场"></a>条件随机场</h4><p>是一种判别式无向图模型，对条件分布进行建模。试图对多个变量在给定观测值后的条件概率进行建模。</p>
<ul>
<li>具体说就是给定$X={x_1,x_2,x_3,….x_n}和Y={y_1,y_2,y_3…y_n}$然后建立模型P(Y|X)。然后对后面给定的$(x_{11},x_{12},x….)$直接使用P(Y|X)模型进行预测。标记变量y可以是结构型变量，即其分量直接具有某种相关性</li>
</ul>
<h4 id="学习和推断"><a href="#学习和推断" class="headerlink" title="学习和推断"></a>学习和推断</h4><ul>
<li><p>学习(参数估计)：<br>如果都知道各个变量，各个属性间的依赖关系，只需要对各个条件概率表进行计数，就能够得到联合概率分布。但实际情况中几乎不会轻易得到所有的关系依赖，所有贝叶斯网络的首要任务是根据训练数据<font color="blue">找出最“恰当”的贝叶斯网，也就是学习出属性间的依赖关系，得到联合概率分布</font>。使用的是评分函数算法</p>
</li>
<li><p>推断(推理)：通过第一步的学习得到了联合概率分布，属性，变量间的依赖关系，也就是<font color="blue">得到了贝叶斯网络后，就可以通过它来回答”查询”，及通过一些已知属性变量的观测值来预测一些其他的属性</font></p>
<ul>
<li><p>我们可以使用推理来解答一些问题:</p>
<ul>
<li>边际推理（marginal inference）：寻找一个特定变量的概率分布。比如，给定一个带有变量 A、B、C 和 D 的图，其中 A 取值 1、2 和 3，求 p(A=1)、p(A=2) 和 p(A=3)。</li>
<li>后验推理（posterior inference）：给定某些显变量 v_E（E 表示证据（evidence）），其取值为 e，求某些隐藏变量 v_H 的后验分布 p(v_H|v_E=e)。</li>
<li>最大后验（MAP）推理（maximum-a-posteriori inference）：给定某些显变量 v_E，其取值为 e，求使其它变量 v_H 有最高概率的配置</li>
</ul>
</li>
<li><p>解答这些问题的流行的算法<br>其中既有精准的算法，也有近似的算法。所有这些算法都既可用于贝叶斯网络，也可用于马尔可夫网络</p>
<ul>
<li>精确推断方法<br>希望能计算出目标变量的边际分布或条件分布的精确值。遗憾的是，一般情形下，此类算法的计算复杂度随着极大团规模的增长呈指数增长，适用范围有限<ul>
<li>变量消去<br>精确推断的实质是一类动态规划算法，它利用图模型所描述的条件独立性来消减计算目标概率值所需的计算量。变量消去是最直观的精确推断算法，也是构建其他精确推断算法的基础。<br>变量消去法有一个明显的缺陷：若需计算多个边际分布，重复使用变量消去法将对造成大量的冗余计算。</li>
<li>信念传播<br>信念传播（Belief Propagation）算法将变量消去法中的求和操作看作一个消息传递过程，较好的解决了求解多个边际分布时重复计算问题。</li>
</ul>
</li>
<li>近似推断方法<br>希望在较低时间复杂度下获得原问题的近似解。此类方法在现实任务中更常用<ul>
<li>采样：通过使用随机化方法完成近似<ul>
<li>MCMC采样:概率图模型中最常用的采用技术是马尔可夫链蒙特卡罗（Markov Chain Monte Carlo，简称MCMC）方法</li>
</ul>
</li>
<li>变分推断（variational inference）<ul>
<li>变分推断通过使用已知简单分布来逼近所需推断的复杂分布，并通过限制近似分布的类型，从而得到一种局部最优、但具有确定解的近似后验分布</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="应用举例"><a href="#应用举例" class="headerlink" title="应用举例"></a>应用举例</h4><ul>
<li><p><strong>三门问题(贝叶斯网络)</strong></p>
<ul>
<li>问题描述：<br>主持人会向你展示三扇关着的门，其中一扇门之后有一辆车，其它门后则有一些无价值的东西。你可以选择一扇门。然后，主持人会打开剩下的两扇门中没有车的一扇。现在，你可以选择是否更换选择的门：坚持你之前选择的那扇门，还是选择主持人剩下的那扇关闭的门。你会更换吗？<ul>
<li>直觉上看，主持人似乎并没有透露任何信息。事实证明这种直觉并不完全正确。让我们使用我们的新工具「图模型」来理解这个问题<img src="/2018/07/22/ml/概率图/resources/5BA177BF6F0E9415D2B19622C743CCAA.jpg"></li>
</ul>
</li>
<li>网络构造<ul>
<li>定义变量<ul>
<li>D：背后有车的门</li>
<li>F：你的第一个选择</li>
<li>H：主持人打开的门</li>
<li>I：F 是否是 D？</li>
</ul>
</li>
<li>D、F 和 H 可取值为 1、2 或 3；I 可取值 0 或 1。D 和 I 是未被观察到的，而 F 是已观察到的。在主持人打开其中一扇门之前，H 都是未被观察到的。因此，我们使用贝叶斯网络来解决我们的问题<img src="/2018/07/22/ml/概率图/resources/D4BFDA14C753F0039371D80EB0098B09.jpg"></li>
</ul>
</li>
<li>计算逻辑<br>注意箭头的方向——D 和 F 是相互独立的，I 显然依赖于 D 和 F，主持人选择的门也取决于 D 和 F。目前你对 D 还一无所知。（这与学生网络的结构类似，即知道学生的智力水平不能让你获得有关课程难度的任何信息<ul>
<li>现在，主持人选择了门 H 并打开了它。所以现在 H 已被观察到<img src="/2018/07/22/ml/概率图/resources/70032238A7BF5A48B7CDBF44977207A1.jpg"></li>
<li>然后下面就是通过计算变量的CPD来得到最大的条件概率(判别式模型),详情见<a href="https://www.jiqizhixin.com/articles/2017-11-29-3?from=synced&amp;keyword=概率图" target="_blank" rel="noopener">机器之心：读懂概率图</a></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>图像去噪</strong></p>
<ul>
<li><p>真实图像和噪音图像</p>
<img src="/2018/07/22/ml/概率图/resources/B79299EA90F5B96C7A44B9BCDA990893.jpg">
<img src="/2018/07/22/ml/概率图/resources/CAA3F15B2AB83ECFE01B7DB0EF10CB36.jpg">
</li>
<li><p>目标：现在你的目标是恢复原始图像。让我们看看如何使用概率图模型来实现</p>
</li>
<li><p>思考步骤</p>
<ul>
<li>首先第一步是思考哪些变量是观察得到的，哪些变量不能观察到，以及<font color="blue">我们可以如何将它们连接起来构成一个图</font></li>
<li>让我们将有噪声图像中的每个像素都<font color="blue">定义为一个观察到的随机变量</font>，并将基准图像中的每个像素都<font color="blue">定义为一个未被观察到的变量</font>。由此，如果该图像的大小为 MxN，那么观察到的变量和未被观察到的变量都各有 MN 个。让我们将观察到的变量表示为 X_ij，未被观察到的变量定义为 Y_ij。每个变量都可取值 +1 或 -1（分别对应于黑色像素和白色像素）。<ul>
<li>模型：给定观察到的变量，我们希望找到未观察到的变量的最有可能的值。这<font color="red">对应于 MAP 推理</font></li>
</ul>
</li>
<li><p>现在让我们使用一些领域知识来构建图结构。很显然，在有噪声图像中的 (i,j) 位置观察到的变量取决于在基准图像中的 (i,j) 位置未观察到的变量。原因是大多数时候它们是相等的。</p>
</li>
<li><p>我们还能得到什么信息？对于基准图像，邻近的像素通常有一样的值——在颜色变化的边界不是这样，但在每个单一颜色的区域内有这个性质。因此，<font color="blue">如果 Y_ij 和 Y_kl 是邻近像素，那么我们将它们连接起来</font></p>
</li>
</ul>
</li>
<li><p>图结构</p>
<ul>
<li>其中，白色节点表示未被观察到的变量 Y_ij，灰色节点表示观察到的变量 X_ij。每个 X_ij 都连接到对应的 Y_ij，每个 Y_ij 都连接到它的相邻节点。</li>
<li>注意<font color="blue">这是一个马尔可夫网络</font>，因为图像的像素之间不存在因果关系，因此这里不适合使用贝叶斯网络中有方向的箭头<img src="/2018/07/22/ml/概率图/resources/2DA40217C2D320AA8E7CCF137F62E066.jpg"></li>
</ul>
</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/15/ml/KNN(K近邻)/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/15/ml/KNN(K近邻)/" itemprop="url">KNN</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-15T07:09:09+08:00">
                2018-07-15
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/机器学习基础/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习基础</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><ul>
<li><p>选取离样本最近的K个点</p>
<ul>
<li>计算距离的方式可以有多种，欧氏距离等等</li>
</ul>
</li>
<li><p>分类策略</p>
<ul>
<li>比较简单的就是直接看这K个点钟类别最多的一个，来作为当前样本点的类别</li>
</ul>
</li>
</ul>
<h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><ul>
<li>K近邻不用进行训练</li>
<li>选择K个点的时候，每次都需要计算全部的样本，计算量比较大</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/10/ml/KMeans/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/10/ml/KMeans/" itemprop="url">KMeans</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-10T21:09:02+08:00">
                2018-07-10
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/机器学习基础/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习基础</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="算法步骤："><a href="#算法步骤：" class="headerlink" title="算法步骤："></a>算法步骤：</h4><ul>
<li>随机选择K个簇中心点$\mu_1,\mu_2,\mu_3…\mu_k$</li>
<li>然后依次加入其它节点，采用距离最近(该节点到各簇中心距离)的判别方式</li>
<li>重新计算各簇中心点(因为有新节点加入)，直到所有节点都已加入<img src="/2018/07/10/ml/KMeans/resources/B02EB1114F53182AC4C8A33C041F5A71.jpg">
</li>
</ul>
<h4 id="Random-initialization"><a href="#Random-initialization" class="headerlink" title="Random initialization"></a>Random initialization</h4><ul>
<li>会重复KMeans n次，找出cost最小的k值<img src="/2018/07/10/ml/KMeans/resources/D1E2FCC1B588D77F14AA8C949B1A487E.jpg">
<img src="/2018/07/10/ml/KMeans/resources/32DA03A8B43321EEE51CE601BF8B214B.jpg">
<h4 id="Cost-funciton"><a href="#Cost-funciton" class="headerlink" title="Cost funciton"></a>Cost funciton</h4>所有点到各簇中心点距离和<img src="/2018/07/10/ml/KMeans/resources/BC93652FCD6FE37D162DE72468A473F6.jpg"></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/28/ml/支持向量机(SVM)/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/28/ml/支持向量机(SVM)/" itemprop="url">SVM</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-28T22:40:02+08:00">
                2018-06-28
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/机器学习基础/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习基础</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="间隔与支持向量"><a href="#间隔与支持向量" class="headerlink" title="间隔与支持向量"></a>间隔与支持向量</h4><ul>
<li>支持向量：假设超平面(w,b)能将训练样本正确分类，即对于$(x_i,y_i)\in D$,若$y_i=1$,则有$w^Tx_i+b&gt;0$;若$y_i=-1$,则有$w^Tx_i+b&lt;0$令<img src="/2018/06/28/ml/支持向量机(SVM)/resources/C545E3E754ED9E6EB32618677B8AC1D9.jpg">
如图 6.2所示，距离超平面<font color="blue">最近的这几个训练样本点</font>使式(6.3)的等号成立， 它们被称为”支持向量” (support vector)<ul>
<li>svm是解决分类问题，但并不会计算所以的点,只计算支持向量</li>
</ul>
</li>
<li>间隔：两个异类支持向量到超平面的距离之和被称为”间隔” (margin).<img src="/2018/06/28/ml/支持向量机(SVM)/resources/B70FA52BD036240F08C998533D4D83A3.jpg">
</li>
</ul>
<h4 id="硬间距与软间距"><a href="#硬间距与软间距" class="headerlink" title="硬间距与软间距"></a>硬间距与软间距</h4><ul>
<li>硬间距:前面介绍的支持向量机形式是要求所有样本均满足约束(6.3)， 即所有样本都必须划分i丘确</li>
<li>软间距：软间隔则是允许某些样本不满足约束6.3，允许某些样本不满足:$y_i(w^Tx_i+b)&gt;=1$<ul>
<li>在最大化间隔的同时，不满足约束的样本应尽可能少<img src="/2018/06/28/ml/支持向量机(SVM)/resources/F0A10EEADFE86AFD179E763B252B3431.jpg"></li>
<li>松弛变量($\varepsilon$)与软间隔支持向量机<img src="/2018/06/28/ml/支持向量机(SVM)/resources/8279825E046F44EC977D442FDD7C903D.jpg"></li>
</ul>
</li>
<li><font color="blue">优化目标：</font><img src="/2018/06/28/ml/支持向量机(SVM)/resources/938EF5CF0044A14AC045C146CD2399F8.jpg">
</li>
</ul>
<h4 id="替代损失"><a href="#替代损失" class="headerlink" title="替代损失"></a>替代损失</h4><ul>
<li>$l_{0/1}$非凸、非连续，数学性质不太好，使得式 (6.29)不易直接求解.于 是，人们通常用其他一些函数来代替$l_{0/1}$</li>
<li>三种常用的替代损失函数:<img src="/2018/06/28/ml/支持向量机(SVM)/resources/5C561540C4679CEBFF77440F59C5DBEA.jpg">
<img src="/2018/06/28/ml/支持向量机(SVM)/resources/67CA8735991FC2CCCEA06FDDE4F2EC87.jpg">
<img src="/2018/06/28/ml/支持向量机(SVM)/resources/D2C130615908A44229AA9B8C067B4A80.jpg">
</li>
</ul>
<h4 id="约束求解-amp-拉格朗日乘子求解-amp-KKT"><a href="#约束求解-amp-拉格朗日乘子求解-amp-KKT" class="headerlink" title="约束求解&amp;拉格朗日乘子求解&amp;KKT"></a>约束求解&amp;拉格朗日乘子求解&amp;KKT</h4><ul>
<li>求解最优问题：1.无约束（一般直接求导）2.等式约束（用拉格朗日求解）3.不等式约束（用KKT求解）</li>
<li>拉格朗日乘子：通过拉格朗日这位大神的办法重新定义一个无约束问题（大家都喜欢无拘无束），这个无约束问题等价于原来的约束优化问题，从而将约束问题无约束化</li>
<li>式(6.35)中每个样本都有一个对应的松弛变量， 用 以表征该样本不满 足约束(6.28)的程度.但是，与式(6.6)相似，这仍是一个二次规划问题于是?类 似式 (6.8)，通过拉格朗日乘子法可得到式 (6.35)的拉格朗日函 数<img src="/2018/06/28/ml/支持向量机(SVM)/resources/8B3EC6F476E13888172948983D4EAE13.jpg">
</li>
</ul>
<h4 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h4><img src="/2018/06/28/ml/支持向量机(SVM)/resources/23FCF88EDB0FDB93728C5A2548BE7E14.jpg">
<h4 id="支持向量回归"><a href="#支持向量回归" class="headerlink" title="支持向量回归"></a>支持向量回归</h4><p>传统回归模型通常直接基于模型输出 f(x) 与真实输出y之 间的差别来计算损失，当且仅当 f(x) 与 y 完全相同时，损失才为零.与此不同， 支持向量回归 (Support Vector Regression，简称 SVR)假设我们能容忍 f(x) 与 y 之间最多有$\epsilon$的偏差，即仅当 f(x) 与 u 之间的差别绝对值大于 E 时才计算损 失.如国 6.7所示?这相当于以 f(x) 为中心?构建了一个宽度为$2\epsilon$的问隔带，若 训练样本落入此间隔带，则认为是被预测正确的.<br><img src="/2018/06/28/ml/支持向量机(SVM)/resources/127FBB39702127EC06BEE121F38B013B.jpg"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/22/ml/PCA/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/22/ml/PCA/" itemprop="url">PCA</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-22T22:01:10+08:00">
                2018-06-22
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/算法/" itemprop="url" rel="index">
                    <span itemprop="name">算法</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="http://blog.codinglabs.org/articles/pca-tutorial.html" target="_blank" rel="noopener">参考文章</a></p>
<h4 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h4><ul>
<li><p>线性代数相关概念</p>
<ul>
<li>对称矩阵:除对角线，其余值一三象限相等</li>
<li>对角矩阵(diagonal matrix):是一个主对角线之外的元素皆为0的矩阵</li>
<li>矩阵对角化：如果存在一个矩阵<strong>A</strong>，使$A^{-1}MA$  的结果为对角矩阵，则称矩阵<strong>A</strong>将矩阵<strong>M</strong>  对角化。对于一个矩阵来说，不一定存在将其对角化的矩阵，但是任意一个n×n矩阵如果存在n个线性不相关的特征向量，则该矩阵可被对角化<ul>
<li>定理（理解特征值和特征向量的作用）：<img src="/2018/06/22/ml/PCA/resources/6A2D68A769C48FF884859CA389ECED5C.jpg"></li>
</ul>
</li>
<li>实对称矩阵<ul>
<li>实对称矩阵A的不同特征值对应的特征向量是正交的。<font color="red">(PCA算法最终就是将初始值的协方差矩阵变化为实对称矩阵)</font></li>
<li>实对称矩阵A的特征值都是实数，特征向量都是实向量。</li>
</ul>
</li>
</ul>
</li>
<li><p>协方差</p>
<ul>
<li>均值描述的是样本集合的中间点，而标准差给我们描述的则是样本集合的各个样本点到均值的距离之平均，都是一维数据<br>$var(X)=\frac{\sum^{n}_{i=1}(X_i-\overline X)(X_i-\overline X)}{n}$</li>
<li>协方差：协方差就是这样一种用来度量两个随机变量关系的统计量<ul>
<li>$cov(X,Y)=\frac{\sum^{n}_{i=1}(X_i-\overline X)(Y_i-\overline Y)}{n}$。如果各维度均值都标准化为0，那么就简化为:$cov(X,Y)=\frac{\sum^{n}_{i=1}(X_i)(Y_i)}{n}$</li>
<li>如果结果为正值，则说明两者是正相关的,否则为负相关。如果为0，表示不相关，也就是正交</li>
</ul>
</li>
</ul>
</li>
<li><p>协方差矩阵</p>
<ul>
<li>如果有多维的情况，就可以使用矩阵的思想来看更直观,比如三维的例子:<br>$C = \begin{bmatrix}<br>cov(x,x),cov(x,y),cov(x,z)<br>\<br>cov(y,x),cov(y,y),cov(y,z)<br>\<br>cov(z,x),cov(z,y),cov(z,z)<br>\end{bmatrix}$</li>
<li>协方差矩阵计算：<ul>
<li>假设我们只有a和b两个字段(两维度特征)，那么我们将它们按行组成矩阵X<br>$X=\begin{bmatrix}<br>a_1,a_2,a_3….a_n<br>\<br>b_1,b_2,b_3….b_n<br>\end{bmatrix}<br>$</li>
<li>然后我们用X乘以X的转置，并乘上系数1/m<br>$\frac1mXX^T=\begin{bmatrix}<br>\frac1m\sum^m_{i=1}a_ia_i  \frac1m\sum^m_{i=1}a_ib_i<br>\<br>\frac1m\sum^m_{i=1}a_ib_i  \frac1m\sum^m_{i=1}b_ib_i<br>\end{bmatrix}<br>$</li>
<li>设我们有m个n维数据记录，将其按列排成n乘m的矩阵X，设$C=1mXX𝖳，则C是一个对称矩阵，其对角线分别个各个字段的方差，而第i行j列和j行i列元素相同，表示i和j两个字段的协方差</li>
</ul>
</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/20/ml/Backpropagation/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/20/ml/Backpropagation/" itemprop="url">Backpropagation</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-20T08:01:20+08:00">
                2018-06-20
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/算法/" itemprop="url" rel="index">
                    <span itemprop="name">算法</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://www.cnblogs.com/andywenzhi/p/7295262.html?utm_source=itdadao&amp;utm_medium=referral" target="_blank" rel="noopener">参考链接</a></p>
<h4 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h4><ul>
<li>BP 算法的目的在于为优化函数（比如梯度下降、其它的高级优化方法）提供梯度值，即使用 BP 算法计算代价函数（cost function）对每个参数的偏导值，其数学形式为：$\frac{\partial}{\partial\theta^l_{ij}}J(\theta)$，并最终得到的值存放在矩阵$\Delta^{(l)}$中。</li>
</ul>
<h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><ul>
<li>简单神经网络结构：<img src="/2018/06/20/ml/Backpropagation/resources/65C926A1DF9A4F2D9EBB22BEC38F663F.jpg"></li>
<li>单个数据的代价函数：<img src="/2018/06/20/ml/Backpropagation/resources/2A341859393F3EFDA0E5F215414C2CD5.jpg">
</li>
</ul>
<h4 id="神经网络学习算法图概览"><a href="#神经网络学习算法图概览" class="headerlink" title="神经网络学习算法图概览"></a>神经网络学习算法图概览</h4><ul>
<li><p>清楚一个函数的求导对象：输入值<br>给定一个函数 f(x)，它的首要求导对象是什么？就是它的输入值，是自变量 x。那 f(g(x)) 呢？即把g(x) 当作一个整体作为它的输入值，它的自变量。那么 g(x) 这个整体就是它的首要求导对象。因此，一个函数的求导对象是它的输入值，是它的自变量。弄清楚这一点，才能在求多元函数偏导的链式法则中游刃有余。</p>
</li>
<li><p>神经网络中各数据之间的关系——谁是谁的输入值<br>图 3.1 自下而上，每一个框是上面一个框的输入值，也即上面一个框中函数的自变量。这张图明确了神经网络中各数据之间的关系——谁是谁的输入值，图中表现得非常清楚。上段提到一个函数的求导对象是它的输入值，那么通过图 3.1 就能非常方便地使用链式法则，也能清楚地观察到 BP 算法的流程（后面一个小节会给出一个更具体的 BP 流程图）。</p>
</li>
<li><p>神经网络的学习(训练)流程(前馈传播和BP算法)<br>对照文首给出的图 1.1 神经网络的模型图，应该很容易理解图 3.1 的含义，它大致地展现了神经网络的学习（训练）流程。前馈传播算法自下而上地向上计算，最终可以得到 a(3)，进一步可以计算得到 J(Θ)。而 BP 算法自顶向下，层层求偏导，最终得到了每个参数的梯度值。下面一个小节将仔细介绍本文的主题，即 BP 算法的流程图解。</p>
</li>
</ul>
<img src="/2018/06/20/ml/Backpropagation/resources/0A913A35D7E650A4E2C65A6A7AF52C4F.jpg">
<h4 id="BP-算法的直观图解"><a href="#BP-算法的直观图解" class="headerlink" title="BP 算法的直观图解"></a>BP 算法的直观图解</h4><ul>
<li>BP 算法的流程在这张图中清晰可见：自顶向下（对应神经网络模型为自输出层向输入层）层层求偏导(核心：找准谁是谁的输入，对求偏导尤其重要)</li>
<li>所以 BP 算法即反向传播算法，<font color="red">就是自顶向下求代价函数$J(\theta)$ 对各个参数$\theta^l_{ij}$ 偏导($\frac{\partial}{\partial\theta^l_{ij}}J(\theta)$)的过程</font>，对应到神经网络模型中即自输出层向输入层层层求偏导<img src="/2018/06/20/ml/Backpropagation/resources/9005E13A2814A7CE44469B4204073DE9.jpg"></li>
<li><p>从这里可以看出：，为什么要使用BP算法：<font color="red"></font></p>
<ul>
<li>自输出层向输入层（即反向传播），逐层求偏导，在这个过程中逐渐得到各个层的参数梯度。</li>
<li>在反向传播过程中，使用$\delta(l)$ 保存了部分结果，从而避免了大量的重复运算。</li>
</ul>
</li>
<li><p>详细推导计算过程可以参考原文</p>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/18/ml/NeuralNetwork/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/18/ml/NeuralNetwork/" itemprop="url">NeuralNetwork</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-18T07:41:11+08:00">
                2018-06-18
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/机器学习基础/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习基础</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://github.com/yuancl/ml-algorithm/tree/master/exe4-NeuralNetwork" target="_blank" rel="noopener">习题代码:NeuralNetwork</a></p>
<h4 id="人脑神经元"><a href="#人脑神经元" class="headerlink" title="人脑神经元"></a>人脑神经元</h4><ul>
<li>人脑内就有很多神经元构成，神经元与神经元之间互相连接，传递电信号等<img src="/2018/06/18/ml/NeuralNetwork/resources/A304F5025E48BA4324FCD8DB562EBBC1.jpg">
</li>
</ul>
<h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><ul>
<li>基础结构：类似逻辑回归模型<img src="/2018/06/18/ml/NeuralNetwork/resources/A155266DFD2672B1850698451F99A310.jpg"></li>
<li><p>网络架构</p>
<ul>
<li>每个神经单元都是一个上图的基础结构，输出a是激活值,g是sigmoid激活函数</li>
<li>架构分为第一层(输入层)，第二层(隐藏层)….输出层<img src="/2018/06/18/ml/NeuralNetwork/resources/445D829784D90FD50DB465E3F95031CC.jpg"></li>
<li>前向计算：每个神经元都依次计算<img src="/2018/06/18/ml/NeuralNetwork/resources/DF4A1399852DE9F18094C55A08479F78.jpg">
</li>
</ul>
</li>
<li><p>直观理解网络含义</p>
<ul>
<li>每一层都是通过逻辑回归来完成，通过增加隐层的数量，或者通过增加一个隐藏中的单元数，来完成不同的功能，就能够得到更复杂的模型，完成更复杂的功能<font color="red">（简单理解为每个神经单位都是一个逻辑回归结构，都能够完成一个独立的功能）</font><ul>
<li>比如亦或问题<img src="/2018/06/18/ml/NeuralNetwork/resources/6B39D24F15F400C523F22D22183EC541.jpg">
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="处理多分类问题"><a href="#处理多分类问题" class="headerlink" title="处理多分类问题"></a>处理多分类问题</h4><ul>
<li>每个分类被一个节点判断出(像上一节类似，一个节点判断and，一个节点判断or，它们的下一层节点就能判断更复杂的内容)</li>
<li>多个分类，可以在一个神经网络中判断输出</li>
<li>逻辑回归时候，是把多分类问题，视为多个二分类问题处理的(多个$h_\theta^i(x)$)<img src="/2018/06/18/ml/NeuralNetwork/resources/3DB7E197A148553098F55835C55544B9.jpg">
</li>
</ul>
<h4 id="Cost-function"><a href="#Cost-function" class="headerlink" title="Cost function"></a>Cost function</h4><ul>
<li>和逻辑回归的cost function非常类似，其中m表示样本数，K表示神经网络最终输出的个数，正则化项包含了所有参数</li>
<li><img src="/2018/06/18/ml/NeuralNetwork/resources/B14FBC4B8353C514D5B36255A966E074.jpg">
</li>
</ul>
<h4 id="求最优解"><a href="#求最优解" class="headerlink" title="求最优解"></a>求最优解</h4><ul>
<li>求解算法最常用的当然也是使用的梯度下降，然后也可以使用深度学习中的<a href="https://yuancl.github.io/2018/09/10/dl/%E7%AC%AC%E4%BA%8C%E9%97%A8%E8%AF%BE-%E7%AC%AC%E4%BA%8C%E5%91%A8/" target="_blank" rel="noopener">动态梯度下降,RMProp,Adam</a>等</li>
<li>梯度的计算也可以使用最原始的方法，对每个参数求解梯度，但是由于神经网络中参数太多，原始方法效率太低，所以有了后面的<a href="https://yuancl.github.io/2018/06/20/ml/Backpropagation/" target="_blank" rel="noopener">Backpropagation算法</a></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/08/ml/Logistic Regression/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/08/ml/Logistic Regression/" itemprop="url">Logistic Regression</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-06-08T19:40:01+08:00">
                2018-06-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/机器学习基础/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习基础</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://github.com/yuancl/ml-algorithm/tree/master/exe2-LogisticRegression" target="_blank" rel="noopener">习题代码:Logistic Regression</a></p>
<h4 id="使用场景：解决分类问题"><a href="#使用场景：解决分类问题" class="headerlink" title="使用场景：解决分类问题"></a>使用场景：解决分类问题</h4><ul>
<li>Logistic Regression使用场景是解决分类问题<ul>
<li>二分类或者多分类,多分类模型(多次二分类或者后面学习到的softmax)</li>
<li>数据分布<ul>
<li>二分类模型中，线性分布：<img src="/2018/06/08/ml/Logistic%20Regression/resources/C491A76AE474D3A6527DE11CE6B5BB48.jpg"></li>
<li>也可能是非线性分布：<img src="/2018/06/08/ml/Logistic%20Regression/resources/34441106EC0A9630E1BA17393165253B.jpg">
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="逻辑回归模型"><a href="#逻辑回归模型" class="headerlink" title="逻辑回归模型"></a>逻辑回归模型</h4><ul>
<li>Hypothese：$h_\theta(x)=g(z)=\frac{1}{1+e^{-z}}$<br>Sigmoid:<img src="/2018/06/08/ml/Logistic%20Regression/resources/9DEA3236A7F3E3EE56B5E39F09094DFE.jpg">
所以：$y = \begin{bmatrix}<br>1(h_\theta(x) &gt;= 0.5)<br>\<br>0(h_\theta(x) &lt; 0.5)<br>\end{bmatrix}$</li>
<li>$z = \begin{bmatrix}<br>\theta^Tx = \theta_0+\theta_1x_1+\theta_2x_2…(数据线性分布)<br>\<br>x^2+y^2-1 (非线性分布，这里是圆形分布)<br>\end{bmatrix}$</li>
<li>Cost fun:<img src="/2018/06/08/ml/Logistic%20Regression/resources/51E29FDE06731C99AB2EAD9F6BFC5705.jpg">
<ul>
<li>推导：<img src="/2018/06/08/ml/Logistic%20Regression/resources/A33831535408D08F4745CEC4917E73A4.jpg">
<img src="/2018/06/08/ml/Logistic%20Regression/resources/3B4C93B55A4452695CDA033B1F77DA1A.jpg"></li>
</ul>
</li>
<li>梯度：<img src="/2018/06/08/ml/Logistic%20Regression/resources/6CCB2EBFDDAC0EEC7FE859C459F82619.jpg">
</li>
</ul>
<h4 id="条件概率角度理解"><a href="#条件概率角度理解" class="headerlink" title="条件概率角度理解"></a>条件概率角度理解</h4><ul>
<li>$h_\theta(x) = P(y=1|x;\theta) = 1-P(y=0|x;\theta)$</li>
<li>$P(y=1|x;\theta) + P(y=0|x;\theta) = 1$</li>
</ul>
<h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><ul>
<li>当数据分布为非线性的时候，如果使用多项式去拟合，那么很容易出现过拟合问题，这个时候就需要使用L1，L2正则化去处理</li>
<li>直观理解：正则化的根本目的就是将过拟合的参数调到合适大小<img src="/2018/06/08/ml/Logistic%20Regression/resources/2A2FB337EA2AB08BF7E63A23C3F2FC15.jpg"></li>
<li>cost_reg:<img src="/2018/06/08/ml/Logistic%20Regression/resources/FA37074C59A156386726A45623DE0E27.jpg"></li>
<li>梯度_reg:<img src="/2018/06/08/ml/Logistic%20Regression/resources/72EA1437BA2B8A620AA0A95846B077B8.jpg"></li>
<li>超参数$\lambda$对拟合程度和准确率等的影响<img src="/2018/06/08/ml/Logistic%20Regression/resources/4749D216F6C432BF5A66591AAEA97A13.jpg">
</li>
</ul>
<h4 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h4><ul>
<li>思想：对每个类别都会有一个$h_\theta(x)$对应：$h_\theta^i(x)$。输入x，获取使最大的$h_\theta^i(x)$的i值<img src="/2018/06/08/ml/Logistic%20Regression/resources/201EA81A2D0FD9F6D4EB13CD721C24E7.jpg"></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/6/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><a class="extend next" rel="next" href="/page/8/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/images/favicon.ico" alt="雷哥">
          <p class="site-author-name" itemprop="name">雷哥</p>
           
              <p class="site-description motion-element" itemprop="description">不积跬步无以至千里</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">72</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">18</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">16</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/yuancl" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-雷哥"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">雷哥</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Gemini
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
    <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  






  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
