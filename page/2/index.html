<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css">


  <meta name="keywords" content="Hexo, NexT">








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2">






<meta name="description" content="不积跬步无以至千里">
<meta property="og:type" content="website">
<meta property="og:title" content="雷哥的博客">
<meta property="og:url" content="http://yoursite.com/page/2/index.html">
<meta property="og:site_name" content="雷哥的博客">
<meta property="og:description" content="不积跬步无以至千里">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="雷哥的博客">
<meta name="twitter:description" content="不积跬步无以至千里">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '雷哥'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/2/">





  <title>雷哥的博客</title>
  














</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">雷哥的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/26/rs/总结篇-推荐算法总结/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/26/rs/总结篇-推荐算法总结/" itemprop="url">总结篇-推荐算法总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-26T07:30:21+08:00">
                2019-03-26
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/" itemprop="url" rel="index">
                    <span itemprop="name">推荐系统</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/推荐系统总结篇/" itemprop="url" rel="index">
                    <span itemprop="name">推荐系统总结篇</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h4><ul>
<li>自己总结<br><a href="https://yuancl.github.io/2019/08/23/ad/CTR模型演进/" target="_blank" rel="noopener">CTR模型演进</a><br><a href="https://yuancl.github.io/2019/08/23/ad/xDeepFM模型/" target="_blank" rel="noopener">xDeepFM模型</a></li>
<li>网络文章<br><a href="https://mp.weixin.qq.com/s/i8ClwpTGMB5cu0evrrZQGw" target="_blank" rel="noopener">算法粗略介绍：搜索与推荐中的深度学习匹配：推荐篇</a><br><a href="https://www.ibm.com/developerworks/cn/web/1103_zhaoct_recommstudy1" target="_blank" rel="noopener">探索推荐引擎内部的秘密</a><br><a href="https://zhuanlan.zhihu.com/c_188941548" target="_blank" rel="noopener">张俊林-推荐系统召回模型</a><br><a href="https://www.hrwhisper.me/machine-learning-fm-ffm-deepfm-deepffm/" target="_blank" rel="noopener">FM、FFM与DeepFM</a><br><a href="http://www.shuang0420.com/2017/03/13/论文笔记%20-%20Wide%20and%20Deep%20Learning%20for%20Recommender%20Systems/" target="_blank" rel="noopener">WDL论文笔记</a><br><a href="https://zhuanlan.zhihu.com/p/37562283" target="_blank" rel="noopener">ESMM</a><br><a href="https://mp.weixin.qq.com/s/cMr_fi9xs1BT5wFWL0ZLzw" target="_blank" rel="noopener">CTR模型演进</a></li>
</ul>
<h4 id="算法分类"><a href="#算法分类" class="headerlink" title="算法分类"></a>算法分类</h4><ul>
<li><p>分类方法1</p>
<ul>
<li>传统<ul>
<li>协同过滤</li>
<li>矩阵分解(MF)</li>
<li>因式分解机(FM)</li>
</ul>
</li>
<li>基于embedding(深度学习兴起后)<ul>
<li>word-embedding:skip-gram等</li>
<li>graph-embedding:DeepWalk,TransE等</li>
</ul>
</li>
<li>基于深度神经网络<ul>
<li>Deep &amp; Wide</li>
</ul>
</li>
</ul>
</li>
<li><p>分类方法2</p>
<ul>
<li>线性<ul>
<li>LR</li>
<li>在排序模型方面很多基于LR模型，现在很多都是基于深度学习来做，不同模型都有不同的应用场景，并不是单一使用一种场景。<font color="blue">LR模型利用人工特征工程</font>，相对于深度学习的优点是可以感知的，是可以debug的</li>
</ul>
</li>
<li>非线性<ul>
<li>FM,FFM,GBDT+LR,XGboost+LR</li>
<li>LR模型对于特征处理是线性的，利用Xgboost+LR或者GBDT+LR<font color="blue">由线性向非线性转化，能够做到多特征组合</font>，对推荐效果也有不同程度的提升</li>
</ul>
</li>
<li>神经网络<ul>
<li>DeepFM,Wide&amp;Deep</li>
<li>目前还有利用Wide&amp;Deep，可以从特征工程中解放出来，在特征选取方面不需要做很多工作,但是在调参方面工作量比较大</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/E3D0F465A0C9A650AFE3BEB09F26AAFA.jpg">
<h4 id="GBDT-LR"><a href="#GBDT-LR" class="headerlink" title="GBDT+LR"></a>GBDT+LR</h4><ul>
<li>LR特征工程很难，那能否自动完成呢？模型级联提供了一种思路，典型的例子就是Facebook 2014年的论文中介绍的通过GBDT（Gradient Boost Decision Tree）模型<font color="blue">解决LR模型的特征组合问题</font>。思路很简单，特征工程分为两部分<ul>
<li>一部分特征用于训练一个GBDT模型，把GBDT模型每颗树的叶子节点编号作为新的特征，加入到原始特征集中</li>
<li>再用LR模型训练最终的模型</li>
</ul>
</li>
<li>GBDT模型能够学习高阶非线性特征组合，对应树的一条路径（用叶子节点来表示）<ul>
<li><font color="blue">通常把一些连续值特征、值空间不大的categorical特征</font>都丢给GBDT模型</li>
<li>空间很大的ID特征（比如商品ID）留在LR模型中训练，<font color="red">既能做高阶特征组合又能利用线性模型易于处理大规模稀疏数据的优势</font><img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/1D843A0BFB0672616943E4C315E62417.jpg"></li>
</ul>
</li>
<li>树模型缺点<ul>
<li>基于树的模型适合<font color="blue">连续中低度稀疏数据，容易学到高阶组合</font></li>
<li>但是树模型却不适合学习高度稀疏数据的特征组合<ul>
<li>一方面高度稀疏数据的特征维度一般很高，这时基于树的模型学习效率很低，甚至不可行；</li>
<li>另一方面树模型也不能学习到训练数据中很少或没有出现的特征组合(比如训练数据完全没有特征x和特征y的训练数据出现)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="协同过滤"><a href="#协同过滤" class="headerlink" title="协同过滤"></a>协同过滤</h4><p><a href="https://www.ibm.com/developerworks/cn/web/1103_zhaoct_recommstudy1" target="_blank" rel="noopener">探索推荐引擎内部的秘密</a></p>
<ul>
<li>基于协同过滤的推荐可以分为三个子类：基于用户的推荐（User-based Recommendation），基于项目的推荐（Item-based Recommendation）和基于模型的推荐（Model-based Recommendation）</li>
<li>基于用户的协同过滤推荐<ul>
<li>基于用户的协同过滤推荐的基本原理是，根据所有用户对物品或者信息的偏好，发现与当前用户口味和偏好相似的“邻居”用户群，在一般的应用中是采用计算“K- 邻居”的算法；然后，基于这 K 个邻居的历史偏好信息，为当前用户进行推荐<img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/F711DB43A7FFBE6FE172AA4E89A3AA25.jpg"></li>
</ul>
</li>
<li>基于项目的协同过滤推荐<ul>
<li>基于项目的协同过滤推荐的基本原理也是类似的，只是说它使用所有用户对物品或者信息的偏好，发现物品和物品之间的相似度，然后根据用户的历史偏好信息，将类似的物品推荐给用户<img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/F7D855B58A3BA17FE77017B75EEAC6F1.jpg"></li>
</ul>
</li>
<li>基于协同过滤的推荐机制是现今应用最为广泛的推荐机制，它有以下几个显著的优点：<ul>
<li>它不需要对物品或者用户进行严格的建模，而且不要求物品的描述是机器可理解的，所以这种方法也是领域无关的。</li>
<li>这种方法计算出来的推荐是开放的，可以共用他人的经验，很好的支持用户发现潜在的兴趣偏好</li>
</ul>
</li>
<li>存在以下几个问题：<ul>
<li>方法的核心是基于历史数据，所以对新物品和新用户都有“冷启动”的问题。</li>
<li>推荐的效果依赖于用户历史偏好数据的多少和准确性。</li>
<li>在大部分的实现中，用户历史偏好是用稀疏矩阵进行存储的，而稀疏矩阵上的计算有些明显的问题，包括可能少部分人的错误偏好会对推荐的准确度有很大的影响等等。</li>
<li>对于一些特殊品味的用户不能给予很好的推荐。</li>
<li>由于以历史数据为基础，抓取和建模用户的偏好后，很难修改或者根据用户的使用演变，从而导致这个方法不够灵活。</li>
</ul>
</li>
</ul>
<h4 id="MF"><a href="#MF" class="headerlink" title="MF"></a>MF</h4><ul>
<li>MF（Matrix Factorization，矩阵分解）模型是个在推荐系统领域里资格很深的老前辈协同过滤模型了。核心思想是通过两个低维小矩阵（一个代表用户embedding矩阵，一个代表物品embedding矩阵）的乘积计算，来模拟真实用户点击或评分产生的大的协同信息稀疏矩阵，本质上是编码了用户和物品协同信息的降维模型<img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/D766787AD9BABE291715A7FA78CA0483.jpg"></li>
<li>当训练完成，每个用户和物品得到对应的低维embedding表达后，如果要预测某个 $User_i 对 Item_j 的评分的时候，只要它们做个内积计算 〈User_i,Item_j 〉$ ，这个得分就是预测得分<img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/7C24B08BD0F51553C0BC91BCA91A3AD3.jpg">
</li>
</ul>
<h4 id="SVD-MF-FISM-SVD"><a href="#SVD-MF-FISM-SVD" class="headerlink" title="SVD,MF,FISM,SVD++"></a>SVD,MF,FISM,SVD++</h4><ul>
<li>CF本质就是解决矩阵填充问题<img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/2BD3B5476445D05EDCC0CEBD67A69CEF.jpg"></li>
<li>矩阵填充一般是用SVD分解来解决<img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/249CBC7DD767EC261F4F6E690BFF123F.jpg">
<ul>
<li>SVD就是在解决以下问题(Loss)<img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/749D83FC437F6C72D57EBC9593D4C313.jpg"></li>
<li>svd有以下缺点：<ul>
<li>missing data(就是没打分的，占比99%)和observed data（观测到的、已打分的）有一样的权重</li>
<li>没有加正则，容易过拟合</li>
</ul>
</li>
<li><font color="purple">注意和MF的区别，这边没没有使用embedding，user，item词嵌入的概念。而是用数学的方法求解的</font></li>
</ul>
</li>
<li>MF<ul>
<li>user和item分别用一个embedding表示，然后用户对item的偏好程度用这两个embedding的内积表示<img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/6244EB6F4E2E5E8FF59AEF1F74BFDE70.jpg"></li>
<li>使用L2-loss（其它loss也可以）和正则：<img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/46C9358BD6ADCED970682B6604BF028F.jpg"></li>
</ul>
</li>
<li>FISM(Factored Item Similarity Model)<ul>
<li><font color="blue">用user作用过的item的embedding的和来表示user</font>，item用另一套embedding下的一个embedding来表示，最后两者的内积表示user对该item的偏好<ul>
<li>这个模型也叫item-based的CF，因为把括号里的东西展开后，<font color="blue">其实就是找用户作用过的item和item[j]的相似度</font><img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/E213592DC17DC84F8E8B69B950B774CE.jpg"></li>
</ul>
</li>
</ul>
</li>
<li>SVD++<ul>
<li>很简单，另外用一个user的embedding，和上述的FISM的方法，融合来表示user。这曾经是netflix比赛中连续三年效果最好的单模型<img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/8113DCE6ED7DFF4BF45247EB2E34189F.jpg">
</li>
</ul>
</li>
</ul>
<h4 id="FM"><a href="#FM" class="headerlink" title="FM"></a>FM</h4><p><a href="https://zhuanlan.zhihu.com/c_188941548" target="_blank" rel="noopener">张俊林-推荐系统召回模型</a></p>
<ul>
<li><p>从LR到SVM再到FM模型<br>LR模型简单易懂，但不能捕获更高维的特征，比如特征组合这类特征，所以不能很好拟合较复杂的场景(欠拟合)</p>
<img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/232BB3F732AD29ECA8A9CDB044A3B36F.jpg">
</li>
<li><p>加入特征组合(提升模型复杂度)</p>
<ul>
<li>虽然这个模型看上去貌似解决了二阶特征组合问题了，但是它有个潜在的问题：它对组合特征建模，泛化能力比较弱</li>
<li>尤其是在大规模稀疏特征存在的场景下，这个毛病尤其突出，比如CTR预估和推荐排序</li>
<li>这些场景的最大特点就是特征的大规模稀疏。所以上述模型并未在工业界广泛采用</li>
<li><font color="blue">主要原因：由于数据稀疏，$x_i x_j$的权重参数$w_{i,j}$很多为0，导致不能学习</font><ul>
<li>在训练数据里两个特征并未同时在训练实例里见到过，意味着 $x_i  and x_j$ 一起出现的次数为0，如果换做SVM的模式，是无法学会这个特征组合的权重的<img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/0405C4C1759F61DB053CCDB696B99C7F.jpg">
</li>
</ul>
</li>
</ul>
</li>
<li><p>FM模型<br>对于因子分解机FM来说，<font color="blue">利用特征的嵌入方式，最大的特点是对于稀疏的数据具有很好的学习能力。现实中稀疏的数据很多</font></p>
<ul>
<li>觉得可以看做是LR和MF的组合</li>
<li><font color="blue">核心：这本质上是在对特征进行embedding化表征</font></li>
<li><p>和SVM模型最大的不同，在于<font color="blue">特征组合权重</font>的计算方法。FM对于每个特征，学习一个大小为k的一维向量，于是，两个特征 $x_i 和 x_j 的特征组合的权重值，通过特征对应的向量 v_i 和 v_j 的内积 &lt;v_i,v_j&gt;$来表示。</p>
<img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/5D30E7FAEDECBCA1AD37754267567059.jpg">
</li>
<li><p>为什么模型泛华能力强，能够解决稀疏数据的问题</p>
<ul>
<li>在训练数据里两个特征并未同时在训练实例里见到过，意味着 $x_i  and x_j$ 一起出现的次数为0，如果换做SVM的模式，是无法学会这个特征组合的权重的。</li>
<li>但是因为FM是学习单个特征的embedding，并不依赖某个特定的特征组合是否出现过，所以只要特征 x_i 和其它<font color="blue">任意特征组合</font>出现过，那么就可以学习自己对应的embedding向量<ul>
<li>于是，尽管 $x_i  and x_j$ 这个特征组合没有看到过，但是在预测的时候，如果看到这个新的特征组合，因为 $x_i 和 x_j$ 都能学会自己对应的embedding，所以可以通过内积算出这个新特征组合的权重。</li>
<li>这是为何说FM模型泛化能力强的根本原因 <img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/7C5B35BBD0C7325E782FFC16AC89535C.jpg">
</li>
</ul>
</li>
</ul>
</li>
<li><p>上式只是对数据的拟合函数，还要根据具体问题比如回归还是分类设以不同的Loss方法进行求解。以及数学求解的方法需要另参考其他资料</p>
</li>
<li><p>算法的效率</p>
<ul>
<li>粗略的看<br>从FM的原始数学公式看，因为在进行二阶（2-order）特征组合的时候，假设有n个不同的特征，那么二阶特征组合意味着任意两个特征都要进行交叉组合，所以可以直接推论得出：FM的时间复杂度是n的平方。但是如果故事仅仅讲到这里，FM模型是不太可能如此广泛地被工业界使用的。因为现实生活应用中的n往往是个非常巨大的特征数，如果FM是n平方的时间复杂度，那估计基本就没人带它玩了</li>
<li>数学公式演示改进(细节也可以参考论文资料等)<br>FM如今被广泛采用并成功替代LR模型的一个关键所在是：它可以通过数学公式改写，把表面貌似是 $O(k<em>n^2 ) 的复杂度降低到 O(k</em>n)$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="MF到FM的转换理解"><a href="#MF到FM的转换理解" class="headerlink" title="MF到FM的转换理解"></a>MF到FM的转换理解</h4><ul>
<li>本质上，MF模型是FM模型的特例，<font color="blue">MF可以被认为是只有User ID 和Item ID这两个特征Fields的FM模型</font>，MF将这两类特征通过矩阵分解，来达到将这两类特征embedding化表达的目的</li>
<li>而FM则可以看作是MF模型的进一步拓展，除了User ID和Item ID这两类特征外，很多其它类型的特征，都可以进一步融入FM模型里，它将所有这些特征转化为embedding低维向量表达，并计算任意两个特征embedding的内积，就是特征组合的权重<img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/270EA9D3CF21C4F363002E84A5E9D766.jpg">
</li>
</ul>
<h4 id="FFM-Field-aware-FM"><a href="#FFM-Field-aware-FM" class="headerlink" title="FFM(Field-aware FM)"></a>FFM(Field-aware FM)</h4><ul>
<li><p>核心思想</p>
<ul>
<li>FM模型的某个特征，在和任意其它特征域的特征进行组合求权重的时候，共享了同一个embedding特征向量</li>
<li>FFM模型是做得更细腻一些，在做特征组合的时候使用的embedding不同的特征向量。</li>
<li>这意味着，如果有F个特征域，那么每个特征由FM模型的一个k维特征embedding，拓展成了（F-1）个k维特征embedding</li>
<li>这个就是Field-aware的深层含义吧</li>
</ul>
</li>
<li><p>算法效率问题</p>
<ul>
<li>FM模型可以通过公式改写，把本来看着是n的平方的计算复杂度，降低到 $O(k*n)$ </li>
<li>而FFM无法做类似的改写，所以它的计算复杂度是 $O(k*n^2$ ) ，这明显在计算速度上也比FM模型慢得多</li>
<li>所以，无论是急剧膨胀的参数量，还是变慢的计算速度，无论从哪个角度看，相对FM模型，FFM模型是略显笨重的。</li>
<li>正因为FFM模型参数量太大，所以在训练FFM模型的时候，很容易过拟合，需要采取早停等防止过拟合的手段</li>
</ul>
</li>
<li><p>数学公式</p>
<ul>
<li>其中$f_i和f_j$分别代表第i个特征和第j个特征所属的field<img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/0A702A17B2566CC94E06A8D70A4EF0E5.jpg">
</li>
</ul>
</li>
</ul>
<h4 id="WDL"><a href="#WDL" class="headerlink" title="WDL"></a>WDL</h4><ul>
<li><p>Wide and Deep Learning<br>简单来说，<font color="blue">人脑就是一个不断记忆（memorization）并且归纳（generalization）的过程</font>，而这篇论文的思想，就是将<font color="red">宽线性模型（Wide Model，用于记忆，下图左侧）和深度神经网络模型（Deep Model，用于归纳，下图右侧）结合</font>，汲取各自优势形成了 Wide &amp; Deep 模型用于推荐排序</p>
<img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/EE2B4D289C05BA7DAE1AB4E0D88A0D10.jpg">
</li>
<li><p>Wide Model<br>要理解的概念是 Memorization，主要是学习特征的共性或者说相关性，产生的推荐是和已经<font color="blue">有用户行为的物品直接相关的物品</font></p>
<ul>
<li>通过线性模型 + 特征交叉。所带来的Memorization以及记忆能力非常有效和可解释。但是Generalization（泛化能力）需要更多的人工特征工程</li>
<li>用的模型是 逻辑回归(logistic regression, LR)，LR 的优点就是简单(simple)、容易规模化(scalable)、可解释性强(interpretable)。LR 的特征往往是二值且稀疏的(binary and sparse)</li>
<li>总结一下，宽度模型的输入是用户安装应用(installation)和为用户展示（impression）的应用间的向量积（叉乘），模型通常训练 one-hot 编码后的二值特征<ul>
<li>缺点：这种操作不会归纳出训练集中未出现的特征对</li>
</ul>
</li>
</ul>
</li>
<li><p>Deep Model<br>要理解的概念是 Generalization，可以理解为<font color="blue">相关性的传递(transitivity)，会学习新的特征组合</font>，来提高推荐物品的多样性，或者说提供泛化能力(Generalization)</p>
<font color="purple">我的理解就是要用DL能够学习各种未出现的特征组合能力，特征模型的泛化能力</font><br>- DNN几乎不需要特征工程。通过对低纬度的dense embedding进行组合可以学习到更深层次的隐藏特征<br>  - 泛化往往是通过学习 <font color="blue">low-dimensional dense embeddings</font> 来探索过去从未或很少出现的新的特征组合来实现的<br>  - 缺点是有点over-generalize（过度泛化）<br>    - 当query-item矩阵是稀疏并且是high-rank的时候（比如user有特殊的爱好，或item比较小众），很难非常效率的学习出低维度的表示。这种情况下，大部分的query-item都没有什么关系。但是dense embedding会导致几乎所有的query-item预测值都是非0的，<font color="blue">这就导致了推荐过度泛化，会推荐一些不那么相关的物品</font><br>  - 相反，linear model却可以通过<font color="red">cross-product transformation</font>来记住这些<font color="blue">exception rules</font>，而且仅仅使用了非常少的参数<br>- <font color="blue">所以WDL结合LR，这点和 LR 正好互补，因为 LR 只能记住很少的特征组合，能够帮助识别Deep Model中《所有 query-item pair 非零的预测》情况</font>
</li>
<li><p>两者区别与互补</p>
<ul>
<li>Memorization趋向于更加保守，推荐用户之前有过行为的items(只认识出现过的)</li>
<li>generalization更加趋向于提高推荐系统的多样性（diversity）</li>
<li>所以WDL结合LR，这点和 LR 正好互补，因为 LR 只能记住很少的特征组合，能够帮助识别Deep Model中《所有 query-item pair 非零的预测》情况</li>
</ul>
</li>
<li><p>GP推荐系统的整体架构</p>
<ul>
<li>由两个部分组成,检索系统(或者说候选生成系统）和排序系统(排序网络)。</li>
<li>首先，用 检索(retrieval) 的方法对大数据集进行初步筛选，返回最匹配 query 的一部分物品列表，这里的检索通常会结合采用 机器学习模型(machine-learned models) 和 人工定义规则(human-defined rules) 两种方法。从大规模样本中召回最佳候选集之后</li>
<li>再使用 排序系统 对每个物品进行算分、排序，分数 P(y|x)。WDL 就是用在排序系统中<img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/3122D99113950432B7C98420BD44982B.jpg">
</li>
</ul>
</li>
</ul>
<h4 id="DeepFM"><a href="#DeepFM" class="headerlink" title="DeepFM"></a>DeepFM</h4><ul>
<li><p>FM模型可以用神经网络进行表示</p>
<img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/2B105C5A04F5C58A4B7AB7B7113F3A85.jpg">
<ul>
<li>这里需要<font color="red">理解Sparse Feature层和Embeddings层的表示，如何和原始的FM模型等价</font><ul>
<li>Sparse Feature层中维数数据可能不一样，有可能是多维的one-hot向量，比如分类的数据，如果是连续的数值型，那么就是本身<ul>
<li>即使各个field的维度是不一样的，但是它们embedding后长度均为k</li>
<li><font color="blue">也就是说one-hot只有一位为非0，其实就是通过矩阵相乘后就表示和原始的FM模型是等价了</font><img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/E65E503C55862E88CCD45DBDB8D3B395.jpg"></li>
</ul>
</li>
<li>最终表现就是这个模型<img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/AE8DD79D27067ABD87C903A33567DD8B.jpg">
</li>
</ul>
</li>
</ul>
</li>
<li><p>DeepFM的模型如下图<br><font color="red">共享整个embedding层，进行多层网络训练，提取高阶特征</font></p>
<ul>
<li>左边就是刚才将的FM模型的神经网络表示</li>
<li>右边的则为deep部分，<font color="blue">为全连接的网络，用于挖掘高阶的交叉特征</font>。整个模型共享embedding层，最后的结果就是把FM部分和DNN的部分做sigmoid<br>$Y=sigmoid(Y_{FM}+Y_{DNN})$<img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/3DE3419525ED615318B841FF4E302F5A.jpg">
</li>
</ul>
</li>
<li><p>DeepFM的结构中包含了因子分解机部分以及深度神经网络部分，分别负责低阶特征的提取和高阶特征的提取</p>
</li>
<li><p>与图像或者语音这类输入不同，图像语音的输入一般是连续而且密集的，然而用于CTR的输入一般是<font color="blue">及其稀疏的</font>。Embedding嵌入层就是这个作用</p>
</li>
<li><p>类比DeepFFM<br>类似于FFM对于FM模型来说，划分了field，对于不同的field内积时采用对应的隐向量。同样可以把DeepFM进行进化为DeepFFM，<font color="blue">即将每一个field embedding为m个维度为k的隐向量</font>（m为field的个数）</p>
</li>
<li><p>类比WDL</p>
<ul>
<li>简单比较就是将WDL的LR部分替换为FM，FM比LR在特征组合上优势比较大，但也比LR复杂很多</li>
</ul>
</li>
</ul>
<h4 id="DCN"><a href="#DCN" class="headerlink" title="DCN"></a>DCN</h4><ul>
<li><p>背景：<br>FM、DeepFM和Inner-PNN都是通过原始特征隐向量的内积来构建vector-wise的二阶交叉特征，这种方式有两个主要的缺点：</p>
<ul>
<li>必须要穷举出所有的特征对，即任意两个field之间都会形成特征组合关系，而过多的组合关系可能会引入无效的交叉特征，给模型引入过多的噪音，从而导致性能下降。</li>
<li>二阶交叉特征有时候是不够的，好的特征可能需要更高阶的组合。虽然DNN部分可以部分弥补这个不足，但bit-wise的交叉关系是晦涩难懂、不确定并且不容易学习的</li>
<li>有没有可能引入更高阶的vector-wise的交叉特征，同时又能控制模型的复杂度，避免产生过多的无效交叉特征呢</li>
</ul>
</li>
<li><p>概念</p>
<ul>
<li>bit-wise VS vector-wise<br>假设隐向量的维度为3维，如果两个特征(对应的向量分别为(a1,b1,c1)和(a2,b2,c2)的话）在进行交互时，交互的形式类似于f(w1 <em> a1 </em> a2,w2 <em> b1 </em> b2 ,w3 <em> c1 </em> c2)的话，此时我们认为特征交互是发生在元素级（bit-wise）上。如果特征交互形式类似于 f(w <em> (a1 </em> a2 ,b1 <em> b2,c1 </em> c2))的话，那么我们认为特征交互是发生在特征向量级（vector-wise）</li>
<li>特征的显示隐式交叉（explicitly VS implicitly）<br>显式的特征交互和隐式的特征交互。以两个特征为例xi和xj，在经过一系列变换后，我们可以表示成 wij <em> (xi </em> xj)的形式，就可以认为是显式特征交互，否则的话，是隐式的特征交互</li>
</ul>
</li>
<li><p>网络架构<br>DCN模型以一个嵌入和堆叠层(embedding and stacking layer)开始，接着并列连一个cross network和一个deep network，接着通过一个combination layer将两个network的输出进行组合<br>完整的网络模型如图：</p>
<img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/4DA07739DB6622637DB826CFD12F5613.jpg">
</li>
<li><p>嵌入和堆叠层<br>将sparse feature转换为Embedding vec，然后combin Dense feature，形成$x_0$</p>
</li>
<li>交叉网络<br>交叉网络（cross network）的核心思想是以有效的方式应用显式特征交叉。交叉网络由交叉层组成，每个层具有以下公式:<img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/E14BAF5ABBD04A90B9F78C1CB2864234.jpg">
<img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/26B5ADC8BBB12A67B4124B16ADF0013C.jpg">
<ul>
<li>模型本质意义<img src="/2019/03/26/rs/总结篇-推荐算法总结/resources/B9124CC5B7C98905A4B63572B20C7DBF.jpg">
</li>
</ul>
</li>
</ul>
<h4 id="xDeepFM模型"><a href="#xDeepFM模型" class="headerlink" title="xDeepFM模型"></a>xDeepFM模型</h4><p><a href="https://yuancl.github.io/2019/08/23/ad/xDeepFM模型/" target="_blank" rel="noopener">xDeepFM模型</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/15/rl/强化学习基础算法对比总结/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/15/rl/强化学习基础算法对比总结/" itemprop="url">强化学习基础算法对比总结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-03-15T22:11:15+08:00">
                2019-03-15
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/强化学习/" itemprop="url" rel="index">
                    <span itemprop="name">强化学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://yuancl.github.io/categories/机器学习/强化学习/" target="_blank" rel="noopener">我的强化学习文章</a><br><a href="https://blog.csdn.net/Hansry/article/details/80808097" target="_blank" rel="noopener">网络:强化学习算法汇总1</a><br><a href="https://blog.csdn.net/hansry/article/details/80829127" target="_blank" rel="noopener">网络:强化学习算法汇总2</a></p>
<h4 id="不同角度分类"><a href="#不同角度分类" class="headerlink" title="不同角度分类"></a>不同角度分类</h4><ul>
<li>Model free/Model based<ul>
<li>Mode based:比如基于MDP(马尔科夫处理过程)，必须知道很多的环境状态，比如状态转移矩阵,reward等</li>
<li>Model free:不用知道环境的一些信息</li>
</ul>
</li>
<li>基于价值还是基于策略<ul>
<li>基于价值，比如V(s),Q(s,a)等，只能处理离散的行为，状态。得到最优值，对连续行为不好处理(Q-Learning,Sarsa,DQN及其变种)</li>
<li>基于策略，能够解决连续行为状态的场景，比如对状态行为进行建模，能够输出该状态行为的价值数值，所以就可以根据所以行为价值值选择最优的进行迭代(Actor-critic,A3C,DDPG等)</li>
<li>只是基于价值的模型(Critic only)，只会对整个序列完成后才给reward，对中间好的action损失较多，而基于价值和策略的模型就不会(例如Actor-critic),从Loss函数就能看出来<img src="/2019/03/15/rl/强化学习基础算法对比总结/resources/7A5A633D6270A244F2F0BC809FF24466.jpg"></li>
</ul>
</li>
<li>是否使用函数近似<ul>
<li>使用类似神经网络模型逼近V(s),Q(s,a)的真实值，本质就是对参数的求解，一定程度上能够解决连续状态行为的问题</li>
</ul>
</li>
<li>迭代更新策略(MC,TD,DP)<ul>
<li>基于采样的MC,TD,区别是迭代关注到后面的几步问题,实用性讲大部分都是基于TD的</li>
<li>DP不基于采样，所以状态都需要探索</li>
</ul>
</li>
<li>一套架构与否(行为策略和目的策略是否一致)<ul>
<li>行为策略和目的策略分开，或两套深度学习模型，典型的如Actor-critic </li>
</ul>
</li>
</ul>
<h4 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h4><ul>
<li><p>所属范畴<br>Model free/基于价值/TD/Off-Policy  </p>
</li>
<li><p>Q-Table(状态-行为表)<br>会有一个状态-行为表，来存储对应的价值，使用的时候只需要查表</p>
<img src="/2019/03/15/rl/强化学习基础算法对比总结/resources/FA2597ED524C0B0B01F2501C2CFA8C7A.jpg">
</li>
<li><p>更新Q-Table</p>
<ul>
<li>注意这里是选择s2状态下的最大行为值max $Q(s_2,a)$<img src="/2019/03/15/rl/强化学习基础算法对比总结/resources/69AF8FAD960EF5E9B78057E0E9ECF094.jpg">
</li>
</ul>
</li>
<li><p>算法伪代码</p>
<img src="/2019/03/15/rl/强化学习基础算法对比总结/resources/FF39F31C3A0DBB2EE7363ECFDD4A778D.jpg">
</li>
</ul>
<h4 id="Sarsa"><a href="#Sarsa" class="headerlink" title="Sarsa"></a>Sarsa</h4><ul>
<li>所属范畴<br>Model free/基于价值/TD and MC(Sarsa($\lambda$))/On-policy</li>
<li><p>和QLearning区别<br>Q-learning 在从状态s−&gt;s′的时候，考虑到的为 max Q(s′,a′), 在s′状态中选取 a′时，永远考虑的是最大。而对于Sarsa 而言，在从s′状态中选取 a′时，会采取与从s选取a 的策略一样，即采用 greedy 或者 ϵ−greedy</p>
<img src="/2019/03/15/rl/强化学习基础算法对比总结/resources/0222EC07D3FE2473155650099A69B238.jpg">
<img src="/2019/03/15/rl/强化学习基础算法对比总结/resources/D42F10E3F36A9105D5A55E2262C8E55B.jpg">
</li>
<li><p>算法伪代码</p>
<img src="/2019/03/15/rl/强化学习基础算法对比总结/resources/5504404F4F95CA41BCB2D2F638CA1333.jpg">
</li>
<li><p>Sarsa($\lambda$)</p>
<img src="/2019/03/15/rl/强化学习基础算法对比总结/resources/15578715F45608C37432FD7A4DA06887.jpg">
</li>
</ul>
<h4 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h4><ul>
<li><p>所属范畴<br>Model free/价值近似函数/TD</p>
</li>
<li><p>和QLearning比较<br>在Q-Learning中，我们提到了用Q表来存储当前状态s1下采取的动作action的值（value，在Q表中也称为Q值）。但是在实际过程中，一个状态s1到下一状态s2，这里的s2可能有很多不同的情况，这将会导致Q表存储的值会很多，不仅占内存，且在搜索的时候也是十分耗时的</p>
<ul>
<li>用神经网络来替代行为策略和目的策略<ul>
<li>输入为状态s和动作a，得到所有的动作值（Q值）</li>
<li>只输入状态值，然后输出所有的动作值<img src="/2019/03/15/rl/强化学习基础算法对比总结/resources/458CEACC21DCDCA5B1DBE0672EDBE177.jpg">
</li>
</ul>
</li>
</ul>
</li>
<li><p>伪代码</p>
<img src="/2019/03/15/rl/强化学习基础算法对比总结/resources/B8A8E886CEF92ACAB6702E24C4E4FFA0.jpg">
</li>
<li><p>变种Double DQN， Dueling DQN等</p>
<ul>
<li>DDQN<br>使用借鉴的思路，使用两个架构相同的近似价值函数<ul>
<li>其中一个用来根据策略生成交互行为并随 时频繁参数 (θ)</li>
<li>另一个则用来生成目标价值</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Policy-Gradients"><a href="#Policy-Gradients" class="headerlink" title="Policy Gradients"></a>Policy Gradients</h4><ul>
<li>可以解决连续行为空间的情况</li>
<li>强化学习是一个通过奖惩来学习正确行为的机制，有学习奖惩值，根据自己认为的高价值选择行为的，如Q Learning、Deep Q Network 等。也有不通过分析奖励值，直接输出行为的方法，如Policy Gradient。Policy Gradient 直接输出动作的最大好处就是，能够在一个连续区间内挑选动作，而基于值的，往往是在所有动作中计算值，然后选择值最高的那个行为</li>
</ul>
<h4 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h4><ul>
<li><p>所属范畴<br>Model free/(基于策略 and 价值/策略近似函数)/TD</p>
</li>
<li><p>Actor Critic 为类似于Policy Gradient 和 Q-Learning 等以值为基础的算法的组合</p>
</li>
<li>Actor Critic 结合了 Policy Gradient（Actor）和 Function Approximation Critic）。Actor 基于概率选择行为，Critic 基于 Actor 的行为评判行为的得分，Actor 根据 Critic 的评分修改选择行为</li>
<li>逻辑<ul>
<li>其中Actor 类似于Policy Gradient，以状态s为输入，神经网络输出动作actions，并从在这些连续动作中按照一定的概率选取合适的动作action。 </li>
<li>Critic 类似于 Q-Learning 等以值为基础的算法，由于在Actor模块中选择了合适的动作action，通过与环境交互可得到新的状态s_, 奖励r，将状态 s_作为神经网络的输入，得到v_，而原来的状态s通过神经网络输出后得到v。 <img src="/2019/03/15/rl/强化学习基础算法对比总结/resources/330CF01329E0F79CDFBF7B3AFABE1701.jpg">
<img src="/2019/03/15/rl/强化学习基础算法对比总结/resources/873B2AE82C380A0DF20A408BAB193BE0.jpg">
<img src="/2019/03/15/rl/强化学习基础算法对比总结/resources/FCB9ECAB27436A64F27F0C3564176EDC.jpg"></li>
</ul>
</li>
<li>QAC算法(最基本的基于行为价值 Q 的 Actor-Critic 算法)<img src="/2019/03/15/rl/强化学习基础算法对比总结/resources/C5285EA40617696A6C7AB03D0049EB03.jpg">
</li>
</ul>
<h4 id="A3C"><a href="#A3C" class="headerlink" title="A3C"></a>A3C</h4><ul>
<li>所属范畴<br>Model free/(基于策略 and 价值/策略近似函数)/TD</li>
<li>A3C 其实采用了Actor-Critic 的形式，但是引入了并行计算的概念。为了训练一对Actor 和 Critic，我们将Actor 和 Critic 复制成多份，然后放在不同的核中进行训练。其中需要声明一个主要的Actor-Critic (global)，不断从多个副本中更新的参数进行学习，获得新的参数，同时副本中的参数也不断从 Actor-Critic (global) 中获得并更新。</li>
<li>A3C 是Google DeepMind 提出的一种解决 Actor Critic 不收敛问题的算法。A3C会创建多个并行的环境，让多个拥有副结构的 agent 同时在这些并行环境上更新主结构中的参数。并行中的 agent 们互不干扰，而主结构的参数更新受到副结构提交更新的不连续性干扰，所以更新的相关性被降低，收敛性提高<img src="/2019/03/15/rl/强化学习基础算法对比总结/resources/956A445EB3C04AB5746C141CC6B281B6.jpg">
</li>
</ul>
<h4 id="DDPG-Deep-Deterministic-Policy-Gradient"><a href="#DDPG-Deep-Deterministic-Policy-Gradient" class="headerlink" title="DDPG(Deep Deterministic Policy Gradient)"></a>DDPG(Deep Deterministic Policy Gradient)</h4><ul>
<li>所属范畴<br>Model free/(基于策略 and 价值/策略近似函数)/TD</li>
<li>DDPG算法能较为稳定地解决连续行为空间下强化学习问题</li>
<li>DDPG用到的神经网络是怎么样的？它其实有点类似于Actor-Critic，也需要有基于策略Policy 的神经网络 和 基于价值 Value 的神经网络，但是为了体现DQN的思想，每种神经网络我们都需要再细分成俩个。Policy Gradient 这边有估计网络和现实网络，估计网络用来输出实时的动作，而现实网络则是用来更新价值网络系统的。再看看价值系统这边，我们也有现实网络和估计网络，他们都在输出这个状态的价值，而输入端却有不同，状态现实网络这边会拿着当时actor施加的动作当做输入。在实际运用中，DDPG的这种做法的确带来了更有效的学习过程<img src="/2019/03/15/rl/强化学习基础算法对比总结/resources/61940048D6BE768A829798B0ACDD87BF.jpg"></li>
<li>噪音函数<ul>
<li>该算法在学习阶段通过在确定性的行为基础上增加一个噪声函数而实现在确定性行为周围的<font color="blue">小范围内探索</font></li>
<li>该算法还为 Actor 和 Critic 网络各备份了一套参数用来计算行为价值的期待值以<font color="blue">更稳定地提升 Critic 的策略指导水平</font>。使用备份参数的网络称为目标网络，其对应的参数每次更新的幅度很小</li>
</ul>
</li>
<li>伪代码<img src="/2019/03/15/rl/强化学习基础算法对比总结/resources/10C881AA627B5435DA098EC77BEC80A7.jpg">
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/02/28/rs/高级推荐模型之二：协同矩阵分解/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/28/rs/高级推荐模型之二：协同矩阵分解/" itemprop="url">高级推荐模型之二：协同矩阵分解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-28T07:20:21+08:00">
                2019-02-28
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/" itemprop="url" rel="index">
                    <span itemprop="name">推荐系统</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/推荐模型简介/" itemprop="url" rel="index">
                    <span itemprop="name">推荐模型简介</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="为什么需要协同矩阵分解"><a href="#为什么需要协同矩阵分解" class="headerlink" title="为什么需要协同矩阵分解"></a>为什么需要协同矩阵分解</h4><p>矩阵分解的核心就是通过矩阵，这个二维的数据结构，来对用户和物品的交互信息进行建模（如何融入更多信息）</p>
<ul>
<li>因为其二维的属性，矩阵往往只能对用户的某一种交互信息直接进行建模，这就带来很大的局限性</li>
<li>思路一，就是通过建立显式变量和隐变量之间的回归关系，从而让矩阵分解的核心结构可以获得更多信息的帮助。</li>
<li>思路二，则是采用分解机这样的集大成模型，从而把所有的特性，都融入到一个统一的模型中去。</li>
<li>思路三，就是我们这周已经讲到的，利用张量，把二维的信息扩展到 N 维进行建模</li>
</ul>
<h4 id="如何组织更多的二元关系"><a href="#如何组织更多的二元关系" class="headerlink" title="如何组织更多的二元关系"></a>如何组织更多的二元关系</h4><p>除了用户和物品这样很明显的二元关系以外，还有其他也很明显的二元关系，如何把这些二元关系有效地组织起来，就变成了一个有挑战的任务</p>
<ul>
<li>在前面的思路里面可以看到，我们似乎需要选择一个主要的关系来作为这个模型的基础框架，<font color="blue">然后把其他的信息作为补充</font>。在这样两类关系中，选择哪一个作为主要关系，哪一个作为补充关系，就显得有一点困难了</li>
<li>这也就让研究人员想出了协同矩阵分解的思路</li>
</ul>
<h4 id="协同矩阵分解的基本思路"><a href="#协同矩阵分解的基本思路" class="headerlink" title="协同矩阵分解的基本思路"></a>协同矩阵分解的基本思路</h4><ul>
<li>协同矩阵分解的基本思路其实非常直观，那就是有多少种二元关系，就用多少个矩阵分解去建模这些关系</li>
<li><p>如果协同(如果让这多个矩阵产生关系?)</p>
<ul>
<li>理论上基于矩阵分解得到的隐变量，相互是独立的，没有关系的</li>
<li>我们必须有其他的假设。这里的其他假设就是，两组不同的用户隐变量其实是一样的。也就是说，我们假设，或者认定，用户隐变量在用户与用户的关系中，以及在用户与物品的关系中，<font color="blue">是同一组用户隐变量在起作用</font><ul>
<li>说得直白一些，我们认定从两个矩阵分解出来的两组来自同一个因素（这里是用户）的<font color="blue">隐变量是完全一样的</font>。用更加学术的语言来说，这就是将两组矩阵分别投影到了相同的用户空间和物品空间</li>
</ul>
</li>
</ul>
</li>
<li><p>优点<br>我们使用“相同隐变量”这样的假设，可以把这些关系都串联起来，然后减少了总的变量数目，同时也让各种关系互相影响</p>
</li>
<li>缺点<ul>
<li>使用同样的一组隐变量去表达所有的同类关系，这样的假设存在一定的局限性，比较难找到</li>
<li>不同关系的数据量会有很大的差距。比如，用户和物品关系的数据总量可能要比用户与用户的多。所以，由于用户和物品关系的数据多，两个矩阵分解用的同一组用户隐变量，很可能会更多地解释用户和物品的部分，从而造成了学到的隐变量未必能够真正表达所有的关系</li>
</ul>
</li>
</ul>
<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>从概念上来看，协同矩阵分解和张量分解之间有怎样的关系？是不是所有的张量分解都可以化为多个协同矩阵分解呢</p>
<ul>
<li>我的理解是ok的</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/02/27/rs/高级推荐模型之一：张量分解模型/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/27/rs/高级推荐模型之一：张量分解模型/" itemprop="url">高级推荐模型之一：张量分解模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-27T08:29:20+08:00">
                2019-02-27
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/" itemprop="url" rel="index">
                    <span itemprop="name">推荐系统</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/推荐模型简介/" itemprop="url" rel="index">
                    <span itemprop="name">推荐模型简介</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="为什么需要张量"><a href="#为什么需要张量" class="headerlink" title="为什么需要张量"></a>为什么需要张量</h4><ul>
<li>矩阵分析的核心思想<ul>
<li>用矩阵这种数据结构来表达用户和物品的相互关系<ul>
<li>这里，我们一般谈论的都是一些最简单的关系，例如评分、点击、购买等（本文我们依然只是讨论评分）</li>
</ul>
</li>
<li><font color="red">在这种二元的模式下，矩阵就是最好的表达用户和物品之间关系的数据结构</font></li>
</ul>
</li>
<li>如何表示更多信息(上下文)<ul>
<li>背景<br>在真实的场景中，用户和物品的关系以及产生这种关系的周围环境是复杂的。一个矩阵并不能完全描述所有的变量。<ul>
<li>例如，用户对于某个物品的评分是发生在某个地点、某个时间段内的。这种所谓的“上下文关系”（Context）往往会对评分产生很大影响。遗憾的是，<font color="blue">一个矩阵无法捕捉这样的上下文关系</font></li>
</ul>
</li>
<li>基于回归的矩阵分解和分解机方法解决<br>我们之前讨论过的“基于回归的矩阵分解”和“分解机”，本质上都是在某种程度上绕开这个问题<ul>
<li>采用的方法就是，<font color="blue">依然用矩阵来表达二元关系，但是把其他信息放进隐变量中</font>，或者是采用基于信息的推荐系统的思路来得到相关信息的建模</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>除了这种思路，还有没有别的方法，<font color="red">可以把上下文关系融入到对用户和物品的建模中去呢</font></p>
<ul>
<li>张量<br>从本质上来说，张量就是矩阵的推广。我们可以这么理解:<ul>
<li>矩阵是对二维关系建模的一种工具；在二维关系中，用户和物品的评分是唯一能够被建模的变量；</li>
<li>而张量，就是对 N维关系的一种建模。而到了 N 维关系中，理论上，我们可以对任意多种上下文关系进行建<ul>
<li>比如，我们刚才提到的时间，就可以组成一个三维的张量，分别为用户、物品和时间。然后，在这个三维的张量中，每一个单元代表着某一个用户对于某一个物品在某一个时间段的评分</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="基于张量分解的推荐模型"><a href="#基于张量分解的推荐模型" class="headerlink" title="基于张量分解的推荐模型"></a>基于张量分解的推荐模型</h4><p>分解多维向量，我理解和之前学习的分解矩阵思想类似：分解为多个矩阵的表示方式</p>
<ul>
<li><p>CP 分解（CANDECOMP/PARAFAC）</p>
<ul>
<li>CP 分解是把一个三维张量分解为三个矩阵。具体来说，比如我们的三维张量是 N 维用户乘以 M 维的物品乘以 R 维的时间段。那么，分解出来的三个矩阵就分别为 N 维乘以 K 维的用户矩阵，M 维乘以 K 维的物品矩阵，以及 R 维乘以 K 维的时间矩阵。</li>
<li>这三个矩阵中的每一个向量都代表某一个用户、某一个物品和某一个时间段。K 在这里是一个参数，类似于矩阵分解中的隐变量的维度，我们也就可以把这个 K 理解成为隐变量的维度</li>
<li>那么在原始的三维张量中，<font color="blue">某一个元素就是这三个矩阵的某三个向量对应元素乘积相加的结果</font></li>
<li>CP 分解的一大好处就是，<font color="blue">分解出来的三个矩阵的隐变量维度是一样的</font>，这也就减少了需要调整的参数的个数</li>
</ul>
</li>
<li><p>HOSVD 分解（High Order Singular Value decomposition）</p>
<ul>
<li>含义<ul>
<li>这种分解和 CP 分解最大的不同就是分解出来的三个矩阵的维度不再一样<ul>
<li>也就是说，在我们之前的例子中，用户矩阵的维度可以是 N 乘以 A，物品矩阵的维度是 M 乘以 B，时间段矩阵的维度是 R 乘以 C。当然，这样就无法还原之前的 N 乘以 M 乘以 R 的三维张量了</li>
<li>于是在技术上，还需要乘以一个 A 乘以 B 乘以 C 的小张量才能对原始数据进行复原。</li>
</ul>
</li>
<li>所以，通俗地讲，HOSVD 分解就是把一个三维的张量，分解成为三个矩阵和一个更小的张量的乘积</li>
</ul>
</li>
<li>优缺点<ul>
<li>好处自然就是给不同的数据以不同的自由度，因为不再限制用户、物品和时间段都必须有一样的维度。</li>
<li>缺点是有了更多的“超参数”需要调整</li>
</ul>
</li>
<li>损失函数<br>在一般的分解过程中，我们可以定义“<strong>平方差</strong>”（Squared Loss），也就是原始数值和预测数值之间的平方差来作为损失函数</li>
</ul>
</li>
</ul>
<h4 id="求解张量分解"><a href="#求解张量分解" class="headerlink" title="求解张量分解"></a>求解张量分解</h4><ul>
<li>随机梯度下降法（SGD, Stochastic Gradient Descent），也就是把张量的分解问题看作是一个一般的优化问题</li>
<li>另外一种方法，也是在矩阵分解中可以使用的，叫作 ALS（Alternating Least Square）方法<ul>
<li>这种方法则是在优化每一轮的时候，按住所有其他的矩阵变量不动，单独优化一个变量</li>
</ul>
</li>
</ul>
<h4 id="问题：从概念上来看，用张量分解对上下文信息进行建模的最大问题是什么"><a href="#问题：从概念上来看，用张量分解对上下文信息进行建模的最大问题是什么" class="headerlink" title="问题：从概念上来看，用张量分解对上下文信息进行建模的最大问题是什么"></a>问题：从概念上来看，用张量分解对上下文信息进行建模的最大问题是什么</h4><ul>
<li>张量的稀疏性</li>
<li>模型更负责，不易求解</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/02/27/rs/基于隐变量的模型之三：分解机/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/27/rs/基于隐变量的模型之三：分解机/" itemprop="url">基于隐变量的模型之三：分解机</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-27T07:19:29+08:00">
                2019-02-27
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/" itemprop="url" rel="index">
                    <span itemprop="name">推荐系统</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/推荐模型简介/" itemprop="url" rel="index">
                    <span itemprop="name">推荐模型简介</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="矩阵分解和基于回归的隐变量模型存在哪些问题"><a href="#矩阵分解和基于回归的隐变量模型存在哪些问题" class="headerlink" title="矩阵分解和基于回归的隐变量模型存在哪些问题"></a>矩阵分解和基于回归的隐变量模型存在哪些问题</h4><ul>
<li>发展脉络 <ul>
<li>首先，矩阵分解主要解决了两个问题，那就是从一个大矩阵降维到两个小矩阵，并且寄希望这两个小矩阵能够抓住用户和物品的相关度。<ul>
<li>然而，单纯的矩阵分解无法融入很多用户和物品的特性</li>
</ul>
</li>
<li>这就引导我们开发出了基于回归的矩阵分解。<ul>
<li>所谓的回归部分，也就是从显式特性出发，建立从显式特性到隐变量之间关系的流程，从而使我们能够把更多的信号放进模型中。</li>
<li>在一定程度上，基于回归的隐变量模型实现了把显式变量和隐变量结合的目的<ul>
<li>但是这类模型的学习过程非常麻烦。实际上，因为这类模型复杂的训练流程，其在实际应用中并不常见。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>那么，有没有其他思路来统一显式变量和隐变量的处理方式呢？</p>
<h4 id="分解机"><a href="#分解机" class="headerlink" title="分解机"></a>分解机</h4><ul>
<li><p>核心原理</p>
<ul>
<li>核心就是认为<font color="red">需要预测的变量（这里我们依然讨论评分）是所有显式变量的一个回归结果</font><ul>
<li>分解机直接借鉴了这一点，也就是说，<font color="blue">分解机的输入是所有的显式变量</font></li>
</ul>
</li>
</ul>
</li>
<li><p>个人理解</p>
<ul>
<li>这种推荐模型本质可以看做是个回归模型：需要预测的变量（这里我们依然讨论评分）是所有显式变量的一个回归结果 </li>
<li><font color="purple">分解机本质就是增加了更多的特征，比如各个显示变量的两两乘积生成新特征</font></li>
<li>适当增加模型复杂度，泛化性强：结合了隐变量，比如显示变量的两两乘积用隐变量的乘积来表示(一种显示变量就用隐变量vector表示)</li>
</ul>
</li>
<li><p>基本思想</p>
<ul>
<li>第一步:把用户的年龄和物品的种类直接当作特性输入到模型中</li>
<li>第二步:分解机是把这两个特性的数值进行乘积，当作一个新的特性，然后进一步处理这种两两配对的关系<ul>
<li>增加了特征(<font color="blue">显示变量两两配对</font>)<br>分解机在对待显式变量的手法上更进了一步，那就是不仅直接对显式变量进行建模，还对显示变量的两两关系进行建模。当然，在原始的论文中，分解机其实还可以对更加高维的关系进行建模，我们这里局限在两两关系上<ul>
<li><font color="blue">把原始特性进行两两配对是构建模型的一种重要的方法，特别是对于非深度学习模型，需要自己做特征工程的模型</font></li>
<li>两两配对问题<ul>
<li>一个问题就是<font color="blue"><strong>特性空间会急速增长</strong></font></li>
<li>另一个更严重的问题就是，如果我们的单独特性中，有一些是“类别特性”（Categorical Feature），那么在两两配对之后就会产生大量的 0，从而变成一个巨大的<font color="blue">稀疏矩阵</font></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="如何建模-如何解决上面问题"><a href="#如何建模-如何解决上面问题" class="headerlink" title="如何建模(如何解决上面问题)"></a>如何建模(如何解决上面问题)</h4><ul>
<li>分解机利用了<font color="blue">矩阵分解的降维思路</font></li>
<li>我们不对一个稀疏矩阵直接建模，而是把这个稀疏矩阵分解之后再进行建模</li>
<li>就是先假定，<font color="blue">所有特性都对应一个隐变量向量，两个显式特性的乘积是两个特性的隐变量的点积</font><ul>
<li>也就是说，我们把两个显式特性的乘积分解为了两个向量的乘积。这样，我们就不需要直接表示原来的稀疏矩阵。<p></p></li>
</ul>
</li>
<li>在这样的思路下，分解机成功地把隐变量和显式变量结合到了一起<ul>
<li>当我们的显式特性仅仅是用户 ID 和物品 ID 的时候，分解机的表达退回了最原始的矩阵分解</li>
<li>也就是说，矩阵分解其实可以表达成为特性的两两作用矩阵的分解</li>
</ul>
</li>
<li>在原始的论文中，作者还用分解机模拟了好几种流行的模型</li>
<li>虽然也是为了建立从显式特性到隐变量的关系，但是对比基于回归的矩阵分解而言，分解机的训练过程大大简化了。在实际应用中，我们经常使用“<strong>随机梯度下降</strong>”（SGD, Stochastic Gradient Descent）来对分解机直接进行求解</li>
</ul>
<h4 id="思考问题"><a href="#思考问题" class="headerlink" title="思考问题"></a>思考问题</h4><ul>
<li>分解机能够解决“冷启动”的问题吗<ul>
<li>个人认为是可以解决的：因为本质就是一个回归模型，由隐变量得到显变量的映射。然后映射到指标(比如说点击率，评论得分)</li>
<li>所以在冷启动问题上，对缺失的变量可以看其分布，然后拿到期望值，这样来处理</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/02/26/rs/基于隐变量的模型之二：基于回归的矩阵分解/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/26/rs/基于隐变量的模型之二：基于回归的矩阵分解/" itemprop="url">基于隐变量的模型之二：基于回归的矩阵分解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-26T08:15:19+08:00">
                2019-02-26
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/" itemprop="url" rel="index">
                    <span itemprop="name">推荐系统</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/推荐模型简介/" itemprop="url" rel="index">
                    <span itemprop="name">推荐模型简介</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="基础矩阵分解问题"><a href="#基础矩阵分解问题" class="headerlink" title="基础矩阵分解问题"></a>基础矩阵分解问题</h4><ul>
<li>第一，矩阵分解的矩阵仅仅是对用户和物品的喜好进行了“编码”（Encode），但在融合多种不同的推荐元素方面，表现却很一般</li>
<li>第二，矩阵分解的核心是学习用户的隐向量和物品的隐向量。原则上，这两类隐向量的学习仅能通过训练过程获得。<ul>
<li>我们无法获得新来用户或者新来物品的隐向量了，因为这些用户和物品并不在训练集里</li>
<li>冷启动问题<br>在推荐系统中，这种情况就叫作不能处理“冷启动”（Cold Start）问题，也就是不能处理“冷”用户和“冷”物品。在这样的场景下，直接使用矩阵分解就会有问题</li>
</ul>
</li>
</ul>
<h4 id="基于回归的矩阵分解"><a href="#基于回归的矩阵分解" class="headerlink" title="基于回归的矩阵分解"></a>基于回归的矩阵分解</h4><ul>
<li>首先，有一组用户特性和物品特性来表述每一个用户和物品。这些特性不是隐变量，是显式表达的特性<ul>
<li>用户特性比如用户的年龄、性别、经常出现的地区、已经表达了喜好的类别等</li>
<li>物品特性比如物品的种类、描述等等</li>
<li>这两组显式的特性就是为了解决我们刚才说的第一个问题(融入更多元素)，矩阵分解无法抓住更多的信号。</li>
</ul>
</li>
<li>现在我们有两个独立的部分<ul>
<li>一个是基于矩阵分解的部分，这一部分是分解一个已知的评分矩阵，从而学习到用户和物品的隐向量</li>
<li>另外一个部分，就是用户特性和物品特性</li>
</ul>
</li>
<li>关联两部分<br>用户的隐向量，其实是从用户的显式特性变换而来的<ul>
<li><font color="blue">我们建立一个从显式特性到隐向量的回归模型，使得隐向量受到两方面的制约：从评分矩阵的分解得来的信息和从显式特性回归得来的信息</font></li>
</ul>
</li>
<li>不怕冷启动<br>不再怕“冷启动”了。或者说，在有一部分“冷启动”的情况下，这样的模型可以处理得更好。原因就是我们使用了显示特性来回归隐向量</li>
<li>贝叶斯角度理解<br>我们还可以从贝叶斯的角度来理解基于回归的矩阵分解。把用户的隐向量和物品的隐向量看作是两个随机变量。我们可以认为这些随机变量加上了先验概率分布。只不过，这个先验概率分布的均值不是我们经常使用的 0，而是一个以回归函数的结果为均值的高斯分布，这个回归函数就是我们由显式特性得到的。本质上，我们认为显示特性的某种变换成为了隐向量的先验信息</li>
</ul>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><ul>
<li>第一，我们简要介绍了矩阵分解的一些问题</li>
<li>第二，我们详细介绍了基于回归的矩阵分解的基本思路，以及这样的模型如何解决了传统矩阵分解关于“冷启动”的难题</li>
<li>第三，如何学习的问题，需要查阅其他资料</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/02/26/rs/基于隐变量的模型之一：矩阵分解/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/26/rs/基于隐变量的模型之一：矩阵分解/" itemprop="url">基于隐变量的模型之一：矩阵分解</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-26T07:10:19+08:00">
                2019-02-26
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/" itemprop="url" rel="index">
                    <span itemprop="name">推荐系统</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/推荐模型简介/" itemprop="url" rel="index">
                    <span itemprop="name">推荐模型简介</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="什么是隐变量"><a href="#什么是隐变量" class="headerlink" title="什么是隐变量"></a>什么是隐变量</h4><ul>
<li>就是“隐藏的变量”或者叫“隐藏的参数”，这里主要是指我们假定实际的数据是由一系列的隐含变量产生的</li>
<li>我们通过模型的假设，知道隐变量之间的关系，但暂时并不知道隐变量的取值。因此需要通过“推断”（Inference）过程来确定隐变量的实际取值</li>
<li><p>隐变量往往还带有“统计分布”（Distribution）的假设。什么意思呢？就是隐变量之间，或者隐变量和显式变量之间的关系，我们往往认为是由某种分布产生的</p>
<ul>
<li>举例:高斯混合模型<br>高斯混合模型假设数据是由多个不同的高斯分布产生的，每一个高斯分布有自己的均值和方差<ul>
<li>对于每一个数据点，我们就有一个隐含的变量，来表达当前这个数据点究竟来自哪一个高斯分布</li>
<li>两个高斯分布的均值和方法其实也不知道</li>
</ul>
</li>
</ul>
<p>高斯混合模型，几乎是最简单的隐变量模型，但也给我们了展示了使用隐变量模型对数据建模的灵活性以及训练的复杂性</p>
</li>
</ul>
<h4 id="矩阵分解作为隐变量模型"><a href="#矩阵分解作为隐变量模型" class="headerlink" title="矩阵分解作为隐变量模型"></a>矩阵分解作为隐变量模型</h4><ul>
<li>关系矩阵(用户，物品，评分)<br>在推荐系统中，有一种最普遍的数据表达，那就是用户和物品的交互，比如评分、点击等等<ul>
<li>这里我们用评分作为一般性的讨论。对于每一个用户，如果我们用一个向量来表达其对所有可能物品的评分，那么把所有用户的向量堆积起来，就可以得到一个矩阵</li>
<li>这个矩阵的每一行代表一个用户，每一列代表一个物品，每一个交叉的元素代表某一个用户对于某一个商品的评分</li>
<li>特点<br>这个矩阵的数据其实非常稀少。因为在一个现实的系统中，一个用户不可能对所有的物品都进行评分</li>
<li>补全任务<br>我们的任务其实就是根据有评分的用户物品交互，对那些还没有评分信息的物品进行预测。如果我们能够“补全”（Complete）整个矩阵里的其他元素</li>
</ul>
</li>
</ul>
<h4 id="如何补全-矩阵分解"><a href="#如何补全-矩阵分解" class="headerlink" title="如何补全(矩阵分解)"></a>如何补全(矩阵分解)</h4><ul>
<li><p>核心<br>我们可以看到，矩阵分解的核心其实就是刚才的假设，用隐向量来表达用户和物品，他们的乘积关系就成为了原始的元素。不同的假设可以得到不同的矩阵分解模型</p>
</li>
<li><p>效果<br>在这样的一个假设下，一个原本 1 百万乘以 1 万的矩阵就可以被分解为 1 百万乘以 100 的用户矩阵和 100 乘以 1 万的物品矩阵的乘</p>
</li>
<li><p>降维<br>原本是需要对整个矩阵，也就是 1 百万乘以 1 万个数据进行建模，而现在缩减到了两个小一些的矩阵 1 百万乘以 100 和 100 乘以 1 万</p>
<ul>
<li>对比来看，之前对于每一个用户，我们需要一万个元素来表达用户对于这一万个物品的评分喜好；现在，对每个用于仅仅需要保存 100 个元素</li>
<li>这一百个元素的数值并不代表任何意义。从这个层面上理解矩阵分解，也就能够帮助我们知道，这个方法其实是一种“<strong>降维</strong></li>
</ul>
</li>
</ul>
<h4 id="如何学习"><a href="#如何学习" class="headerlink" title="如何学习"></a>如何学习</h4><ul>
<li>目标函数<br>前面讲到，矩阵里的每一个元素来自于<font color="blue">两个隐向量的点积</font>，我们就可以利用这一点来构造一个目标函数。<ul>
<li>利用<strong>最小二乘法</strong>的原理（Least Square）来拟合求解这些隐向量</li>
<li>这个目标函数其实就是说，这两个隐向量的点积一定要与我们观测到的矩阵数值相近。这里的“相近”是用这两个数值的误差，在这里也就是<strong>平方差</strong>（Square Error）来衡量的</li>
</ul>
</li>
</ul>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><ul>
<li>第一，我们简要介绍了隐变量模型的基本原理</li>
<li>第二，我们详细介绍了矩阵分解作为隐变量模型的假设和原理</li>
<li>第三，我们简要地讨论了如何求解矩阵分解里的隐变量</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/02/25/rs/简单推荐模型之三 基于内容信息的推荐模型/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/25/rs/简单推荐模型之三 基于内容信息的推荐模型/" itemprop="url">简单推荐模型之三:基于内容信息的推荐模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-25T07:32:10+08:00">
                2019-02-25
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/" itemprop="url" rel="index">
                    <span itemprop="name">推荐系统</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/推荐模型简介/" itemprop="url" rel="index">
                    <span itemprop="name">推荐模型简介</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><ul>
<li>对于基于流行度预测的推荐的问题<ul>
<li>推荐结果不是个性化的。因为流行度预测是一种全局的预测，每个人得到的推荐结果是一样的</li>
</ul>
</li>
<li>而协同过滤的问题<ul>
<li>强烈依赖相似用户以及相似物品的定义</li>
<li>而且对于新用户或者新物品来说有数据稀缺的问题</li>
</ul>
</li>
</ul>
<p>因此，在实际应用中，往往不能在整个系统中单独使用协同过滤</p>
<h4 id="基于内容信息的推荐系统"><a href="#基于内容信息的推荐系统" class="headerlink" title="基于内容信息的推荐系统"></a>基于内容信息的推荐系统</h4><ul>
<li>定义<br>基于内容信息的推荐系统，其实就是<font color="blue">用特征（Feature）来表示用户、物品以及用户和物品的交互，从而能够把推荐问题转换成为监督学习任务</font></li>
</ul>
<h4 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h4><ul>
<li>特征工程(物品信息)<br>需要将用户和物品的所有信号用特征来表示<ul>
<li>物品的文本信息<ul>
<li>用 TF-IDF 的方法来形成文本向量。当然，因为文本信息的噪声相对比较大，并且数据维度也比较大（维度等于文本所对应语言的词汇量），很多时候我们都寻求降低这部分数据的维度，降低到一个固定的维度。这种时候，很多所谓“降维”的工具就很有必要了</li>
<li>用“<strong>话题模型</strong>”（Topic Model）对文本进行降维的。也就是说，我们针对每一个文字描述都可以学习到一个话题的分布，这个分布向量可能是 50 维、100 维等等</li>
<li>使用各种“<strong>词嵌入向量</strong>”（Word Embedding）的方法来为文字信息降维，从而能够使用一个固定的维度来表达文字信息</li>
</ul>
</li>
<li>物品的类别信息<ul>
<li>直接获取分类，比如新闻的分类，或者通过其他渠道得到直接的分类</li>
<li>使用ML的手段，来得到分类</li>
</ul>
</li>
<li>知识图谱挖掘信息<ul>
<li>利用知识图谱进行知识的深挖</li>
<li>举个例子，某一篇新闻文章是关于美国总统特朗普的，于是这篇文章可能就会自动被打上美国总统、美国政治等其他标签。这种通过一些原始的信息来进一步推断更加丰富的知识信息，也是重要的物品类别特征的处理工作</li>
</ul>
</li>
</ul>
</li>
<li>特征工程(用户信息)<ul>
<li>最基础、最首要的肯定是用户的基本特性，包括性别、年龄、地理位置</li>
<li>还有围绕这三个特性发展出来的三大种类的特性。比如，不同性别在文章点击率上的差异，不同年龄层在商品购买上的差异，不同地理位置对不同影视作品的喜好等</li>
<li>我们可以为用户进行画像（Profiling）<ul>
<li>有显式的用户画像，比如用户自己定义的喜好，或者用户自己认为不愿意看到的物品或者类别</li>
<li>隐式的：通过用户的“隐反馈”（Implicit Feedback），来对用户的喜好进行建模</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h4><p>对于究竟在哪种场景中使用什么样的目标函数，这依然是当前的一个主要研究方向</p>
<ul>
<li>基于评分<br>纯粹的基于评分（Rating）的协同过滤推荐系统一样，我们可以设置监督学习的目标函数是<strong>拟合评分</strong>。当然，已经有很多学者指出评分并不是推荐系统的真正目标。</li>
<li>基于点击率，购买率<br>在实际系统中比较常见的目标函数有点击率和购买率，也有一些相对比较复杂的目标函数，比如预测用户在某一个物品上的停留时长</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li>背景：基于流行度和协同过滤的缺点</li>
<li>如何构建特征工程，包括物品和用户</li>
<li>目标函数的确定很重要</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/02/24/rs/简单推荐模型之二 基于相似信息的推荐模型/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/24/rs/简单推荐模型之二 基于相似信息的推荐模型/" itemprop="url">简单推荐模型之二:基于相似信息的推荐模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-24T15:30:10+08:00">
                2019-02-24
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/" itemprop="url" rel="index">
                    <span itemprop="name">推荐系统</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/推荐模型简介/" itemprop="url" rel="index">
                    <span itemprop="name">推荐模型简介</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="什么是相似信息的推荐模型"><a href="#什么是相似信息的推荐模型" class="headerlink" title="什么是相似信息的推荐模型"></a>什么是相似信息的推荐模型</h4><ul>
<li>定义<br>相似信息的推荐模型又叫<strong>“临近”（Neighborhood）模型</strong>。顾名思义，就是我们希望利用临近、或者相似的数据点来为用户推荐。</li>
<li>协同过滤<br>临近模型的内在假设是推荐系统中著名的“<strong>协同过滤</strong>”（Collaborative Filtering）<ul>
<li><strong>相似的用户可能会有相似的喜好，相似的物品可能会被相似的人所偏好</strong>。</li>
<li>于是，如果我们能够定义怎么寻找相似的用户或者相似的物品，那么我们就可以利用这些类别的人群或者物品来给用户进行推荐。<p></p></li>
</ul>
</li>
<li>举例<br>A,B都看了战狼2，B还看了红海，那么就给A推荐红海<br>思考过程：<ul>
<li>第一，联系用户 A 和用户 B 的是他们都看过《战狼 2》。这就帮助我们定义了 A 和 B 是相似用户</li>
<li>第二，我们的假定是，相似的用户有相似的观影偏好，于是我们就直接把 B 的另外一个观看过的电影《红海行动》拿来推荐给了 A</li>
<li>意义：</li>
<li>这两个步骤其实就很好地解释了“协同过滤”中“协同”的概念，意思就是<font color="blue">相似的用户互相协同，互相过滤信息</font></li>
<li><strong>“协同过滤”从统计模型的意义上来讲，其实就是“借用数据”，<font color="blue">在数据稀缺的情况下帮助建模</font></strong>。掌握这个思路是非常重要的建模手段</li>
<li>在用户 A 数据不足的情况下，我们挖掘到可以借鉴用户 B 的数据<ul>
<li>因此，我们其实是把用户 A 和用户 B“聚类”到了一起，认为他们代表了一个类型的用户。</li>
<li>当我们把对单个用户的建模<font color="red">抽象上升到某一个类型的用户</font>的时候，这就把更多的数据放到了一起</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="基于相似用户的协同过滤"><a href="#基于相似用户的协同过滤" class="headerlink" title="基于相似用户的协同过滤"></a>基于相似用户的协同过滤</h4><p>如何才能够比较系统地定义这样的流程呢？</p>
<ul>
<li>首先，问题被抽象为我们需要估计用户 I 针对一个没有“触碰过”（这里指点击、购买、或者评分等行为）的物品 J 的偏好<ul>
<li>第一步，我们需要构建一个用户集合，这个用户集合得满足两个标准：<ul>
<li>第一，这些用户需要已经触碰过物品 J，这是与用户 I 的一大区别；</li>
<li>第二，这些用户在其他的行为方面需要与用户 I 类似</li>
</ul>
</li>
<li>然后进行打分<ul>
<li>简单的做法<br>首先，我们已经得到了所有和 I 相似的用户对 J 的打分。那么，一种办法就是，直接用这些打分的平均值来预估 J 的评分。也就是说，如果我们认为这个相似集合都是和用户 I 相似的用户，那么他们对 J 的偏好，我们就认为是 I 的偏好。显然这是一个很粗糙的做法</li>
<li>改进方法<ul>
<li>采用加权平均的做法<br>也就是说，和用户 I 越相似的用户，我们越倚重这些人对 J 的偏好</li>
<li>我们也需要对整个评分进行一个修正<ul>
<li>虽然这个相似集合的人都对 J 进行过触碰，但是每个人的喜好毕竟还是不一样的。比如有的用户可能习惯性地会<font color="blue">对很多物品有很强的偏好</font>。因此，仅仅是借鉴每个人的偏好，而忽略了这些用户的偏差，这显然是不够的。所以，<font color="blue">我们需要对这些评分做这样的修正，那就是减去这些相似用户对所有东西的平均打分</font>，也就是说，我们需要把这些用户本人的<font color="blue">偏差</font>给去除掉</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>方法总结：<br>综合刚才说的两个因素，可以得到一个更加合适的打分算法，那就是，用户 I 对物品 J 的打分来自两个部分：<ul>
<li>一部分是 I 的平均打分</li>
<li>另外一部分是 I 对于 J 的一个在平均打分之上的补充打分<ul>
<li>这个补充打分来自于刚才我们建立的相似用户集，是这个相似用户集里每个用户对于 J 的补充打分的一个<font color="blue">加权平均<ul>
<li>权重依赖于这个用户和 I 的相似度</li>
<li>每个用户对于 J 的补充打分是他们对于 J 的直接打分减去他们自己的平均打分</li></ul></font></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>


<h4 id="相似信息的构建"><a href="#相似信息的构建" class="headerlink" title="相似信息的构建"></a>相似信息的构建</h4><p>几个关键要素：</p>
<ul>
<li>我们怎么来定义两个用户是相似的<ul>
<li>一种最简单的办法，就是计算两个用户对于他们都偏好物品的“<strong>皮尔森相关度</strong>”（Pearson Correlation）</li>
<li>皮尔森相关度是针对每一个“两个用户”都同时偏好过的物品，看他们的偏好是否相似，这里的相似是用乘积的形式出现的。当两个偏好的值都比较大的时候，乘积也就比较大</li>
</ul>
</li>
<li>设定一些“阈值”来筛选刚才所说的相关用户集合<ul>
<li>我们可以设置最多达到前 K 个相似用户（比如 K 等于 100 或者 200）</li>
</ul>
</li>
<li>加权平均里面的权重问题<ul>
<li>一种权重，就是直接使用两个用户的相似度，也就是我们刚计算的皮尔森相关度<ul>
<li>当然，这里有一个问题，如果直接使用，我们可能会过分“相信”有一些相关度高但自身数据也不多的用户</li>
<li>所以我们可以把皮尔森相关度乘以一个系数，这个系数是根据自身的偏好数量来定的</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li>协同过滤<br>相似的用户可能会有相似的喜好，相似的物品可能会被相似的人所偏好</li>
<li>基于相似用户协同过滤<ul>
<li>问题被抽象为我们需要估计用户 I 针对一个没有“触碰过”（这里指点击、购买、或者评分等行为）的物品 J 的偏好</li>
<li>假设已经构建了这样的用户组，然后就是对需要推荐的物品进行打分，有分为简单的平均打分，和加权打分等</li>
</ul>
</li>
<li>相似信息的构建(皮尔森相似度，设置阈值构建用户集合)</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/02/23/rs/简单推荐模型之一 基于流行度的推荐模型/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/23/rs/简单推荐模型之一 基于流行度的推荐模型/" itemprop="url">简单推荐模型之一:基于流行度的推荐模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-23T19:30:13+08:00">
                2019-02-23
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/" itemprop="url" rel="index">
                    <span itemprop="name">推荐系统</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/推荐模型简介/" itemprop="url" rel="index">
                    <span itemprop="name">推荐模型简介</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="最简单的流行度估计"><a href="#最简单的流行度估计" class="headerlink" title="最简单的流行度估计"></a>最简单的流行度估计</h4><ul>
<li><p>定义<br>什么是基于流行度（Popularity-based）？通俗地讲，就是什么内容吸引用户，就给用户推荐什么内容</p>
<ul>
<li>隐含假设：就是物品质量的好坏和流行度成正比例关系</li>
</ul>
</li>
<li><p>流行度常见影响因素<br>不能简单看流量或者点击的绝对量来评价</p>
<ul>
<li>时间<br>比如用户对新闻的观看时间一般是有规律的，所以会影响绝对数据</li>
<li>位置<br>物品显示的位置也会影响绝对数据</li>
</ul>
</li>
<li><p>如何衡量</p>
<ul>
<li>采用比值Ratio,或者某种可能性<ul>
<li>比如点击率，但是点击率估计所需要的数据依赖会收到时间和位置的偏差的影响，所以如何构建无偏差数据呢？</li>
</ul>
</li>
</ul>
</li>
<li><p>构建无偏差数据(点击率)<br>本身就是一个负责的问题，这里举简单的解放办法：</p>
<ul>
<li>EE算法(Exploitation &amp; Exploration)(epsilon贪心算法)<br>就是将流量分为两部分，一部分随机显示物品，然后基于这部分流量计算流行度，第二部分跟进这个流行度来显示物品</li>
</ul>
</li>
</ul>
<h4 id="对点击率建模"><a href="#对点击率建模" class="headerlink" title="对点击率建模"></a>对点击率建模</h4><p>如果从数学上对点击率建模，其实可以把一个物品在显示之后是否被点击看成是一个“<strong>伯努利随机变量</strong>”，于是对点击率的估计，就变成了对一个<font cloor="blue">伯努利分布参数估计的过程</font></p>
<ul>
<li>最大似然估计(Maximum Likelihood Estimation)<ul>
<li>定义<br>就是说，<font color="blue">希望找到参数的取值可以最大限度地解释当前的数据</font><ul>
<li>这个估计的数值就是某个物品当前的点击总数除以被显示的次数。通俗地讲，如果我们显示某个物品 10 次，被点击了 5 次，那么在最大似然估计的情况下，点击率的估计值就是 0.5</li>
</ul>
</li>
<li>弊端<br>如果我们并没有显示当前的物品，那么，最大似然估计的分母就是 0；如果当前的物品没有被点击过，那么分子就是 0<ul>
<li>在这两种情况下，最大似然估计都无法真正体现出物品的流行度</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="高级流行度估计"><a href="#高级流行度估计" class="headerlink" title="高级流行度估计"></a>高级流行度估计</h4><p>我们从统计学的角度来讲了讲，如何利用<font color="blue">最大似然估计法来对一个伯努利分布所代表的点击率的参数进行估计</font>。这里面的第一个问题就是刚才我们提到的分子或者分母为 0 的情况。显然，<font color="blue">这种情况下并不能很好地反应这些物品的真实属性</font></p>
<ul>
<li><p>先验信息(解决方案之一)<br>虽然我们现在没有显示这个物品或者这个物品没有被点击，但是，我们“主观”地认为，比如说在显示 100 次的情况下，会有 60 次的点击</p>
<ul>
<li>注意，这些显示次数和点击次数都还没有发生</li>
<li>在这样的先验概率的影响下，点击率的估计，或者说得更加精确一些，点击率的后验概率分布的均值，<font color="blue">就成为了实际的点击加上先验的点击，除以实际的显示次数加上先验的显示次</font>数。</li>
<li>你可以看到，在有先验分布的情况下，这个比值永远不可能为 0<ul>
<li>利用先验信息来“平滑”（Smooth）概率的估计，<font color="blue">是贝叶斯统计（Bayesian Statistics）中经常使用的方法</font></li>
</ul>
</li>
<li>如果用更加精准的数学语言来表述这个过程，我们其实是为这个伯努利分布加上了一个 Beta 分布的先验概率，并且推导出了后验概率也是一个 Beta 分布。这个 Beta 分布参数的均值，就是我们刚才所说的均值</li>
</ul>
</li>
<li><p>时间折扣(基于前后时间关联)(另一种方法)<br>每个时段的估计和前面的时间是有一定关联的。这也就提醒我们是不是可以用之前的点击信息，来更加准确地估计现在这个时段的点击率</p>
<ul>
<li>一种最简单的方法还是利用我们刚才所说的先验概率的思想<ul>
<li>那就是，当前 T 时刻的点击和显示的先验数值是 T-1 时刻的某种变换<ul>
<li>比如早上 9 点到 10 点，某个物品有 40 次点击，100 次显示。那么 10 点到 11 点，我们在还没有显示的情况下，就可以认为这个物品会有 20 次点击，50 次显示。注意，我们把 9 点到 10 点的真实数据乘以 0.5 用于 10 点到 11 点的先验数据</li>
</ul>
</li>
<li>这种做法是一种主观的做法。而且是否乘以 0.5 还是其他数值需要取决于测试。但是这种思想，有时候叫作“<strong>时间折扣</strong>”（Temporal Discount），是一种非常普遍的时序信息处理的手法</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li>我们简要介绍了为什么需要基于流行度进行推荐</li>
<li>我们详细介绍了如何对流行度进行估计以及从统计角度看其含义</li>
<li>我们简要地提及了一些更加高级的流行度估计的方法</li>
</ul>
<p><a href="https://time.geekbang.org/column/article/4090?code=9OSAmHd%2525252FF2EaNhyxDg%2525252FluR3kRDdJC3ytZGLNXMqbtFY%2525253D" target="_blank" rel="noopener">极客时间版权所有</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/images/favicon.ico" alt="雷哥">
          <p class="site-author-name" itemprop="name">雷哥</p>
           
              <p class="site-description motion-element" itemprop="description">不积跬步无以至千里</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">64</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">18</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">16</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/yuancl" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-雷哥"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">雷哥</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Gemini
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
    <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  






  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
