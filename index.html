<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css">


  <meta name="keywords" content="Hexo, NexT">








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2">






<meta name="description" content="不积跬步无以至千里">
<meta property="og:type" content="website">
<meta property="og:title" content="雷哥的博客">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="雷哥的博客">
<meta property="og:description" content="不积跬步无以至千里">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="雷哥的博客">
<meta name="twitter:description" content="不积跬步无以至千里">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '雷哥'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>雷哥的博客</title>
  














</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">雷哥的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/01/06/nlp/BERT模型/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/06/nlp/BERT模型/" itemprop="url">BERT模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-06T19:34:10+08:00">
                2019-01-06
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/模型理解/" itemprop="url" rel="index">
                    <span itemprop="name">模型理解</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>首先理解：<br><a href="https://yuancl.github.io/2019/01/05/nlp/Attention模型/" target="_blank" rel="noopener">Attention模型</a><br><a href="https://yuancl.github.io/2019/01/06/nlp/Transformer模型/" target="_blank" rel="noopener">Transformer模型</a></p>
<p>参考文章：<br><a href="https://blog.csdn.net/malefactor/article/details/83961886" target="_blank" rel="noopener">从Word Embedding到Bert模型——自然语言处理预训练技术发展史</a><br><a href="https://www.jiqizhixin.com/articles/2018-11-01-9?from=synced" target="_blank" rel="noopener">谷歌终于开源BERT代码：3 亿参数量，机器之心全面解读</a></p>
<h4 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h4><ul>
<li>效果：Bert 具备广泛的通用性，就是说绝大部分 NLP 任务都可以采用类似的两阶段模式直接去提升效果</li>
</ul>
<h4 id="预训练发展史"><a href="#预训练发展史" class="headerlink" title="预训练发展史:"></a>预训练发展史:</h4><h4 id="ELMO-Embedding-from-Language-Models"><a href="#ELMO-Embedding-from-Language-Models" class="headerlink" title="ELMO(Embedding from Language Models)"></a>ELMO(Embedding from Language Models)</h4><ul>
<li>参考图像领域预训练<a href="https://yuancl.github.io/2018/10/10/dl/第三门课-第二周/" target="_blank" rel="noopener">预训练理解</a></li>
<li><p>词嵌入<a href="https://yuancl.github.io/2018/12/01/dl/第五门课-第二周/" target="_blank" rel="noopener">Word Embedding,NNLM,Word2Vec(CBOW,Skip-gram),Glove</a></p>
</li>
<li><p>解决问题：Work Embedding的多义性问题</p>
<ul>
<li>静态的方式:训练好之后每个单词的表达就固定住了，以后使用的时候，不论新句子上下文单词是什么，这个单词的 Word Embedding 不会跟着上下文场景的变化而改变，所以对于比如 Bank 这个词</li>
</ul>
</li>
<li>本质思想<ul>
<li>用事先用语言模型学好一个单词的 Word Embedding，然后根据当前上下文对 Word Embedding 动态调整的思路</li>
</ul>
</li>
<li>两阶段：<ul>
<li>第一个阶段是利用语言模型进行预训练</li>
<li>第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的 Word Embedding 作为<font color="blue">新特征</font>补充到下游任务中<ul>
<li>Feature-based Pre-Training<br>因为 ELMO给下游提供的是每个单词的特征形式，所以这一类预训练的方法被称为“Feature-based Pre-Training”<img src="/2019/01/06/nlp/BERT模型/resources/A28E00CA5B362C88488EABCE29B9311F.jpg"></li>
</ul>
</li>
</ul>
</li>
<li>ELMO和图像预训练的区别<ul>
<li>ELMO 代表的这种基于特征融合的预训练方法</li>
<li>NLP还有一种：基于 Fine-tuning 的模式,而 GPT 就是这一模式的典型开创者，这种和图像预训练比较像</li>
</ul>
</li>
<li>ELMO有什么缺点(GPT和Bert出来之后对比)<ul>
<li>LSTM抽取特征能力远低于<a href="https://yuancl.github.io/2019/01/06/nlp/Transformer模型/" target="_blank" rel="noopener">Transformer</a></li>
<li>拼接方式双向融合特征融合能力偏弱</li>
</ul>
</li>
</ul>
<h4 id="GPT-Generative-Pre-Training"><a href="#GPT-Generative-Pre-Training" class="headerlink" title="GPT(Generative Pre-Training)"></a>GPT(Generative Pre-Training)</h4><ul>
<li><p>和ELMO区别</p>
<ul>
<li>使用的Fine-tuning模式,特征抽取器不是用的 RNN，而是用的 <a href="https://yuancl.github.io/2019/01/06/nlp/Transformer模型/" target="_blank" rel="noopener">Transformer</a></li>
<li>GPT 的预训练虽然仍然是以语言模型作为目标任务，但是采用的是单向的语言模型<ul>
<li>这限制了其在更多应用场景的效果，比如阅读理解这种任务，在做任务的时候是可以允许同时看到上文和下文一起做决策的</li>
</ul>
</li>
<li>下游使用(Fine-tuning)<ul>
<li>要向 GPT 的网络结构看齐，把任务的网络结构改造成和 GPT 的网络结构是一样的。然后，在做下游任务的时候，利用第一步预训练好的参数初始化 GPT 的网络结构<img src="/2019/01/06/nlp/BERT模型/resources/2627CF7F4812570097CDA2E24151AC93.jpg">
</li>
</ul>
</li>
</ul>
</li>
<li><p>下游任务如何改造靠近 GPT 的网络结构呢</p>
<img src="/2019/01/06/nlp/BERT模型/resources/D424E7A94A3A8AFA95BF948622EBF372.jpg">
</li>
<li><p>GPT有什么问题:</p>
<ul>
<li>最主要的就是那个单向语言模型</li>
</ul>
</li>
</ul>
<h4 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h4><ul>
<li><p>两阶段模型</p>
<ul>
<li>在预训练阶段采用了类似 ELMO 的双向语言模型，和GPT一样使用<a href="https://yuancl.github.io/2019/01/06/nlp/Transformer模型/" target="_blank" rel="noopener">Transformer</a>网络</li>
<li>Fine-Tuning 阶段，这个阶段的做法和 GPT 是一样的</li>
</ul>
</li>
<li><p>和GPT,ELMO关系</p>
<ul>
<li>如果我们把 GPT预训练阶段换成双向语言模型，那么就得到了 Bert</li>
<li>如果我们把 ELMO 的特征抽取器换成<a href="https://yuancl.github.io/2019/01/06/nlp/Transformer模型/" target="_blank" rel="noopener">Transformer</a>，那么我们也会得到 Bert<img src="/2019/01/06/nlp/BERT模型/resources/E701F5F85BCFD4AE2C7F2ABE1AA7AE4E.jpg">
</li>
</ul>
</li>
<li><p>输入表征<br>BERT 最核心的过程就是同时预测加了 MASK 的缺失词与 A/B 句之间的二元关系，而这些首先都需要体现在模型的输入中</p>
<ul>
<li>特殊符 [SEP] 是用于分割两个句子的符号</li>
<li>前面半句会加上分割编码 A，后半句会加上分割编码 B,预测 B 句是不是 A 句后面的一句话</li>
<li>为了令 Transformer感知词与词之间的位置关系，我们需要使用位置编码给每个词加上位置信息<img src="/2019/01/06/nlp/BERT模型/resources/56096F6E25AD52E2E76CF6E2649085FB.jpg">
</li>
</ul>
</li>
<li><p>预训练过程<br>BERT 最核心的就是预训练过程，这也是该论文的亮点所在。简单而言，模型会从数据集抽取两句话，其中B句有 50% 的概率是 A句的下一句，然后将这两句话转化前面所示的输入表征。现在我们<font color="blue">随机遮掩（Mask 掉）</font>输入序列中 15% 的词，并<font color="red">要求 Transformer 预测这些被遮掩的词，以及 B 句是 A 句下一句的概率这两个任务</font></p>
<ul>
<li>对于二分类任务，在抽取一个序列（A+B）中，B 有 50% 的概率是 A 的下一句。如果是的话就会生成标注「IsNext」，不是的话就会生成标注「NotNext」，这些标注可以作为二元分类任务判断模型预测的凭证</li>
<li>对于 Mask 预测任务，首先整个序列会随机 Mask 掉 15% 的词，这里的 Mask 不只是简单地用「[MASK]」符号代替某些词，因为这会引起预训练与微调两阶段不是太匹配。所以谷歌在确定需要 Mask 掉的词后，80% 的情况下会直接替代为「[MASK]」，10% 的情况会替代为其它任意的词，最后 10% 的情况会保留原词<img src="/2019/01/06/nlp/BERT模型/resources/368B1596DC8027EAB8592E9AC0B1D5C2.jpg">
</li>
</ul>
</li>
<li><p>微调过程</p>
<ul>
<li><p>下图展示了 BERT 在 11 种任务中的微调方法，它们都只添加了一个额外的输出层。在下图中，Tok 表示不同的词、E 表示输入的嵌入向量、T_i 表示第 i 个词在经过 BERT 处理后输出的上下文向量</p>
<img src="/2019/01/06/nlp/BERT模型/resources/90D75BE758BD70EE89D79AB4258221C9.jpg">
<ul>
<li>(a)中判断问答对是不是包含正确回答的 QNLI、判断两句话有多少相似性的 STS-B 等，它们都用于处理句子之间的关系</li>
<li>(b)中判语句中断情感趋向的 SST-2 和判断语法正确性的 CoLA 任务，它们都是处理句子内部的关系</li>
</ul>
</li>
<li><p>NLP四类任务</p>
<ul>
<li>一类是序列标注，这是最典型的 NLP 任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题，它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。</li>
<li>第二类是分类任务，比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可。</li>
<li>第三类任务是句子关系判断，比如 Entailment，QA，语义改写，自然语言推理等任务都是这个模式，它的特点是给定两个句子，模型判断出两个句子是否具备某种语义关系。</li>
<li>第四类是生成式任务，比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类。它的特点是输入文本内容后，需要自主生成另外一段文字</li>
<li><img src="/2019/01/06/nlp/BERT模型/resources/52B6633F1E96B86DC6B4477CA91629D3.jpg">
</li>
</ul>
</li>
</ul>
<img src="/2019/01/06/nlp/BERT模型/resources/C8C2770559C89A232F97E4DCA20834D3.jpg">
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/01/06/nlp/Transformer模型/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/06/nlp/Transformer模型/" itemprop="url">Transformer模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-06T18:23:10+08:00">
                2019-01-06
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/模型理解/" itemprop="url" rel="index">
                    <span itemprop="name">模型理解</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>参考文章：<br><a href="https://shimo.im/docs/gmRW4WV2mjoXzKA1" target="_blank" rel="noopener">神经机器翻译 之 谷歌 transformer 模型</a><br><a href="https://www.cnblogs.com/robert-dlut/p/8638283.html" target="_blank" rel="noopener">Self-attention and Transformer</a><br><a href="https://mp.weixin.qq.com/s?__biz=MjM5MTQzNzU2NA==&amp;mid=2651666707&amp;idx=1&amp;sn=2e9149ccdba746eaec687038ce560349&amp;chksm=bd4c1e808a3b97968a15cb3d21032b5394461a1be275476e4fd26563aa28d99be0b798ccee17&amp;mpshare=1&amp;scene=1&amp;srcid=0111kbdci7utfkYpw9bNBcpF&amp;key=f8b9d5856fa70f7d2eabd677b381f98687650f8caf1af873c78466b32517ee5af4eecc661ae63d35bf90beca422d1abea7b7c897e43f33ab3ef7de4c816797d4bad752a5e6f5acc1908b28ffd604355f&amp;ascene=0&amp;uin=MTE0NTY4MjMyMQ%3D%3D&amp;devicetype=iMac+MacBookPro14%2C1+OSX+OSX+10.13.6+build(17G65" target="_blank" rel="noopener">大数据文摘-BERT大火却不懂Transformer</a>&amp;version=11020201&amp;lang=zh_CN&amp;pass_ticket=sLCET0Y%2BZTkYDsKSej3nfbOS885niL2%2Bt2ffNlFmQw3FszFuawe4q3nwl02gUnCe)</p>
<h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><ul>
<li>首先理解<a href="https://yuancl.github.io/2019/01/05/nlp/Attention模型/" target="_blank" rel="noopener">Attention模型</a>，个人理解Transformer本质其实就是对selt-attention的包装</li>
<li>论文《Attention is all you need》特点：<br>重点关注了复杂度，并行度，长距离依赖学习三个问题<ul>
<li>现在做神经翻译里最好的BLUE结果</li>
<li>没有采取大热的RNN/LSTM/GRU的结构，而是使用attention layer 和全连接层，达到了较好的效果，并且解决了 RNN/LSTM/GRU 里的long dependency problem </li>
<li>解决了传统RNN 训练并行度的问题，并降低了计算复杂度</li>
</ul>
</li>
</ul>
<h4 id="Encoder-Decoder架构"><a href="#Encoder-Decoder架构" class="headerlink" title="Encoder-Decoder架构"></a>Encoder-Decoder架构</h4><ul>
<li><p>Encoder-Decoder架构整体</p>
<ul>
<li>在编码器的一个网络块中，由一个多头attention子层和一个前馈神经网络子层组成，整个编码器栈式搭建了N个块</li>
<li>解码器的一个网络块中多了一个多头attention层。为了更好的优化深度网络，整个网络使用了残差连接和对层进行了规范化（Add&amp;Norm）<ul>
<li>这里有个特别点就是masking,  masking 的作用就是防止在训练的时候 使用未来的输出的单词。 比如训练时， 第一个单词是不能参考第二个单词的生成结果的。 Masking就会把这个信息变成0， 用来保证预测位置 i 的信息只能基于比 i 小的输出<img src="/2019/01/06/nlp/Transformer模型/resources/C421D21BFAA4597BC61F35AC079AF098.jpg">
</li>
</ul>
</li>
</ul>
</li>
<li><p>Encode组件</p>
<img src="/2019/01/06/nlp/Transformer模型/resources/93027EA8E8E28E2249EA065D24A54A0F.jpg">
</li>
<li><p>Decode组件</p>
<ul>
<li>编码器通过处理输入序列开启工作。顶端编码器的输出之后会变转化为一个包含向量K（键向量）和V（值向量）的注意力向量集 。这些向量将被每个解码器用于自身的“编码-解码注意力层”，而这些层可以帮助解码器关注输入序列哪些位置合适<img src="/2019/01/06/nlp/Transformer模型/resources/B464700544F8040B5FAC358F5070AEE0.gif"></li>
<li>接下来的步骤重复了这个过程，直到到达一个特殊的终止符号，  -  它表示transformer的解码器已经完成了它的输出。每个步骤的输出在下一个时间步被提供给底端解码器，并且就像编码器之前做的那样，这些解码器会输出它们的解码结果</li>
<li>这个“编码-解码注意力层”工作方式基本就像多头自注意力层一样，只不过它是通过在它下面的层来创造查询矩阵，并且从编码器的输出中取得键/值矩阵<p style="margin-left: 8px;margin-right: 8px;"><span data-ratio="1.9636363636363636" id="js_tx_video_container_0.7003880785346812" class="js_tx_video_container" style="display: block; width: 661px; height: 372px;"><iframe frameborder="0" width="661" height="371.8125" allow="autoplay; fullscreen" allowfullscreen="true" src="//v.qq.com/txp/iframe/player.html?origin=https%3A%2F%2Fmp.weixin.qq.com&amp;vid=m13563cy49o&amp;autoplay=false&amp;full=true&amp;show1080p=false&amp;isDebugIframe=false"></iframe></span></p>
</li>
</ul>
</li>
<li><p>最终的线性变换和Softmax层<br>解码组件最后会输出一个实数向量。我们如何把浮点数变成一个单词？这便是线性变换层要做的工作，它之后就是Softmax层</p>
<ul>
<li>线性变换层是一个简单的全连接神经网络，它可以把解码组件产生的向量投射到一个比它大得多的、被称作对数几率（logits）的向量里</li>
<li>接下来的Softmax 层便会把那些分数变成概率（都为正数、上限1.0）。概率最高的单元格被选中，并且它对应的单词被作为这个时间步的输出<img src="/2019/01/06/nlp/Transformer模型/resources/1493FFA7472D4E31F49EF536CF6F1F54.jpg">
</li>
</ul>
</li>
</ul>
<h4 id="scaled-dot-Product-attention"><a href="#scaled-dot-Product-attention" class="headerlink" title="scaled dot-Product attention"></a>scaled dot-Product attention</h4><ul>
<li>本质：其实scaled dot-Product attention就是我们常用的使用<font color="blue">点积进行相似度计算</font>的attention，只是多除了一个（为K的维度）起到调节作用，使得内积不至于太大</li>
<li>操作步骤<ul>
<li>每个query-key 会做出一个点乘的运算过程</li>
<li>最后会使用soft max 把他们归一</li>
<li>再到最后会乘以V (values) 用来当做attention vector. <img src="/2019/01/06/nlp/Transformer模型/resources/70C79702B44D8406D4B78338B0C7ADAD.jpg"></li>
</ul>
</li>
<li>详细理解<img src="/2019/01/06/nlp/Transformer模型/resources/F21545F043FA58B804CAEC5D3C7EF995.jpg">
</li>
</ul>
<h4 id="Multi-head-attention-1"><a href="#Multi-head-attention-1" class="headerlink" title="Multi-head attention(1)"></a>Multi-head attention(1)</h4><ul>
<li>它扩展了模型专注于不同位置的能力<ul>
<li><font color="blue">我理解就是不同的每个注意力头都会对会有不同的关注点,就是丰富了一个词的注意点</font></li>
</ul>
</li>
<li><p>它给出了注意力层的多个“表示子空间”（representation subspaces）</p>
<ul>
<li>接下来我们将看到，对于“多头”注意机制，我们有多个查询/键/值权重矩阵集(Transformer使用八个注意力头，因此我们对于每个编码器/解码器有八个矩阵集合)。这些集合中的每一个都是随机初始化的</li>
<li>在训练之后，每个集合都被用来将输入词嵌入(或来自较低编码器/解码器的向量)<font color="blue">投影到不同的表示子空间中</font><img src="/2019/01/06/nlp/Transformer模型/resources/68E2D18A10963F046370DB126E452A3F.jpg">
</li>
</ul>
</li>
<li><p>如果我们做与上述相同的自注意力计算，只需八次不同的权重矩阵运算，我们就会得到八个不同的Z矩阵</p>
<img src="/2019/01/06/nlp/Transformer模型/resources/881CCDA1BDBA993EA7665D2F8B9FC3A9.jpg">
</li>
<li><p><font color="blue">前馈层不需要8个矩阵，它只需要一个矩阵</font>(由每一个单词的表示向量组成)</p>
<ul>
<li>所以我们需要一种方法把这八个矩阵压缩成一个矩阵。那该怎么做？其实可以直接把这些矩阵拼接在一起，然后用一个附加的权重矩阵WO与它们相乘<img src="/2019/01/06/nlp/Transformer模型/resources/7903100853302A431B813728D73BBFA0.jpg">
</li>
</ul>
</li>
</ul>
<h4 id="Multi-head-attention-2"><a href="#Multi-head-attention-2" class="headerlink" title="Multi-head attention(2)"></a>Multi-head attention(2)</h4><ul>
<li>Query，Key，Value首先进过一个线性变换</li>
<li>然后输入到放缩点积attention<ul>
<li>注意这里要做h次，其实也就是所谓的多头，每一次算一个头</li>
<li>每次Q，K，V进行线性变换的参数W是不一样的，所以每个头都计算自己的特征</li>
<li>进行h个头计算好处：好处是可以允许模型在不同的表示子空间里学习到相关的信息</li>
</ul>
</li>
<li>然后将h次的放缩点积attention结果进行拼接(concat)</li>
<li>再进行一次线性变换得到的值作为多头attention的结果</li>
</ul>
<img src="/2019/01/06/nlp/Transformer模型/resources/8D8F43D711E9C94BA7013FD1DE9A666F.jpg">
<h4 id="如何使用attention"><a href="#如何使用attention" class="headerlink" title="如何使用attention"></a>如何使用attention</h4><ul>
<li>首先在编码器到解码器的地方使用了多头attention进行连接,编码器的层输出（这里K=V）和解码器中都头attention的输入。其实就和主流的机器翻译模型中的attention一样<img src="/2019/01/06/nlp/Transformer模型/resources/F318057213E0EE97E53E6A165DF5E892.jpg"></li>
<li>在Encoder,Decoder中都使用的self-attention<ul>
<li>例如输入一个句子，那么里面的每个词都要和该句子中的<font color="blue">所有词进行attention计算</font>。目的是<font color="blue">学习句子内部的词依赖关系，捕获句子的内部结构</font><img src="/2019/01/06/nlp/Transformer模型/resources/A60F81F9218EB99D06E9DB202DB6F8AD.jpg">
</li>
</ul>
</li>
</ul>
<h4 id="Transformer解决问题"><a href="#Transformer解决问题" class="headerlink" title="Transformer解决问题"></a>Transformer解决问题</h4><ul>
<li>并行问题<ul>
<li>首先each head， 是可以并行计算的， 然后每个head 都有自己对应的weight, 实现不同的线性转换， 这样每个head 也就有了自己特别的表达信息</li>
<li>多头attention和CNN一样不依赖于前一时刻的计算，可以很好的并行，优于RNN</li>
</ul>
</li>
<li>长距离依赖学习<ul>
<li>self-attention是每个词和所有词都要计算attention，所以不管他们中间有多长距离，最大的路径长度也都只是1。可以捕获长距离依赖关系</li>
</ul>
</li>
<li>RNN，CNN计算复杂度的比较<ul>
<li>如果输入序列n小于表示维度d的话，每一层的时间复杂度self-attention是比较有优势的。当n比较大时，作者也给出了一种解决方案self-attention（restricted）即每个词不是和所有词计算attention，而是<font color="blue">只与限制的r个词去计算attention</font></li>
</ul>
</li>
</ul>
<h4 id="训练部分-amp-损失函数"><a href="#训练部分-amp-损失函数" class="headerlink" title="训练部分&amp;损失函数"></a>训练部分&amp;损失函数</h4><p>Transformer只是一个encode-decode框架架构，后面的，训练&amp;损失函数&amp;优化算法就也能共用其他了，比如BP，集束搜索(beam search)…</p>
<ul>
<li>既然我们已经过了一遍完整的transformer的前向传播过程，那我们就可以直观感受一下它的训练过程。</li>
<li>在训练过程中，一个未经训练的模型会通过一个完全一样的前向传播。但因为我们用有标记的训练集来训练它，所以我们可以用它的输出去与真实的输出做比较</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/01/05/nlp/Attention模型/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/05/nlp/Attention模型/" itemprop="url">Attention模型</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-05T18:41:10+08:00">
                2019-01-05
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index">
                    <span itemprop="name">NLP</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/模型理解/" itemprop="url" rel="index">
                    <span itemprop="name">模型理解</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://yuancl.github.io/2018/12/06/dl/第五门课-第三周/" target="_blank" rel="noopener">吴恩达课程-Attention实现</a><br>参考文章：<a href="https://zhuanlan.zhihu.com/p/37601161" target="_blank" rel="noopener">深度学习中的注意力模型</a></p>
<h4 id="Encoder-Decoder框架"><a href="#Encoder-Decoder框架" class="headerlink" title="Encoder-Decoder框架"></a>Encoder-Decoder框架</h4><ul>
<li>可以把它看作适合处理由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型</li>
<li>Encoder顾名思义就是对输入句子X进行编码，将输入句子通过非线性变换转化为中间语义表示C：$C=F(X_1,X_2,…X_m)$</li>
<li>解码器Decoder来说，其任务是根据句子X的中间语义表示C和之前已经生成的历史信息y1,y2….yi-1来生成i时刻要生成的单词yi $y_i=g(c,y_1,y_2,…y_{i-1})$</li>
<li>总结：<ul>
<li>Encoder-Decoder是个非常通用的计算框架，常见的比如CNN/RNN/BiRNN/GRU/LSTM/Deep LSTM等，这里的变化组合非常多</li>
<li>框架很多应用场景：<ul>
<li>机器翻译来说，&lt;X,Y&gt;就是对应不同语言的句子，比如X是英语句子，Y是对应的中文句子翻译</li>
<li>对于文本摘要来说，X就是一篇文章，Y就是对应的摘要</li>
<li>对话机器人来说，X就是某人的一句话，Y就是对话机器人的应答</li>
</ul>
</li>
</ul>
</li>
</ul>
<img src="/2019/01/05/nlp/Attention模型/resources/F2F31AE3AFCF5E63870687850E25053E.jpg">
<h4 id="Attention模型"><a href="#Attention模型" class="headerlink" title="Attention模型"></a>Attention模型</h4><ul>
<li>分心模型：在生成目标句子的单词时，不论生成哪个单词，是y1,y2也好，还是y3也好，他们使用的句子X的<font color="blue">语义编码C都是一样的，没有任何区别。</font></li>
<li>添加注意力：$y_i的输出对应不同的输入c_i$<img src="/2019/01/05/nlp/Attention模型/resources/24F1729383B4C502BDF631EC89F3ED8D.jpg"></li>
<li>例如生成$y_3$:汤姆这个词的对应输入词的权重：<img src="/2019/01/05/nlp/Attention模型/resources/3C10CE8870D6AD2D2C85274AFB51FC42.jpg"></li>
<li><p>如何确定这些权重？(可以看下面的三阶段计算Attention过程)</p>
<ul>
<li>通过函数F(hj,Hi)来获得目标单词Yi和每个输入单词对应的对齐可能性(<font color="blue">其实也是通过一个函数获取各个单词的权重</font>)<ul>
<li>对于采用RNN的Decoder来说，如果要生成yi单词，在时刻i，我们是可以知道在生成Yi之前的隐层节点i时刻的输出值Hi的，而我们的目的是要计算生成Yi时的输入句子单词“Tom”、“Chase”、“Jerry”对Yi来说的注意力分配概率分布。那么可以用i时刻的隐层节点状态Hi去一一和输入句子中每个单词对应的RNN隐层节点状态hj进行对比，即通过函数F(hj,Hi)来获得目标单词Yi和每个输入单词对应的对齐可能性</li>
</ul>
</li>
<li>这个F函数在不同论文里可能会采取不同的方法，然后函数F的输出经过Softmax进行归一化就得到了符合概率分布取值区间的注意力分配概率分布数值</li>
</ul>
</li>
<li><p>直观理解</p>
<ul>
<li>目标句子生成的每个单词对应输入句子单词的概率分布可以理解为输入句子单词和这个目标生成单词的对齐概率，这在机器翻译语境下是非常直观的：传统的统计机器翻译一般在做的过程中会专门有一个短语对齐的步骤，而注意力模型其实起的是相同的作用</li>
<li>从概念上理解的话，把AM模型理解成影响力模型也是合理的，就是说生成目标单词的时候，输入句子每个单词对于生成这个单词有多大的影响程度<ul>
<li>例子：矩阵中每一列代表生成的目标单词对应输入句子每个单词的AM分配概率，颜色越深代表分配到的概率越大<img src="/2019/01/05/nlp/Attention模型/resources/339E562920007C2CC0BD095CE03620E4.jpg">
</li>
</ul>
</li>
</ul>
</li>
<li><p>Attention机制的本质思想<br>我们可以这样来看待Attention机制（参考图9）：将Source中的构成元素想象成是由一系列的&lt;Key,Value&gt;数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，<font color="blue">然后对Value进行加权求和，即得到了最终的Attention数值</font>。<font color="red">所以本质上Attention机制是对Source中元素的Value值进行加权求和</font>，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式:</p>
<img src="/2019/01/05/nlp/Attention模型/resources/B514B9C65D76AEC9E2D3C1BA0E1714FC.jpg">
<img src="/2019/01/05/nlp/Attention模型/resources/08DC337F6EEC1B4ECCCE3069BCF2DEE8.jpg">
</li>
<li><p>三阶段计算Attention过程</p>
<ul>
<li>第一个阶段根据Query和Key计算两者的相似性或者相关性<ul>
<li>可以引入不同的函数和计算机制，根据Query和某个Key_i，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值<img src="/2019/01/05/nlp/Attention模型/resources/C46D9EBEDC7DC8BDD2FD25C876452832.jpg"></li>
</ul>
</li>
<li>第二个阶段对第一阶段的原始分值进行归一化处理<ul>
<li>引入类似SoftMax的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布</li>
<li>另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重<img src="/2019/01/05/nlp/Attention模型/resources/CFA555D76CFF7EF83BB388D9DCC83E83.jpg"></li>
</ul>
</li>
<li>第三个阶段根据权重系数对Value进行加权求和<ul>
<li>计算结果a_i即为value_i对应的权重系数，然后进行加权求和即可得到Attention数值<img src="/2019/01/05/nlp/Attention模型/resources/670426A82FAECEDC11036020CAE91B6B.jpg">
<img src="/2019/01/05/nlp/Attention模型/resources/B0A35F4001A8D56A581543FF91E10844.jpg"></li>
</ul>
</li>
<li>总结：通过如上三个阶段的计算，即可求出针对Query的Attention数值，目前绝大多数具体的注意力机制计算方法都符合上述的三阶段抽象计算过程</li>
</ul>
</li>
</ul>
<h4 id="从微观视角看自注意力机制"><a href="#从微观视角看自注意力机制" class="headerlink" title="从微观视角看自注意力机制"></a>从微观视角看自注意力机制</h4><ul>
<li>第一步就是从每个编码器的输入向量（每个单词的词向量）中生成三个向量<ul>
<li>我们创造一个<font color="blue">查询向量、一个键向量和一个值向量。这三个向量是通过词嵌入与三个权重矩阵后相乘创建的</font></li>
<li>这些新向量在维度上比词嵌入向量更低。他们的维度是64，而词嵌入和编码器的输入/输出向量的维度是512<img src="/2019/01/05/nlp/Attention模型/resources/A6F39C8F454935AAFEBB83A0ACF2E1B0.jpg"></li>
</ul>
</li>
<li>第二步是计算得分<ul>
<li>假设我们在为这个例子中的第一个词“Thinking”计算自注意力向量，我们需要拿输入句子中的每个单词对“Thinking”打分。<font color="blue">这些分数决定了在编码单词“Thinking”的过程中有多重视句子的其它部分</font><ul>
<li>这些分数是通过打分单词（所有输入句子的单词）的键向量与“Thinking”的查询向量相点积来计算的</li>
<li>所以如果我们是处理位置最靠前的词的自注意力的话，第一个分数是q1和k1的点积，第二个分数是q1和k2的点积<img src="/2019/01/05/nlp/Attention模型/resources/807C3954F1BF0F7C006209437905F305.jpg"></li>
</ul>
</li>
</ul>
</li>
<li>第三步第四步<ul>
<li>将分数除以8(8是论文中使用的键向量的维数64的平方根，这会让梯度更稳定。这里也可以使用其它值，8只是默认值)</li>
<li>然后通过softmax传递结果。<font color="blue">softmax的作用是使所有单词的分数归一化，得到的分数都是正值且和为1</font><ul>
<li>这个softmax分数决定了每个单词对编码当下位置（“Thinking”）的贡献。显然，已经在这个位置上的单词将获得最高的softmax分数<img src="/2019/01/05/nlp/Attention模型/resources/EED7B3888E7ED2122DF172489F13BE4C.jpg"></li>
</ul>
</li>
</ul>
</li>
<li>第五步是将每个值向量乘以softmax分数(这是为了准备之后将它们求和)。<ul>
<li>这里的直觉是<font color="blue">希望关注语义上相关的单词，并弱化不相关的单词</font>(例如，让它们乘以0.001这样的小数)</li>
</ul>
</li>
<li>第六步是对加权值向量求和，然后即得到自注意力层在该位置的输出(在我们的例子中是对于第一个单词)<ul>
<li>译注：自注意力的另一种解释就是在编码某个单词时，就是将所有单词的表示（值向量）进行加权求和，而权重是通过该词的表示（键向量）与被编码词表示（查询向量）的点积并通过softmax得到。<img src="/2019/01/05/nlp/Attention模型/resources/1748301F371EF3428E3F4DB1A5F47730.jpg">
</li>
</ul>
</li>
</ul>
<h4 id="Self-Attention模型"><a href="#Self-Attention模型" class="headerlink" title="Self Attention模型"></a>Self Attention模型</h4><ul>
<li>一般任务中的情况<br>在一般任务的Encoder-Decoder框架中，输入Source和输出Target内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，<font color="blue">Attention机制发生在Target的元素Query和Source中的所有元素之间</font></li>
<li>Self Attention<br>Attention顾名思义，指的不是Target和Source之间的Attention机制，<font color="blue">而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制</font>。其具体计算过程是一样的，只是计算对象发生了变化而已，所以此处不再赘述其计算过程细节</li>
<li>例子<img src="/2019/01/05/nlp/Attention模型/resources/78832378181B08156139C411039FF734.jpg"></li>
<li>优点：<ul>
<li>更擅长捕获句子中长距离特征<br>引入Self Attention后会更容易捕获句子中长距离的相互依赖的特征，因为如果是RNN或者LSTM，需要依次序序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小<ul>
<li>但是SelfAttention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短</li>
</ul>
</li>
<li>增加计算的并行性</li>
</ul>
</li>
</ul>
<h4 id="Attention机制的应用"><a href="#Attention机制的应用" class="headerlink" title="Attention机制的应用"></a>Attention机制的应用</h4><p>Attention机制不仅在NLP有广泛应用，在图像和语言领域也有很多的应用</p>
<ul>
<li>图片描述<ul>
<li>很明显这种应用场景也可以使用Encoder-Decoder框架来解决任务目标，此时Encoder输入部分是一张图片，一般会用CNN来对图片进行特征抽取，Decoder部分使用RNN或者LSTM来输出自然语言句子</li>
<li>此时如果加入Attention机制能够明显改善系统输出效果，Attention模型在这里起到了类似人类视觉选择性注意的机制，在输出某个实体单词的时候会将注意力焦点聚焦在图片中相应的区域上<img src="/2019/01/05/nlp/Attention模型/resources/D65829551454CDE399C3DBE51C953E17.jpg"></li>
</ul>
</li>
<li>Attention让每个单词对应图片中的注意力聚焦区域<img src="/2019/01/05/nlp/Attention模型/resources/495110CA9218AF232B0D47BE88497073.jpg"></li>
<li>语音识别中音频序列和输出字符之间的Attention<ul>
<li>展示了在Encoder-Decoder框架中加入Attention机制后，当用户用语音说句子how much would a woodchuck chuck 时，输入部分的声音特征信号和输出字符之间的注意力分配概率分布情况，颜色越深代表分配到的注意力概率越高。从图中可以看出，在这个场景下，Attention机制起到了将输出字符和输入语音信号进行对齐的功能<img src="/2019/01/05/nlp/Attention模型/resources/7F7EE44196F2FAAF3F1BC2BFAB29DA4A.jpg"></li>
</ul>
</li>
<li>总结：Encoder-Decoder加Attention架构由于其卓越的实际效果，目前在深度学习领域里得到了广泛的使用，了解并熟练使用这一架构对于解决实际问题会有极大帮助</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/23/ml/强化学习/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/23/ml/强化学习/" itemprop="url">强化学习</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-23T21:30:02+08:00">
                2018-12-23
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/强化学习/" itemprop="url" rel="index">
                    <span itemprop="name">强化学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="Reforcement-Learning"><a href="#Reforcement-Learning" class="headerlink" title="Reforcement Learning"></a>Reforcement Learning</h4><ul>
<li><p>什么叫RL<br>和监督学习的区别是，Agent会通过观察(Observation)当前环境(Environment),获得当前的State，接着Action后能够得到Environment的反馈：Reward，然后Agent根据Reward再调整做出正确的Action去改变Environment</p>
<ul>
<li>Agent:学习主体，可以是一个NN或者其它，作用：Learns to take actions maximizing expected reward</li>
<li>Environment:外部环境，比如下围棋就是对手，玩电玩的时候，就是主机</li>
<li>State：Agent通过观察获取到的当前环境的输入状态</li>
<li>Action：根据当前状态做出的动作，当然此动作会影响Environment，比如下围棋的时候会影响对手下一步落棋</li>
<li>Reward:做出action，当前Environment给予的评价<img src="/2018/12/23/ml/强化学习/resources/089167ED63C58D955C18087D366BF2A3.jpg">
<img src="/2018/12/23/ml/强化学习/resources/260B08ED8010A47991A9DF4BE29D1E57.jpg">
</li>
</ul>
</li>
<li><p>Look for a function</p>
<img src="/2018/12/23/ml/强化学习/resources/B620070EEBB401F2C731FCC8E23FAB60.jpg">
</li>
<li><p>举例</p>
<ul>
<li>state:输入为当前画面(像素值)</li>
<li>action:左移，右移或者开火</li>
<li>reward:Score<img src="/2018/12/23/ml/强化学习/resources/F64DEE320E6175A281141261369F122A.jpg">
</li>
</ul>
</li>
</ul>
<ul>
<li>outline<img src="/2018/12/23/ml/强化学习/resources/813056D16FC4398AB40E7F56841C8828.jpg">
</li>
</ul>
<h4 id="Policy-based-Approach-Learining-an-Actor"><a href="#Policy-based-Approach-Learining-an-Actor" class="headerlink" title="Policy-based Approach(Learining an Actor)"></a>Policy-based Approach(Learining an Actor)</h4><ul>
<li><p>总体分三步</p>
<img src="/2018/12/23/ml/强化学习/resources/478F4C2660ABFC1B5E675896C8571F84.jpg">
<ul>
<li><p>First step：</p>
<img src="/2018/12/23/ml/强化学习/resources/E5859C0699A3CB2FB7D06715DEF7C293.jpg">
</li>
<li><p>Second step：</p>
<img src="/2018/12/23/ml/强化学习/resources/035061213843511C0A7561A1F0E06792.jpg">
</li>
<li><p>Third step(pick the best function)：</p>
<img src="/2018/12/23/ml/强化学习/resources/FEB8FFD784F357B5CFE54CCD02E5A05B.jpg">
</li>
</ul>
</li>
<li><p>Policy Gradient作用<br>如果Actor,Env,Reward都看做是DNN，那么本质上就是一个NN网络，求极值就很容易。但实际上Env，Reward不是一个NN网络，所以就不能进行微分，不能求出极值，解决办法就是用policy gradient进行处理</p>
<img src="/2018/12/23/ml/强化学习/resources/61A9812B02E141127554B513DA22F2CA.jpg">
</li>
</ul>
<h4 id="Value-based-Approach-Learning-a-critic"><a href="#Value-based-Approach-Learning-a-critic" class="headerlink" title="Value-based Approach(Learning a critic)"></a>Value-based Approach(Learning a critic)</h4><ul>
<li>是评价一个agent(actor)的好坏，$V^{\pi}(s)$表示给定一个Agent(Actor:$\pi$)，并给定一个State:s，到最后游戏完成后，得到最后的reward expects的值<ul>
<li>比如下面坐标的图，有很多怪，到游戏结束期望分数就容易获得比较高的值，右边图就比较小(因为这个图看到到最后游戏结束，能杀的怪已经很少了)</li>
<li>同一个state，不同的actor，那么$V^{\pi}(s)$值也会不一样<img src="/2018/12/23/ml/强化学习/resources/84B70D51C472CE9220B50917CF343EBE.jpg">
</li>
</ul>
</li>
</ul>
<ul>
<li><p>How to estimate $V^{\pi}(s)$</p>
<ul>
<li><p>Monte-Carlo,观察到两个state,$S_a,S_b$，并且最后episode结束，Greed为$G_a,G_b$，这个时候，只需要让$V^{\pi}(s_a)\approx G_a,V^{\pi}(s_b)\approx G_b$</p>
<img src="/2018/12/23/ml/强化学习/resources/314B5DFA5F6A0AEEACBE25BEEC618D05.jpg">
</li>
<li><p>Temporal-difference,不会等到episode才开始计算(不用等到游戏结束就可以更新参数)，原理是$V^{\pi}(s_t),V^{\pi}(s_{t+1})$中间是相差的$r_t$,所以只需要$V^{\pi}(s_t)-V^{\pi}(s_{t+1})\approx r_t$</p>
<img src="/2018/12/23/ml/强化学习/resources/D113811A3EE6DF3759AC23138C68A4C6.jpg">
</li>
</ul>
</li>
<li><p>Q Learning</p>
<ul>
<li><p>Another Critic</p>
<ul>
<li>输入为state和action,可以对所有action做穷举（如果不能穷举，其实还有其他的方法的），看哪一个reward得分最高(得到Q function)<img src="/2018/12/23/ml/强化学习/resources/26D289911C892B3832B7E88BDA9E3848.jpg">
</li>
</ul>
</li>
<li><p>每一次都找最大的Qfunction数值的action a</p>
<img src="/2018/12/23/ml/强化学习/resources/1060DC495D95BBDB0CD2FB19749CD14E.jpg">
</li>
</ul>
</li>
</ul>
<h4 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor+Critic"></a>Actor+Critic</h4><ul>
<li>核心原理：不会像Actor那样跟着环境去学习，因为环境变化是比较多的，所以Actor-Critic是跟着critic去学习<img src="/2018/12/23/ml/强化学习/resources/EA44E3EFC2F7B6582C5207FB6461F7EE.jpg">
<img src="/2018/12/23/ml/强化学习/resources/7BB0A900A5C3D4D5567050C7A7D0FA0A.jpg"></li>
<li>A3C<img src="/2018/12/23/ml/强化学习/resources/1FDE16A21B149590A05D2335FFC45D90.jpg">
</li>
</ul>
<h4 id="Inverse-Reforcement-learning"><a href="#Inverse-Reforcement-learning" class="headerlink" title="Inverse Reforcement learning"></a>Inverse Reforcement learning</h4><ul>
<li>背景：其实生活中的大多数场景，都不好找reward的，不像围棋或者是电玩，有很明确的规则.<ul>
<li>比如如果交通违规，如何处罚等。还比如说让机器人放盘子，之前并没有告诉摔坏盘子会扣分，机器人就不知道</li>
</ul>
</li>
<li>原来的方案：<img src="/2018/12/23/ml/强化学习/resources/ADBCCF56067A9D873705B18C483D8B4D.jpg"></li>
<li>Inverse Reforcement learning方案：<ul>
<li>正好相反，并不知道Reward Function，是通过学习学习到Reward Function后，然后使用它选择出最好的actor<img src="/2018/12/23/ml/强化学习/resources/57B1F1A69DD14E80166F5776FB4E9CF5.jpg"></li>
</ul>
</li>
<li><p>步骤：</p>
<ul>
<li>expert和IRL都会自己学习，获得最终Reward，我们假设专家的Reward总数比学习到的好</li>
<li>然后从中我们找到一个最好的Reward Function R</li>
<li>再通过R去得到Actor $\pi$</li>
<li>要注意下是，如果规则变化了，那么下面的循环圈需要不断的重新循环取学习<img src="/2018/12/23/ml/强化学习/resources/DE2E7716C70854CC5CE59AC271378ED4.jpg">
</li>
</ul>
</li>
<li><p>发现和GAN比较像</p>
<img src="/2018/12/23/ml/强化学习/resources/FE61F494902468450399EBA1D4B37642.jpg"></li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
    
	
    
	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/06/dl/第五门课-第三周/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/06/dl/第五门课-第三周/" itemprop="url">第五门课-第三周</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-06T08:17:19+08:00">
                2018-12-06
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/吴恩达课程总结/" itemprop="url" rel="index">
                    <span itemprop="name">吴恩达课程总结</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://github.com/yuancl/dl-algorithm/tree/master/5-RecurrentNeuralNetworks/Week3/Machine-Translation" target="_blank" rel="noopener">习题代码:Machine-Translation</a><br><a href="https://github.com/yuancl/dl-algorithm/tree/master/5-RecurrentNeuralNetworks/Week3/Trigger-word-detection" target="_blank" rel="noopener">习题代码:Trigger-word-detection</a></p>
<h4 id="序列结构的各种序列"><a href="#序列结构的各种序列" class="headerlink" title="序列结构的各种序列"></a>序列结构的各种序列</h4><ul>
<li><p>seq2seq<br>分为编码网络的解码网络</p>
<img src="/2018/12/06/dl/第五门课-第三周/resources/4A4F9C5B535DDB65265853ACD2EDC912.jpg">
</li>
<li><p>应用场景</p>
<ul>
<li>机器翻译，比如英文和法文的互相翻译</li>
<li>Image-to-seq<br>例子中使用的AlexNet网络，将最后的softmax输出替换为RNN网络，输出为一个序列<img src="/2018/12/06/dl/第五门课-第三周/resources/49BD4A13FE8A1C4571A631492527A168.jpg">
</li>
</ul>
</li>
<li><p>选择最可能的句子</p>
<ul>
<li>语言模型和翻译模型比较<ul>
<li>语言模型总是以零向量开始$(P(y^{&lt; 1 &gt;},y^{&lt; 2 &gt;},y^{&lt; 3 &gt;}…y^{&lt; n &gt;}))$</li>
<li>而翻译模型输入是法语的encoder，可以理解为条件模型:$P(y^{&lt; 1 &gt;},y^{&lt; 2 &gt;},y^{&lt; 3 &gt;}…y^{&lt; n &gt;}|x^{&lt; 1 &gt;},x^{&lt; 2 &gt;},x^{&lt; 3 &gt;}…x^{&lt; n &gt;})$</li>
<li>所以一个是随机输出seq，另一个是需要概率最大的seq<img src="/2018/12/06/dl/第五门课-第三周/resources/BB67CF9637B51A081225158E964A74BA.jpg"></li>
<li>如何保证翻译模型中的条件概率最大<ul>
<li>贪心算法：没一个输出都保证当前一个term($y^{&lt; i &gt;}$)概率最大，但是并不能保证<font color="red">整个序列概率最大</font>，所以一次挑选一个词并不是最佳的选择</li>
<li>穷举法：将所有句子进行穷举测试，但由单词生成的句子量太大，无法穷举</li>
<li>集束算法</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="集束算法-Beam-Search"><a href="#集束算法-Beam-Search" class="headerlink" title="集束算法(Beam Search)"></a>集束算法(Beam Search)</h4><ul>
<li><p>集束算法</p>
<ul>
<li>贪婪算法只会挑出最可能的那一个单词，然后继续。而集束搜索则会考虑多个选择，选择数量就是集束宽(Beam width)</li>
<li>第一步：<br>需要 输入法语句子到编码网络，然后会解码这个网络，这个 softmax 层(上图编号 3 所示)会输 出 10,000 个概率值，得到这 10,000 个输出的概率值，取前三个存起来</li>
<li>第二步：<ul>
<li>选择第一步中的一个词$P(y^{<2>}|x, y^{<1>})$如下图3，输入为第一步的输出和x</1></2></li>
<li>不仅仅 是第二个单词有最大的概率，而是第一个、第二个单词对有最大的概率</li>
<li>$P(y^{<1>},y^{<2>}|x)=P(y^{<1>}|x)P(y^{<2>}|x, y^{<1>})$</1></2></1></2></1></li>
</ul>
</li>
<li>后面的步骤也是模仿第二步进行计算</li>
<li>重复第二步，选择第一步中的其他词进行计算<img src="/2018/12/06/dl/第五门课-第三周/resources/09E08ACC8EC0182CF63C63DD96E9F0EC.jpg">
<img src="/2018/12/06/dl/第五门课-第三周/resources/D81DFB526C95CD2E4362B500F8E26472.jpg"></li>
<li>可以看见，如果集束宽为1，那么其实就是贪心算法</li>
</ul>
</li>
<li><p>改进集束搜索</p>
<ul>
<li><p>问题1：$P(y^{<1>}|x)P(y^{<2>}|x, y^{<1>})$…..由于每一步概率值都小于0，所以比容易造成数值下溢，也就是导致电脑的浮点表示不能精确的存储</1></2></1></p>
<ul>
<li>解决方案：<br>我们总是记录概率的对数和，而不是概率的乘积，就用到了对数函数乘积的性质<img src="/2018/12/06/dl/第五门课-第三周/resources/A0E79789CBD92DE575602B479DAD35D3.jpg">
</li>
</ul>
</li>
<li><p>问题2：由于每一项乘积都小于1，并且目标函数是最大概率的输出，所以一般会偏向于比较短的句子，这样乘积项就比较少</p>
<ul>
<li>解决方案：<br>通过除以翻译结果的单词数量。这样就是取每个单词的概率对数值的平均了，这样很明显地 减少了对输出长的结果的惩罚</li>
</ul>
</li>
<li><p>更柔和的方法：并不直接除以单词数，而是会乘上一个超参数a,比如a=0.7，如果a=1那么就相当于完全用长度来做归一化</p>
</li>
<li><p>集束宽选择<br>集束宽越大效果当然越好，但是计算也越复杂。在产品中，经常可以看到把束宽设到 10</p>
</li>
</ul>
</li>
</ul>
<h4 id="集束搜索误差分析"><a href="#集束搜索误差分析" class="headerlink" title="集束搜索误差分析"></a>集束搜索误差分析</h4><h4 id="Bleu得分"><a href="#Bleu得分" class="headerlink" title="Bleu得分"></a>Bleu得分</h4><ul>
<li>背景：比如机器翻译会得到很多都不错的答案，那么如何选择呢？Bleu得分算法，就是解决这个问题，它做的是；给定一个机器生成的翻译，能够自动地计算一个分数来衡量好坏</li>
</ul>
<h4 id="注意力模型"><a href="#注意力模型" class="headerlink" title="注意力模型"></a>注意力模型</h4><ul>
<li>背景：从Bleu评分可以看出，长句的效果比较差<img src="/2018/12/06/dl/第五门课-第三周/resources/37CDD1E8D8F3D180CB8C734953DD30E0.jpg"></li>
<li><font color="blue">原理：模仿人工翻译，不会一次性把整个文章都读完再翻译，会读一部分内容，翻译一部分</font><ul>
<li>下层的双向LSTM就是先跑所有的法语单词，然后形成context，看具体时间步的注意力需要多少前后关注做准备</li>
</ul>
</li>
<li>理解：在计算当前时间步的时候，会花多少精力去关注多少当前词附近的词语<ul>
<li>这些是注意力权重，即$a^{&lt;t,t’&gt;}$告诉你，当你尝试生成第𝑡个英文词，它应该花多少注意力在第𝑡个法语词上面。当生成一个 特定的英文词时，这允许它在每个时间步去看周围词距内的法语词要花多少注意力<img src="/2018/12/06/dl/第五门课-第三周/resources/EEBE423431B34A94171A26BDFA7882A4.jpg"></li>
</ul>
</li>
<li><p>编程练习：</p>
<ul>
<li>将人工理解(Tuesday 09 Oct 1993)的日期翻译为机器理解的日期(1993-10-09)</li>
<li>这里前一个时间步的输出不用喂给后一个输入，并不像前面练习的恐龙名字一样，前后两个单词是有关联的</li>
<li>在训练过程中，每一个时间步的注意力长度是不一样的<img src="/2018/12/06/dl/第五门课-第三周/resources/691EABB718941C596A7B2C25AF3B853D.jpg"></li>
<li><p>model：<br>总体分为2步，1：右边，one “Attention”如何计算,2:左边，整体的attention模型<br>In this part, you will implement the attention mechanism presented in the lecture videos. Here is a figure to remind you how the model works. The diagram on the left shows the attention model. The diagram on the right shows what one “Attention” step does to calculate the attention variables $\alpha^{\langle t, t’ \rangle}$, which are used to compute the context variable $context^{\langle t \rangle}$ for each timestep in the output ($t=1, \ldots, T_y$).</p>
<img src="/2018/12/06/dl/第五门课-第三周/resources/2868A3ED41699CAA4210EAF9E26E0328.jpg">
<p>Here are some properties of the model that you may notice: </p>
<ul>
<li><p>模型分两个LSTM层，下面一个双向LSTM根据前向后向计算的激活值a，然后匹配上各个时间步的注意力的长度，产出context，第二层LSTM是用context和前一个时间步的输出$S^{&lt; t-1 &gt;}$输出最终值<br>There are two separate LSTMs in this model (see diagram on the left). Because the one at the bottom of the picture is a Bi-directional LSTM and comes <em>before</em> the attention mechanism, we will call it <em>pre-attention</em> Bi-LSTM. The LSTM at the top of the diagram comes <em>after</em> the attention mechanism, so we will call it the <em>post-attention</em> LSTM. The pre-attention Bi-LSTM goes through $T_x$ time steps; the post-attention LSTM goes through $T_y$ time steps. </p>
</li>
<li><p>The post-attention LSTM passes $s^{\langle t \rangle}, c^{\langle t \rangle}$ from one time step to the next. In the lecture videos, we were using only a basic RNN for the post-activation sequence model, so the state captured by the RNN output activations $s^{\langle t\rangle}$. But since we are using an LSTM here, the LSTM has both the output activation $s^{\langle t\rangle}$ and the hidden cell state $c^{\langle t\rangle}$. However, unlike previous text generation examples (such as Dinosaurus in week 1), in this model the post-activation LSTM at time $t$ does will not take the specific generated $y^{\langle t-1 \rangle}$ as input; it only takes $s^{\langle t\rangle}$ and $c^{\langle t\rangle}$ as input. We have designed the model this way, because (unlike language generation where adjacent characters are highly correlated) there isn’t as strong a dependency between the previous character and the next character in a YYYY-MM-DD date. </p>
</li>
<li><p>We use $a^{\langle t \rangle} = [\overrightarrow{a}^{\langle t \rangle}; \overleftarrow{a}^{\langle t \rangle}]$ to represent the concatenation of the activations of both the forward-direction and backward-directions of the pre-attention Bi-LSTM. </p>
</li>
<li><p>The diagram on the right uses a <code>RepeatVector</code> node to copy $s^{\langle t-1 \rangle}$’s value $T_x$ times, and then <code>Concatenation</code> to concatenate $s^{\langle t-1 \rangle}$ and $a^{\langle t \rangle}$ to compute $e^{\langle t, t’}$, which is then passed through a softmax to compute $\alpha^{\langle t, t’ \rangle}$. We’ll explain how to use <code>RepeatVector</code> and <code>Concatenation</code> in Keras below. </p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="语音模型"><a href="#语音模型" class="headerlink" title="语音模型"></a>语音模型</h4><ul>
<li>CTC cost<ul>
<li>背景：很多时候语音输入比输出会多很多，如何在RNN中相同输入输出的模型中使用呢？</li>
<li>CTC 损失函数的一个基本规则是 将空白符之间的重复的字符折叠起来，再说清楚一些，我这里用下划线来表示这个特殊的空 白符(a special blank character)，这样就可以让输出和输入相同数量了</li>
</ul>
</li>
</ul>
<h4 id="触发字检测"><a href="#触发字检测" class="headerlink" title="触发字检测"></a>触发字检测</h4><ul>
<li>生谱图特征得到特征向量$x^{<1>},x^{<2>,…}$</2></1></li>
<li>训练集如何标注：检测到关键词的以后就标注为1，未检测到就标注为0<img src="/2018/12/06/dl/第五门课-第三周/resources/EBCF128EB7F18F6FFFC673EA682EAFE1.jpg"></li>
<li>问题：这样导致0和1的数量很不平衡，简单的处理方式：比 起只在一个时间步上去输出 1，其实你可以在输出变回 0 之前，多次输出 1，或说在固定的 一段时间内输出多个 1</li>
<li>练习题：<ul>
<li>网络架构<img src="/2018/12/06/dl/第五门课-第三周/resources/3927A66C61616AB5B8F15E0F82790B80.jpg"></li>
<li>首先通过生谱图将声音转换为特征向量</li>
<li>然后开始使用了CNN，使5511时间步减小到1375，这个在RNN中大量减小了复杂大</li>
<li>然后用两层RNN，进行0，1预测输出，注意并没有使用双向RNN，因为语音说完就要快速给出结果，并不是等10s说完后才给</li>
<li>Here’s what you should remember:<ul>
<li>Data synthesis is an effective way to create a large training set for speech problems, specifically trigger word detection. </li>
<li>Using a spectrogram and optionally a 1D conv layer is a common pre-processing step prior to passing audio data to an RNN, GRU or LSTM.</li>
<li>An end-to-end deep learning approach can be used to built a very effective trigger word detection system. </li>
</ul>
</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/01/dl/第五门课-第二周/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/01/dl/第五门课-第二周/" itemprop="url">第五门课-第二周</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-01T18:37:09+08:00">
                2018-12-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/吴恩达课程总结/" itemprop="url" rel="index">
                    <span itemprop="name">吴恩达课程总结</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://github.com/yuancl/dl-algorithm/tree/master/5-RecurrentNeuralNetworks/Week2" target="_blank" rel="noopener">习题代码:Operations-on-word- vectors</a></p>
<h4 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h4><ul>
<li>背景：one-hot的方式不能表示相近词之间的相似度，比如用语言模型得到 i want a glass of orange juice,如果换成 i want a glass of apple …，  如果是one-hot就不能得到好的结果</li>
<li>原因：one-hot向量通常只有一位为非0，所以它们之间的内积为0，多维空间表示就是垂直的，没有相关性</li>
<li>词嵌入：把之前的10000+维度的one-hot转换为300维的相关特征，把该词嵌入300维的空间中<img src="/2018/12/01/dl/第五门课-第二周/resources/C41732C6B42FCD3CC1C0DF349DDA0403.jpg">
<img src="/2018/12/01/dl/第五门课-第二周/resources/A21F2659F855954FA841C05F9E5F0FB7.jpg"></li>
<li>t-SNE算法，将多维空间映射到2为空间中，如上图左图</li>
</ul>
<h4 id="使用词嵌入-命名实体识别"><a href="#使用词嵌入-命名实体识别" class="headerlink" title="使用词嵌入(命名实体识别)"></a>使用词嵌入(命名实体识别)</h4><ul>
<li>背景：Robert Lin is an apple farmer 与 Robert Lin is a durian cultivator(Robert Lin 是一个榴莲培育家)榴莲培育家训练样本很少包含该词，很可能不能做到命名实体的识别</li>
<li>词嵌入做迁移学习优点及步骤<ul>
<li>泛化性比较好</li>
<li>先从大量的文本集中学习词嵌入。一个非常大的文本集，这个可以解决你的训练样本比较少，比如根本没有durian(榴莲)这个词汇的训练样本</li>
<li>你可以用这些词嵌入模型把它迁移到你的新的只有少量标注训练集的任务中，比如说用这个300维的词嵌入来表示你的单词，比one-hot更低维</li>
<li>是否进行微调？如果你有大量的训练数据集可以考虑微调已有的嵌入词</li>
<li>通常迁移任务 A-&gt;B，通常是A中有大量数据集，B中只有少量数据集这种情况效果比较好<ul>
<li>比如在机器翻译领域词嵌入用得比较少，原因就是我们有大量的翻译样本</li>
</ul>
</li>
</ul>
</li>
<li>词嵌入和人脸编码类比<ul>
<li>相同点，都是通俗理解都是通过编码，得到标识一个物体的唯一向量</li>
<li>不同点，词嵌入是固定的词汇，而人脸编码是给定任意的图片都可以进行编码<img src="/2018/12/01/dl/第五门课-第二周/resources/DA8D7754F10BE0DAA530258811F9AC1E.jpg">
</li>
</ul>
</li>
</ul>
<h4 id="词嵌入的特性"><a href="#词嵌入的特性" class="headerlink" title="词嵌入的特性"></a>词嵌入的特性</h4><ul>
<li>类比推理<ul>
<li>解决问题：man对应woman，那么king对应什么？</li>
<li>目标函数：$e_{man}-e_{woman}\approx e_{king}-e_?$,也就是Find work w：argmax sim($e_w, e_{king}-e_{man}+e_{woman}$)</li>
<li>实现算法：<ul>
<li>该场景的四个词如果映射为二维空间，一般会是平行四边形，通常只有一维数据有一定差异<img src="/2018/12/01/dl/第五门课-第二周/resources/52B643F271BBA6B8880FF8C6499D3894.jpg"></li>
<li>余弦相识度：<ul>
<li>公式：sim(u,v)=$\frac{u^T*v}{||u||_2||v||_2}$=cos($\theta$)</li>
<li>理解：两个向量之间的角度的余弦是衡量它们有多相似的指标，这里就是衡量两个词嵌入向量的相似度<img src="/2018/12/01/dl/第五门课-第二周/resources/CAD5599965AB00A4C6BFCD3BBFC387F5.jpg">
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="嵌入矩阵-Embedding-Matrix"><a href="#嵌入矩阵-Embedding-Matrix" class="headerlink" title="嵌入矩阵(Embedding Matrix)"></a>嵌入矩阵(Embedding Matrix)</h4><ul>
<li>当你应用算法来学习词嵌入时，实际上是学习一个嵌入矩阵，我们的目标就是学习一个嵌入矩阵E</li>
<li>有了嵌入矩阵，然后乘以one-hot向量，就能够得到$E*o_j=e_j$,$e_j$就是表示我们想要的词嵌入</li>
<li>但实践中一般不会$E*o_j=e_j$,$e_j$这样计算，一般会有一个专门的函数来单独查找矩阵E的某列<img src="/2018/12/01/dl/第五门课-第二周/resources/C6433150B98AB53156F78848934AB4F2.jpg">
</li>
</ul>
<h4 id="学习词嵌入"><a href="#学习词嵌入" class="headerlink" title="学习词嵌入"></a>学习词嵌入</h4><p>建立一个语言模型是学习词嵌入的好方法</p>
<ul>
<li>方法1：<br>$Eo_j=e_j$使用这种方法来学习词嵌入矩阵，训练集中有很多序列语句，然后放入神经网络中，来学习E，这种方法比较复杂，最后会有1800维300*6的向量进入隐藏层，然后进入10000维的softmax层<img src="/2018/12/01/dl/第五门课-第二周/resources/34DABF58D294A16191D22C8DED48AEBB.jpg"></li>
<li>方法2(采用固定窗口)：<br>只取前4个词(作为一个窗口)来训练，用一个固定的历史窗口就意味着你可以处理任意长度的句子，因为输入的维度(1200)总是固定的<img src="/2018/12/01/dl/第五门课-第二周/resources/97D2B0D30C4C77C50ADF9A5BB86C1C70.jpg"></li>
<li>方法3(前后数量不固定)<br>可以前后个4个词，<br>或者就附件一个词等(Skip-Gram思想)…<img src="/2018/12/01/dl/第五门课-第二周/resources/74CF627BF0907DB5F4DD4F0A47FC905E.jpg"></li>
<li>总结：<br>如果你真想<font color="red">建立一个语言模型</font>，用目标词的前几个单词作为上下文是常见做法(上图编号9所示)。但如果你的<font color="red">目标是学习词嵌入</font>，那么你就可以用这些其他类型的上下文(上图编号10所示)，它们也能得到很好的词嵌入</li>
</ul>
<h4 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h4><p>上面都是讲的通过学习一个神经语言模型来得到更好的词嵌入，而<br>Word2Vec是更简单高效的方式来学习这种类型的嵌入</p>
<ul>
<li><p>Skip-Gram</p>
<ul>
<li>在 Skip-Gram模型中，我们要做的是抽取上下文和目标词配对，来构造一个<font color="red">监督学习问题</font><ul>
<li>但是构造这个监督学习问题的目标并不是想要解决这个监督学习问题本身，而是想要使用这个学习问题来学到一个好的词嵌入模型<font color="red">(构建一个监督学习问题，实际想解决学习嵌入词模型)</font></li>
</ul>
</li>
<li>步骤：我们随机选择一个词作为上下文c，然后在随机抽取一个词作为target y<font color="purple">(随机选择y，我理解只是重点强调这个算法本身，实际情况如果要训练得到一个好的词嵌入，必然是不能随机的，是需要选真实的y)</font>，然后就是建立x(c)–&gt;y的映射<ul>
<li>c和E相乘，得到c的词嵌入向量，然后此向量通过softmax进行预测，最后和target建立Loss function来达到学习E的目的</li>
</ul>
</li>
<li>softmax模型，预测不同目标词的概率:softmax:p(t|c)=$\frac{e^{\theta^{T}_{t}e_c}}{\sum^{10,000}_{j=1}e^{\theta{^T_je_c}}}$输出是10000(词典中单词数量)为的向量<ul>
<li>$\theta_t$是一个与输出t有关的参数，即某个词t和标签相符的概率是多少</li>
</ul>
</li>
<li>softmax损失函数：$L(\hat y,y)=-\sum^{10,000}_{i=1}y_ilog\hat y_i$ <font color="red">其中y是one-hot向量，$\hat y$是10.000维的各个词的预测输出概率</font><ul>
<li><font color="red">这是softmax常有的损失模型</font>，$\hat y$是各个分类的概率，可以画一下log函数图形来理解该损失函数，如果$\hat y_i$为1，也就是第i个分类为概率为1，这个时候$L(\hat y,y)$就为0，损失值最小</li>
<li><font color="red">直观理解损失函数</font>：<font color="blue">就是y和$\hat y$都是10000维的，Loss计算就是分别将这两个向量做”内积”，要使”内积”最小，假设第i维，$y_i=1$这个时候softmax的输出$\hat y_i=1$-&gt;$\log\hat y_i=0$(就是所第i维也正是softmax输出最大的分类的概率)-&gt;$y_i\log\hat y_i=0$-&gt;Loss最小</font></li>
</ul>
</li>
<li>缺点(problem)：<br>在 softmax 模型中，每次你想要计算这个概率，你需要对你词汇表中的所有 10,000 个词做求和计算</li>
<li>优化方案：<ul>
<li>分级softmax分类器：不用一下子告诉属于10,000类中的哪一类，可以先告诉属于左边的5000中的一类还是右边5000中的，然后是2500….,这样来构造一颗分类树<img src="/2018/12/01/dl/第五门课-第二周/resources/95F9428AAE4FEA373515DBC5F8A622B8.jpg">
</li>
</ul>
</li>
</ul>
</li>
<li><p>CBOW模型</p>
<ul>
<li>不同版本的 Word2Vec模型，Skip-Gram只是其中的一个，另一个叫做 CBOW，即连续词袋模型(Continuous Bag-Of-Words Model)，它获得中间词两边的的上下 文，然后用周围的词去预测中间的词</li>
<li><font color="red">CBOW 是从原始语句推测目标字词;而Skip-Gram正好相反，是从目标字词推 测出原始语句</font></li>
<li>CBOW对小型数据库比较合适，而Skip-Gram在大型语料中表现更好<img src="/2018/12/01/dl/第五门课-第二周/resources/FF2E1F1A0F94A8221B8B801595C6D5EC.jpg">
</li>
</ul>
</li>
<li><p>负采样(Negative Sampling)</p>
<ul>
<li>背景：之前的softmax分类器中，解决每次都要计算所有样本的问题</li>
<li>Model：选择一个正样本和K个负样本，将softmax的分类转化为一系列二分类问题，只计算其中的正负样本的二分类问题<ul>
<li>正负样本作为输入x，然后用$P(y=1|c,t)=sigmoid(\theta^T_te_c)$进行二分类计算<img src="/2018/12/01/dl/第五门课-第二周/resources/7D0E816C9C5C70572D286ADC59924886.jpg"></li>
</ul>
</li>
<li>负样本k的选取<ul>
<li>语库中的经验频率进行采样，这种会有很多like，the等高频词出现</li>
<li>另一个极端就是1除以词汇表总词数，均匀的抽取样本，但这样对英语单词的分布是没有代表性的</li>
<li>采用一种处于完全独立分布和训练集的观测分布两个极端之间<img src="/2018/12/01/dl/第五门课-第二周/resources/5A531D6FD207171B174D364AF500C77F.jpg">
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Glove-Glove-word-vectors"><a href="#Glove-Glove-word-vectors" class="headerlink" title="Glove(Glove word vectors)"></a>Glove(Glove word vectors)</h4><ul>
<li>定义：定义上下文和目标词为任意两个位置相近的单词，假设是左右各 10 词的距离，那么$X_{ij}$就是一个能够获取单词𝑖和单词𝑗出现位置相近时或是彼此接近的频率的计数器。</li>
<li>Glove模型就是进行优化，将他们之间的差距最小化处理<ul>
<li><font color="blue">就是通过一种优化方法，对目前方程进行优化，从而间接学习到词嵌入矩阵</font><img src="/2018/12/01/dl/第五门课-第二周/resources/E4BFB45184A8FE9F410D8920BDD53ED4.jpg"></li>
</ul>
</li>
<li>函数f选择原则：对加权函数f的选择有着启发性的原则，就是既不给这些词(this，is，of，a)过分的权重，也不给这些不常用词(durion)太小的权值</li>
<li>问题：<ul>
<li>你不能保证嵌入向量的独立组成部分是能够理解的</li>
<li>你不能保证这些用来表示特征的轴能够等同于人类可能简单理解的轴，具体而言，第一个特征 可能是个Gender、Roya、Age、Food Cost 和 Size的组合<font color="red">(并不是我们所理解的单纯的Gender维)</font>，它也许是名词或是一个行为动词 和其他所有特征的组合，所以很难看出独立组成部分<img src="/2018/12/01/dl/第五门课-第二周/resources/31256B83B3F1A2CF14D942BFECB3ADFB.jpg">
</li>
</ul>
</li>
</ul>
<h4 id="情感分类"><a href="#情感分类" class="headerlink" title="情感分类"></a>情感分类</h4><ul>
<li>定义：情感分类任务就是看一段文本，然后分辨这个人是否喜欢他们在讨论的这个东西</li>
<li>简单模型：采用的是平均值单元，适合任意长度的评论<ul>
<li><font color="blue">重点理解词嵌入应用的思想：将单词通过词嵌入矩阵得到词嵌入，简单理解为将字符或者是单词映射为数值的一种方法，然后使用End-to-End思想，通过softmax就直接预测输出</font><img src="/2018/12/01/dl/第五门课-第二周/resources/3769B484BF94E06ABF4038346AA63078.jpg"></li>
<li>问题：没有考虑词序，<font color="red">因为均值模型就是把各个单词各个维数值相加然后再平均</font><ul>
<li>比如not good,或者 good…not ,就不能区分了</li>
</ul>
</li>
</ul>
</li>
<li>RNN模型解决词序问题 <ul>
<li>此模型是一个one-many的模型<ul>
<li><font color="blue">直观理解就是，RNN本身就是有一个词一个词有顺序的输入，上一个词对下一个词有影响，所以就能够反映词序问题</font><img src="/2018/12/01/dl/第五门课-第二周/resources/62FD68B5D4C30C44B61209A772F5BD86.jpg">
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="词嵌入除偏"><a href="#词嵌入除偏" class="headerlink" title="词嵌入除偏"></a>词嵌入除偏</h4><ul>
<li>定义：比如通过词嵌入，Man:Computer Programmer， 同时输出 Woman:Homemaker</li>
<li><font color="blue">解决方法：主要是在词向量中寻找出哪些维度的偏见相关的维度(bias axis)，然后在这些维度上进行调整</font></li>
<li><font color="blue">重点理解：单词到词向量的映射，实际上就是映射到n维空间(n为词向量矩阵)中的一个点,然后看点与点之间的距离来判断词与词之间的相似度</font></li>
<li><p><font color="red">达到目的：让无性别的词到有性别的词的点的距离是相等的,比如Computer Programmer,doctor分别到man,woman的距离是相等的</font></p>
<ul>
<li>找出偏见趋势<ul>
<li>确定哪些轴(哪些维度)，对于性别歧视这种情况来说，我们能做的是$e_{he} − e_{she}$，因为它们的性别不同，然后 将$e_{male}−e_{female}$，然后将这些值取平均(上图编号2所示)，将这些差简单地求平均。这个趋势(上图编号3所示)看起来就是性别趋势或说是偏见趋势<ul>
<li>non bias就是垂直于偏见轴的方向<img src="/2018/12/01/dl/第五门课-第二周/resources/FE259E9589DE8B2F3259660CB076E522.jpg"></li>
</ul>
</li>
</ul>
</li>
<li><p>中和步骤：某些无性别歧视的词语，让他们在non bias轴上进行靠拢，尽量减少映射在bias轴上的距离来减少性别歧视问题</p>
<ul>
<li>比如Computer Programmer,doctor应该是无性别关系的，所以应该尽量靠近non bias轴，<font color="blue">这样的理想效果就是到达空间中man和woman的点距离是相等的</font><img src="/2018/12/01/dl/第五门课-第二周/resources/761E18C6CBBE247CE32CCD5CE15F5E54.jpg">
</li>
</ul>
</li>
<li><p>均衡步：<br>让有偏见的词进行，移动与中轴线等距的一些点上，这样让它们在bias轴上都有一致的相似度，图中由1移到图2的两点</p>
<ul>
<li><font color="blue">直观理解：也是为了让无偏见词，比如上面说的Computer Programmer,doctor达到分性别关系的词man,woman距离相等</font><img src="/2018/12/01/dl/第五门课-第二周/resources/18EA7A252EAD3B3F90796A808AAAC7F5.jpg">
<ul>
<li>经过统计这种偏见词实际上是很少的，需要均衡的词是很少的</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/images/favicon.ico" alt="雷哥">
          <p class="site-author-name" itemprop="name">雷哥</p>
           
              <p class="site-description motion-element" itemprop="description">不积跬步无以至千里</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">43</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">12</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">10</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-雷哥"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">雷哥</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Gemini
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
    <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  






  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
