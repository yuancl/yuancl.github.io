<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css">


  <meta name="keywords" content="Hexo, NexT">








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2">






<meta name="description" content="不积跬步无以至千里">
<meta property="og:type" content="website">
<meta property="og:title" content="雷哥的博客">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="雷哥的博客">
<meta property="og:description" content="不积跬步无以至千里">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="雷哥的博客">
<meta name="twitter:description" content="不积跬步无以至千里">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '雷哥'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>雷哥的博客</title>
  














</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">雷哥的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/02/03/rl/基于模型的学习和规划/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/03/rl/基于模型的学习和规划/" itemprop="url">基于模型的学习和规划</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-03T13:11:12+08:00">
                2019-02-03
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/强化学习/" itemprop="url" rel="index">
                    <span itemprop="name">强化学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul>
<li>前面的算法都是与真实的环境进行交互，个体并不试图去理解环境动力学</li>
<li>如果能构建一个较为准确地模拟环境动力学特 征的模型或者问题的模型本身就类似于一些棋类游戏是明确或者简单的，个体就可以通过<font color="blue">构建这样的模型来模拟其与环境的交互</font>，这种依靠模型模拟而不实际与环境交互的过程<font color="blue">类似于“思考”过程</font><ul>
<li>通过思考，个体可以对问题进行规划、在与环境实际交互时<font color="blue">搜索交互可能产生的各种 后果</font>并从中选择对个体有利的结果<ul>
<li><font color="purple">我理解是通过模拟的环境，得到各自行为，然后与真实环境进行交互的时候，从这些模拟环境得到的行为中搜索，选择最优的结果 </font>


</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="环境的模型"><a href="#环境的模型" class="headerlink" title="环境的模型"></a>环境的模型</h3><p>模型是个体构建的对于环境动力学特征的表示</p>
<h4 id="基于模型的强化学习流程"><a href="#基于模型的强化学习流程" class="headerlink" title="基于模型的强化学习流程"></a>基于模型的强化学习流程</h4><ul>
<li><p>当个体得到了一 个较为准确的描述环境动力学的模型时，它在与环境交互的过程中，既可以通过实际交互来提高模型的准确程度，也可以在交互间隙利用构建的模型进行思考、规划，决策出对个体有力的行为</p>
<img src="/2019/02/03/rl/基于模型的学习和规划/resources/FAAFC16238D8AB743A35514D8FA86155.jpg">
</li>
<li><p>学习一个模型相当于丛经历 $S_1, A_1, R_2, . . . , S_T$ 中通过监督学习得到一个模型 $M_η$</p>
<ul>
<li>训练数据为<br>$S_1,A_1 →R_2,S_2$<br>$S_2,A_2 →R_3,S_3$..<br>$S_{T−1},A_{T−1} →R_T,S_T$</li>
</ul>
</li>
<li><p>使用近似的模型解决强化学习问题与使用价值函数或策略函数的近似表达来解决强化学习问题并不冲突，它们是从不同角度来近似求解一个强化学习问题</p>
<ul>
<li>当构建一个模型比构建近似价值函数 或近似策略函数更方便时，那么使用近似模型来求解会更加高效</li>
</ul>
</li>
<li><p>特别注意模型参数要随着个体与环境交互而不断地动态更新，<font color="blue">即通过实际经历要与使用模型产生的虚拟经历相结合来解决问题</font></p>
<ul>
<li>这就催生了一类整合了学习与规划的强化学习算法——Dyna</li>
</ul>
</li>
</ul>
<h4 id="Dyna算法-整合学习与规划"><a href="#Dyna算法-整合学习与规划" class="headerlink" title="Dyna算法(整合学习与规划)"></a>Dyna算法(整合学习与规划)</h4><ul>
<li>Dyna 算法从实际经历中学习得到模型，同时<font color="blue">联合</font>使用实际经历和基于模型采样得到的虚拟经历来学习和规划，更新价值和 (或) 策略函数<img src="/2019/02/03/rl/基于模型的学习和规划/resources/DC72D9AC050BA5334B17A5B74CDAA7C9.jpg"></li>
<li>伪代码<img src="/2019/02/03/rl/基于模型的学习和规划/resources/B080914282D2D2B32DECD04A75A4DFDB.jpg">
<ul>
<li>我的理解：<ul>
<li>从开始是通过真实环境去得到模型环境Model</li>
<li>下面会使用Model得到虚拟的经历，结合上面真实经历来更新Q(S,A)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="基于模拟的搜索"><a href="#基于模拟的搜索" class="headerlink" title="基于模拟的搜索"></a>基于模拟的搜索</h4><ul>
<li>前向搜索形式<br>在强化学习中，基于模拟的搜索 (simulation-based search) 是一种前向搜索形式,它从当前 时刻的状态开始，<font color="blue">利用模型来模拟采样，构建一个关注短期未来的前向搜索树</font>，将构建得到的搜索树作为一个<font color="blue">学习资源</font>，使用不基于模型的强化学习方法来寻找当前状态下的最优策略<ul>
<li><font color="purple">我的理解就是通过模型来构造一颗状态行为树，然后搜索在当前状态下的最优行为，也就是最优策略</font></li>
</ul>
</li>
<li>如果使用蒙特卡罗学习方法则称为蒙特卡罗搜索，如果使用 Sarsa 学习方法，则称为 TD 搜索</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/02/02/rl/基于策略梯度的深度强化学习/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/02/rl/基于策略梯度的深度强化学习/" itemprop="url">基于策略梯度的深度强化学习</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-02T23:10:15+08:00">
                2019-02-02
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/强化学习/" itemprop="url" rel="index">
                    <span itemprop="name">强化学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul>
<li><p>在行为空间规模庞大或者是连续行为的情况下，基于价值的强化学习将很难学习到一个好的结果，这种情况下可以直接进行策略的学习</p>
<ul>
<li>即将策略看成是状态和行为的带参数的策略函 数，通过建立恰当的目标函数、利用个体与环境进行交互产生的奖励来学习得到策略函数的参数。</li>
<li>策略函数针对连续行为空间将可以<font color="blue">直接产生具体的行为值，进而绕过对状态的价值的学习</font></li>
</ul>
</li>
<li><p>在实际应用中通过<font color="blue">建立分别对于状态价值的近似函数和策略函数</font></p>
<ul>
<li>使得一方面可以基于价值函 数进行策略评估和优化</li>
<li>另一方面优化的策略函数又会使得价值函数更加准确的反应状态的价 值，两者相互促进最终得到最优策略</li>
</ul>
</li>
</ul>
<h3 id="基于策略学习的意义"><a href="#基于策略学习的意义" class="headerlink" title="基于策略学习的意义"></a>基于策略学习的意义</h3><h4 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a>背景</h4><p>基于价值的强化学习虽然能出色地解决很多问题，但面对行为空间连续、观测受 限、随机策略的学习等问题时仍然显得力不从心</p>
<ul>
<li><p>问题1(行为空间连续)</p>
<ul>
<li>基于近似价值函数的学习可以较高效率地解决连续状态空间的强化学习问题，但其行为空间仍然是离散的</li>
<li>如果行为空间是连续的，可以认为单纯基于价值函数近似的强化 学习无法解决连续行为空间的问题，其中每一 个方向上的分量可以是 [-1,1] 之间的任何连续值。在这个例子 (图 7.1) 中，行为由两个特征来描 述，其中每一个特征具体的值是连续的。比如：<img src="/2019/02/02/rl/基于策略梯度的深度强化学习/resources/249F71A8DFF7CC68F09189A2CBD8FA32.jpg">
</li>
</ul>
</li>
<li><p>问题2(观测受限)</p>
<ul>
<li>此外，在使用特征来描述状态空间中的某一个状态时，有可能因为个体观测的限制或者建 模的局限，导致本来不同的两个状态却拥有相同的特征描述，进而导致无法得到最优解<ul>
<li>在这种情况下，由于个体对于状态观测的特征不够多，导致了多个状 态发生重名情况，进而导致基于价值的学习得不到最优解<img src="/2019/02/02/rl/基于策略梯度的深度强化学习/resources/62B8EFCF8DD2ED23F7457C8AE9F9D726.jpg">
</li>
</ul>
</li>
</ul>
</li>
<li><p>问题3(随机策略的学习)</p>
<ul>
<li>基于价值的学习对应的最优策略通常是确定性策略，因为其是从众多行为价值中选择一个 最大价值的行为，而有些问题的最优策略却是随机策略，这种情况下同样是无法通过基于价值的学习来求解的。</li>
<li>这其中最简单的一个例子是人们小时候经常玩的“石头剪刀布”游戏。对于这个 游戏，玩家的最优策略是随机出石头剪刀布中的一个，因为一旦你遵循一个确定的策略，将很容 易被对手发现并利用进而输给对方</li>
</ul>
</li>
</ul>
<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><ul>
<li>策略 π 可以被被描述为一个包含参数 θ 的函数<br>$π_θ(s,a) = P[a | s,θ]$</li>
<li>含义：<br>策略函数 $π_θ$ 确定了在给定的状态和一定的参数设置下，采取任何可能行为的概率，<font color="blue">是一个概率密度函数</font><ul>
<li>在实际应用这个策略时，<font color="blue">选择最大概率对应的行为</font>或者以此为基础进行一定程度的采样探索。可以认为，参数 θ 决定了策略的具体形式。</li>
<li>因而求解基于策略的学习问题就转变为了如何确定策略函数的参数 θ。同样可以通过设计一个基于参数 θ 的目标函数 J(θ)，通过相应 的算法来寻找最优参数</li>
</ul>
</li>
</ul>
<h3 id="策略目标函数"><a href="#策略目标函数" class="headerlink" title="策略目标函数"></a>策略目标函数</h3><p>强化学习的目标就是让个体在与环境交互过程中获得尽可能多的累计奖励，一个好的策略 应该能准确反映强化学习的目标</p>
<ul>
<li>初始状态收获的期望<br>对于一个能够形成完整状态序列的交互环境来说，由于一个策 略决定了个体与环境的交互，因而可以设计目标函数 J1(θ) 为使用策略 πθ 时初始状态价值 (start value):<br>$J_1(θ) = V_{π_θ} (s_1) = E_{π_θ} [G_1]$</li>
<li>有些环境是没有明确的起始状态和终止状态，个体持续的与环境进行交互。在这种情况下可以使 用平均价值 (average value) 或者每一时间步的平均奖励 (average reward per time-step) 来设计策<br>略目标函数:<ul>
<li>$d^{π<em>θ} (s)$ 是基于策略 $π</em>θ$生成的马尔科夫链关于状态的静态分布<img src="/2019/02/02/rl/基于策略梯度的深度强化学习/resources/8C3E7DC59B8D8ED2899C674AD3D6EE09.jpg"></li>
</ul>
</li>
<li>与价值函数近似的目标函数 不同，策略目标函数的值越大代表着策略越优秀。可以使用与梯度下降相反的梯度上升来求解最优参数</li>
<li>假设现在有一个单步马尔科夫决策过程，对应的强化学习问题是个体与环境每产生一个行 为交互一次即得到一个即时奖励 r = Rs,a，并形成一个完整的状态序列。根据公式 (7.1)，策略目 标函数为<img src="/2019/02/02/rl/基于策略梯度的深度强化学习/resources/28D0F3C14799FD88BB04860C6B0192D6.jpg"></li>
<li>分值函数(score function)<br>上式中 $∇<em>θlogπ</em>θ(s,a)$ 称为分值函数 (score function)。<ul>
<li>存在如下的策略梯度定理:<br>对于任何 可微的策略函数 $π_θ(s, a)$ 以及三种策略目标函数 $J = J_1, J_{avV} 和 J_{avR}$ 中的任意一种来说，策略 目标函数的梯度 (策略梯度) 都可以写成用分值函数表示的形式:<img src="/2019/02/02/rl/基于策略梯度的深度强化学习/resources/F403A2CB9D3BD5E27FBDB2DC8B8F5BB4.jpg"></li>
<li>意义<ul>
<li><font color="blue">分值越高意味着在当前策略下对应行为被选中的概率越大</font></li>
<li>算法将结合某一行为的分值对应的奖励来得到对应的梯度，并在此基础上调整参 数，<font color="blue">最终使得奖励越大的行为对应的分值越高</font></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Actor-Critic-算法"><a href="#Actor-Critic-算法" class="headerlink" title="Actor-Critic 算法"></a>Actor-Critic 算法</h3><h4 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h4><ul>
<li>Actor-Critic 算法的名字很形象，它包含一个策略函数和行为价值函数<ul>
<li>其中策略函数充当 演员 (Actor), 生成行为与环境交互</li>
<li>行为价值函数充当 (Critic)，负责评价演员的表现，并指导 演员的后续行为动作</li>
</ul>
</li>
<li>Critic 的行为价值函数是基于策略 $π_θ$ 的一个近似<br>$Q_w(s, a) ≈ Q_{π_θ} (s, a)$</li>
<li>基于此，Actor-Critic 算法遵循一个近似的策略梯度进行学习<img src="/2019/02/02/rl/基于策略梯度的深度强化学习/resources/832F18299F2131CC68EA719ECB28EFE0.jpg">
</li>
</ul>
<h4 id="QAC算法-最基本的基于行为价值-Q-的-Actor-Critic-算法"><a href="#QAC算法-最基本的基于行为价值-Q-的-Actor-Critic-算法" class="headerlink" title="QAC算法(最基本的基于行为价值 Q 的 Actor-Critic 算法)"></a>QAC算法(最基本的基于行为价值 Q 的 Actor-Critic 算法)</h4><img src="/2019/02/02/rl/基于策略梯度的深度强化学习/resources/BAE69DD8E4E51BE876EA997076555EE1.jpg">
<h4 id="参数更新"><a href="#参数更新" class="headerlink" title="参数更新"></a>参数更新</h4><ul>
<li>Critic 的函数 $V_w(s)$ 的参数 w更新<img src="/2019/02/02/rl/基于策略梯度的深度强化学习/resources/A499D8701005947722D2C5F6A40E3C49.jpg"></li>
<li>策略函数 $π_θ(s,a)$ 的参数θ更新<img src="/2019/02/02/rl/基于策略梯度的深度强化学习/resources/9796183388A806045FA369B4ADA63A36.jpg">
</li>
</ul>
<h3 id="深度确定性策略梯度-DDPG-算法"><a href="#深度确定性策略梯度-DDPG-算法" class="headerlink" title="深度确定性策略梯度(DDPG)算法"></a>深度确定性策略梯度(DDPG)算法</h3><p>DDPG算法能较为稳定地解决连续行为空间下强化学习问题</p>
<h4 id="算法理解"><a href="#算法理解" class="headerlink" title="算法理解"></a>算法理解</h4><ul>
<li>深度确定性策略梯度算法是使用深度学习技术、同时基于 Actor-Critic 算法的确定性策略算法</li>
<li>该算法中的 Actor 和 Critic 都使用深度神经网络来建立近似函数</li>
<li>由于该算法可以直接从 Actor 的策略生成确定的行为而不需要依据行为的概率分布进行采样而被称为确定性策略</li>
<li>噪声函数<br>该算法在学习阶段通过在确定性的行为基础上增加一个噪声函数而实现在确定性行为周围的小范围 内探索</li>
<li>备份了一套参数<ul>
<li>该算法还为 Actor 和 Critic 网络各备份了一套参数<font color="blue">用来计算行为价值的期待值</font>以更稳定地提升 Critic 的策略指导水平。使用备份参数的网络称为目标网络，其对应的参数每次更新的幅度很小</li>
<li>另一套参数对应的 Actor 和 Critic 则用来生成实际交互的行为以及计算相应 的策略梯度，这一套参数每学习一次就更新一次。这种双参数设置的目的是为了减少因近似数据 的引导而发生不收敛的情形。</li>
</ul>
</li>
<li>这四个网络具体使用的情景为<ul>
<li>Actor 网络:根据当前状态 $s_0$ 生成的探索或不探索的具体行为$a_0$;</li>
<li>Target Actor 网络:根据环境给出的后续状态$s_1$ 生成预估价值用到的$a_1$;</li>
<li>Critic 网络:计算状态$s_0$和生成的行为$a_0$ 对应的行为价值;</li>
<li>Target Critic 网络:根据后续状态$ s_1,a_1 生成用来计算目标价值 y = Q(s_0, a_0) 的 Q′(s_1, a_1)$</li>
</ul>
</li>
</ul>
<h4 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h4><img src="/2019/02/02/rl/基于策略梯度的深度强化学习/resources/83B47D21AA2E85D11C6D3C28FA6816ED.jpg">
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/02/01/rl/价值函数的近似表示/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/02/01/rl/价值函数的近似表示/" itemprop="url">价值函数的近似表示</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-01T13:11:15+08:00">
                2019-02-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/强化学习/" itemprop="url" rel="index">
                    <span itemprop="name">强化学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul>
<li><p>问题的复杂性</p>
<ul>
<li>本章之前的内容介绍的多是规模比较小的强化学习问题，生活中有许多实际问题要复杂得 多，有些是属于状态数量巨大甚至是连续的，有些行为数量较大或者是连续的。这些问题要是使 用前几章介绍的基本算法效率会很低，甚至会无法得到较好的解决</li>
<li>解决这类问题的常用方法是不再使用字典之类的查表式的方法来存储状态或行为的价值，而 是引入适当的参数，选取恰当的描述状态的特征，通过构建一定的函数来近似计算得到状态或行 为价值</li>
<li>在引入近似价值函数后，强化学习中不管是预测问题还是控制问题，<font color="blue">就转变成近似函数的设 计以及求解近似函数参数这两个问题了</font></li>
</ul>
</li>
<li><p>状态价值 $v_π(s)$ 的近似表示<br>如果能建立一个函数 vˆ, 这个函数由参数 w 描述，它可以直接接受表示状态特征的连续变量 s 作为输入，通过计算得到一个状态的价值，通过调整参数 w 的取值，使得其符合基于某一策略 π 的最终状态价值，那么这个函数就是状态价值 $v_π(s)$ 的近似表示<br>$\hat v{(s,w)} ≈ v_π(s)$</p>
</li>
<li><p>行为价值 $q_π(s,a)$ 的近似表示<br>$\hat q(s,a,w) ≈ q_π(s,a)$</p>
<img src="/2019/02/01/rl/价值函数的近似表示/resources/326008A02C059001C56943A603459FEC.jpg">
</li>
</ul>
<h3 id="常用的近似价值函数-DQN算法"><a href="#常用的近似价值函数-DQN算法" class="headerlink" title="常用的近似价值函数(DQN算法)"></a>常用的近似价值函数(DQN算法)</h3><p>理论上任何函数都可以被用作近似价值函数，实际选择何种近似函数需根据问题的特点。比较常用的近似函数有线性函数组合、神经网络、决策树、傅里叶变换等等,这里会重点介绍基于深度学习的神经网络计数进行特征表示，包括卷积神经网络。</p>
<h4 id="DQN算法"><a href="#DQN算法" class="headerlink" title="DQN算法"></a>DQN算法</h4><p>DQN 算法主要使用经历回放 (experience replay) 来 实现价值函数的收敛。其具体做法为:</p>
<ul>
<li>个体能记住既往的状态转换经历，对于每一个完整状态序 列里的每一次状态转换，</li>
<li>依据当前状态的 $st 价值以 ε-贪婪策略选择一个行为 a_t，执行该行为得 到奖励 r_{t+1} 和下一个状态 s_{t+1}$</li>
<li>将得到的状态转换存储至记忆中</li>
<li>当记忆中存储的<font color="blue">容量足够大时，随机从记忆力提取一定数量的状态转换</font></li>
<li>用状态转换中下一状态来计算当前状态的目标价值，使用公式 (6.4) 计算目标价值与网络输出价值之间的均方差代价，使用小块梯度下降算法更 新网络的参数<img src="/2019/02/01/rl/价值函数的近似表示/resources/19A3C6F4B479FFAB3813A1B5D22D9D6A.jpg">
</li>
</ul>
<font color="blue">重点在理解它的loss函数：下一状态+$R_t$ 逼近 当前状态值，的计算方法</font>

<ul>
<li>伪代码<img src="/2019/02/01/rl/价值函数的近似表示/resources/85EE22FEE6B916F23806FCFEF85BEAAC.jpg">
<ul>
<li>该算法中 的状态 S 都由特征 φ(S) 来表示</li>
</ul>
</li>
</ul>
<h3 id="DDQN-double-deep-Q-network"><a href="#DDQN-double-deep-Q-network" class="headerlink" title="DDQN(double deep Q network)"></a>DDQN(double deep Q network)</h3><ul>
<li>背景：<br>DQN得了不俗的成绩，不过其并不能保证一直收敛，研究表明这种估计目标价值的算法过于乐观的高 估了一些情况下的行为价值，导致算法会将次优行为价值一致认为最优行为价值，最终不能收敛 至最佳价值函数</li>
<li>和DQN区别<br>该算法使用<font color="blue">两个架构相同的近似价值函数</font>：<ul>
<li>其中一个用来根据策略生成交互行为并随 时频繁参数 (θ)</li>
<li>另一个则用来生成目标价值, 其参数 (θ−) 每隔一定的周期进行更新。该算法绝 大多数流程与 DQN 算法一样，只是在更新目标价值时使用公式 (6.20):<img src="/2019/02/01/rl/价值函数的近似表示/resources/6905289A9CBBAEFBB91F2BBCFAA38F7A.jpg"></li>
<li>该式表明，DDQN 在生成目标价值时使用了生成交互行为并频繁更新参数的价值网络 Q(θ)， 在这个价值网络中挑选状态 S′下最大价值对应的行为 $A′_t$，随后再用状态行为对 $(S_t′, A′_t)$ 代入目标价值网络 Q(θ−) 得出目标价值。实验表明这样的更改比 DQN 算法更加稳定，更容易收敛值 最优价值函数和最优策略</li>
</ul>
</li>
<li>同样存在深度学习的问题<br>在使用神经网络等深度学习技术来进行价值函数近似时，有可能会碰到无法得到预期结果的情况,深度学习的问题这里也会遇到</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/01/31/rl/不基于模型的控制/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/31/rl/不基于模型的控制/" itemprop="url">不基于模型的控制</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-31T17:10:15+08:00">
                2019-01-31
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/强化学习/" itemprop="url" rel="index">
                    <span itemprop="name">强化学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><h4 id="本章内容"><a href="#本章内容" class="headerlink" title="本章内容"></a>本章内容</h4><ul>
<li>前一章内容讲解了个体在不依赖模型的情况下如何进行预测，也就是求解在给定策略下的 状态价值或行为价值函数</li>
<li>本章则主要讲解在不基于模型的条件下如何<font color="blue">通过个体的学习优化价值函数，同时改善自身行为的策略以最大化获得累积奖励的过程</font>，这一过程也称作不基于模型的 控制</li>
</ul>
<h4 id="问题举例"><a href="#问题举例" class="headerlink" title="问题举例"></a>问题举例</h4><ul>
<li>生活中有很多关于优化控制的问题，比如控制一个大厦内的多个电梯使得效率最高;</li>
<li>控制直 升机的特技飞行</li>
<li>机器人足球世界杯上控制机器人球员，围棋游戏等等</li>
<li>两类问题：<ul>
<li>所有的这些问题要么我们对其<font color="blue">环境动力学的特点无法掌握，但是我们可以去经历、去尝试构建理解环境的模型</font>;</li>
<li>要么虽然问题的<font color="blue">环境动力学特征是已知</font>的，但由<font color="blue">问题的规模太大以至于计算机根据一般算法无法高效 的求解</font>，除非使用采样的办法。</li>
<li>无论问题是属于两种情况中的哪一个，强化学习都能较好的解决</li>
</ul>
</li>
</ul>
<h4 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h4><ul>
<li>行为策略<br>我们把用来指导个体产生与环境进 行实际交互行为的策略称为行为策略</li>
<li>目标策略(优化策略)<br>把用来评价状态或行为价值的策略或者待优化的策略称 为目标策略。</li>
<li>现时策略学习 (on-policy learning)<br>个体在学习过程中优化的策略与自己的行为策略是同一个策略时，这种学习方 式称为现时策略学习 (on-policy learning)</li>
<li>借鉴策略学习 (off-policy learning)<br>个体在学习过程中优化的策略与自己的行为策略 是不同的策略时，这种学习方式称为借鉴策略学习 (off-policy learning)</li>
</ul>
<h4 id="算法的整体关系"><a href="#算法的整体关系" class="headerlink" title="算法的整体关系"></a>算法的整体关系</h4><ul>
<li>从<font color="blue">已知模型的、基于全宽度采样</font>的动态规划学习转至<font color="blue">模型未知的、基于采样的蒙特卡洛或时序差分学习</font>进行控制是<font color="purple">朝着高效解决中等规模实际问题的一个突破</font></li>
<li>基于这些思想产生了一 些经典的理论和算法:<ul>
<li>如不完全贪婪搜索策略、现时蒙特卡洛控制，现时时序差分学习</li>
<li>属于借鉴学习算法的 Q 学习等</li>
</ul>
</li>
</ul>
<h3 id="行为价值函数的重要性"><a href="#行为价值函数的重要性" class="headerlink" title="行为价值函数的重要性"></a>行为价值函数的重要性</h3><ul>
<li>生活学习哲学到强化学习<ul>
<li>生活中有些人喜欢做事但不善于总结，这类人一般要比那些勤于总结的人进步得慢，从策略 迭代的角度看，<ul>
<li>这类人策略更新<font color="blue">迭代的周期较长</font></li>
<li>有些人在<font color="blue">总结经验上过于勤快，甚至在一件事 情还没有完全定论时就急于总结并推理过程之间的关系，这种总结得到的经验有可能是错误的</font></li>
</ul>
</li>
<li>强化学习中的个体也是如此，为了让个体的尽早地找到最优策略，可以适当加快策略迭代的速 度:<ul>
<li>但是从一个不完整的状态序列学习则要注意不能过多地依赖状态序列中相邻状态行为对的 关系。</li>
<li>由于基于蒙特卡洛的学习利用的是完整的状态序列，<font color="blue">为了加快学习速度可以在只经历一个完整状态序列后就进行策略迭代</font></li>
<li>而在进行基于时序差分的学习时，<font color="blue">虽然学习速度可以更快，但 要注意减少对事件估计的偏差</font></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="ε−-贪婪策略"><a href="#ε−-贪婪策略" class="headerlink" title="ε− 贪婪策略"></a>ε− 贪婪策略</h3><h4 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a>背景</h4><ul>
<li>如上在动态规划策略迭代中，可看见：贪婪搜索策略在基于模型的动态规划算法中能收敛至最优 策略(价值)，但这在不基于模型、基于采样的蒙特卡罗或时序差分学习中却通常不能收敛至最优策略</li>
<li>因为后两者则仅能考虑到有限次数的、已经采样经历过 的状态，那些事实存在但还没经历过的状态对于后两者算法来说都是未探索的不被考虑的状态<ul>
<li>有些状态虽然经历过，但由于经历次数不多对其价值的估计也不一定准确。</li>
<li>如果存在一些价值更高的未被探索的状态使用贪婪算法将式中无法探索到这些状态，而已经经历过但价值较低的状 态也很难再次被经历，如此将无法得到最优策略</li>
</ul>
</li>
<li><font color="purple">总体来说，使用贪婪策略，在MC,TD中，因为是采样，会导致有些状态不能经历或者经历的次数不够，造成无法得到最优策略</font></li>
<li>贪婪策略产生问题的根源是<font color="blue">无法保证持续的探索</font>，为了解决这个问题，一种不完全的贪婪 (ε-greedy 搜索策略被提出，其基本思想就是保证能做到持续的探索，具体通过设置一个较小的 ε 值，使用 1 − ε 的概率贪婪地选择目前认为是最大行为价值的行为，而用 ε 的概率随机的从所有m 个可选行为中选择行为，即:<img src="/2019/01/31/rl/不基于模型的控制/resources/194778403757900F6531FF5CC25E0F59.jpg">
</li>
</ul>
<h3 id="现时策略蒙特卡罗控制"><a href="#现时策略蒙特卡罗控制" class="headerlink" title="现时策略蒙特卡罗控制"></a>现时策略蒙特卡罗控制</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>现时策略蒙特卡罗控制通过 <font color="blue">ε-贪婪策略采样一个或多个</font>完整的状态序列后，平均得出某一状态行为对的价值，并持续进行策略的评估和改善。通常可以在<font color="blue">仅得到一个完整状态序列后就进行一次策略迭代以加速迭代过程</font></p>
<h4 id="GLIE"><a href="#GLIE" class="headerlink" title="GLIE"></a>GLIE</h4><ul>
<li><p>背景<br>使用 ε-贪婪策略进行现时蒙特卡罗控制<font color="blue">仍然只能得到基于该策略的近似行为价值函数</font>，这 是因为<font color="blue">该策略一直在进行探索，没有一个终止条件</font>。因此我们必须关注以下两个方面:</p>
<ul>
<li>一方面我 们不想丢掉任何更好信息和状态</li>
<li>另一方面随着我们策略的改善我们最终希望能终止于某一个最优策略</li>
</ul>
<p>为此引入了另一个理论概念:GLIE(greedy in the Limit with Infinite Exploration)</p>
</li>
<li><p>含义</p>
<ul>
<li>一是所有的状态行为对会被无限次探索<img src="/2019/01/31/rl/不基于模型的控制/resources/CDCA6232DAA3B7FD2ADDF260F7AC72D6.jpg"></li>
<li>二是另外随着采样趋向无穷多，策略收敛至一个贪婪策略:<img src="/2019/01/31/rl/不基于模型的控制/resources/F7DF439FCAAF7A9CBF47C4EA1F2EDBEA.jpg"></li>
<li>定理：GLIE 蒙特卡洛控制能收敛至最优的状态行为价值函数<img src="/2019/01/31/rl/不基于模型的控制/resources/631E80B1822E328FE59C42B522B01F15.jpg">
</li>
</ul>
</li>
</ul>
<h4 id="GLIE应用"><a href="#GLIE应用" class="headerlink" title="GLIE应用"></a>GLIE应用</h4><p>如果在使用 ε-贪婪策略时，能令 ε 随采样次数的无限增加而趋向于 0 就符合 GLIE。这样基 于 GLIE 的蒙特卡洛控制流程如下:</p>
<ul>
<li>基于给定策略 π，采样第 k 个完整的状态序列:${S_1, A_1, R_2, · · · , S_T}$</li>
<li>对于该状态序列里出现的每一状态行为对$(S_t, A_t)$，更新其计数N 和行为价值函数Q:<img src="/2019/01/31/rl/不基于模型的控制/resources/B0C00EB2E651AE3104EE103045C1328A.jpg"></li>
<li>基于新的行为价值函数 Q 以如下方式改善策略<ul>
<li>在实际应用中，ε 的取值可不局限于取 1/k，只要符合 GLIE 特性的设计均可以收敛至最优策略(价值)<img src="/2019/01/31/rl/不基于模型的控制/resources/FBB92C833C02BF5CB4A474B22F491A56.jpg">
</li>
</ul>
</li>
</ul>
<h3 id="现时策略时序差分控制"><a href="#现时策略时序差分控制" class="headerlink" title="现时策略时序差分控制"></a>现时策略时序差分控制</h3><h4 id="前言介绍"><a href="#前言介绍" class="headerlink" title="前言介绍"></a>前言介绍</h4><ul>
<li>预测问题上TD比MC有点多<br>我们体会到时序差分 (TD) 学习相比蒙特卡罗 (MC) 学习有很 多优点:低变异性，可以在线实时学习，可以学习不完整状态序列等</li>
<li>控制问题上任很多优点,会介绍一些算法<br>在控制问题上使用TD学习同样具备上述的一些优点。<ul>
<li>本节的现时策略TD学习中，我们将介绍Sarsa算法和Sarsa(λ)算法</li>
<li>在下一节的借鉴策略TD学习中将详细介绍Q学习算法</li>
</ul>
</li>
</ul>
<h4 id="Sarsa-算法"><a href="#Sarsa-算法" class="headerlink" title="Sarsa 算法"></a>Sarsa 算法</h4><ul>
<li>算法理解<br>Sarsa 的名称来源于下图所示的序列描述:针对一个状态 S，个体通过行为策略产生一个行 为 A，执行该行为进而产生一个状态行为对 (S,A)，环境收到个体的行为后会告诉个体即时奖励 R 以及后续进入的状态 S’;个体在状态 S’ 时遵循当前的行为策略产生一个新行为 A’，个体此时 并不执行该行为，而是通过行为价值函数得到后一个状态行为对 (S’,A’) 的价值，利用这个新的 价值和即时奖励 R 来更新前一个状态行为对 (S,A) 的价值<img src="/2019/01/31/rl/不基于模型的控制/resources/92D7C8C43D16469D32C9E6BCF3E4272C.jpg">
与 MC 算法不同的是，Sarsa 算法在单个状态序列内的每一个时间步，在状态 S 下采取一个 行为 A 到达状态 S’ 后都要更新状态行为对 (S,A) 的价值 Q(S,A)。这一过程同样使用 ε-贪婪策略进行策略迭代:<img src="/2019/01/31/rl/不基于模型的控制/resources/27B5FDBA52F91B855D8FEAED88528FB6.jpg"></li>
<li>算法伪代码<img src="/2019/01/31/rl/不基于模型的控制/resources/E0FD9C768B78EF331240CEA6936899BF.jpg"></li>
<li>Sarsa 算法将收敛至最优策略和最优价值函数条件<ul>
<li>在更新行为价值时，参数 α 是学习速率参数，γ 是衰减因子。当行为策略满足前文所述的 GLIE 特性同时学习速率参数 α 满足<img src="/2019/01/31/rl/不基于模型的控制/resources/3540C340F9D7FC610615C1A687534486.jpg"></li>
</ul>
</li>
<li>举例：有风格子<ul>
<li>为了使用计算机程序解决这个问题，我们首先将这个问题用强化学习的语言再描述一遍。<font color="blue">这是一个不基于模型的控制问题，也就是要在不掌握马尔科夫决策过程的情况下寻找最优策略</font>。环 境世界中每一个格子可以用水平和垂直坐标来描述，如此构成拥有 70 个状态的状态空间 S。行 为空间 A 具有四个基本行为。环境的的动力学特征不被个体掌握，但个体每执行一个行为，会进入一个新的状态，该状态由环境告知个体，但环境不会直接告诉个体该状态的坐标位置。即时 奖励是根据任务目标来设定，现要求尽快从起始位置移动到目标位置，我们可以设定每移动一步 只要不是进入目标位置都给予一个 -1 的惩罚，直至进入目标位置后获得奖励 0 同时永久停留在 该位置<img src="/2019/01/31/rl/不基于模型的控制/resources/DEB132EA9A43EB956A6ED9E67184EAD1.jpg"></li>
<li>这里先给出最 优策略为依次采取如下的行为序列:<br>右、右、右、右、右、右、右、右、右、下、下、下、下、左、左<br>个体找到该最优策略的进度以及最优策略下个体从起始状态到目标状态的行为轨迹如图 5.3 所示。</li>
<li>可以看出个体在一开始的几百甚至上千步都在尝试各种操作而没有完成一次从起始位置到 目标位置的经历。不过一旦个体找到一次目标位置后，它的学习过程将明显加速，最终找到了一 条只需要 15 步的最短路径。由于世界的构造以及其内部风的影响，个体两次利用风的影响，先向右并北漂到达最右上角后折返南下再左移才找到这条最短路径。其它路径均比该路径所花费<br>的步数要多<img src="/2019/01/31/rl/不基于模型的控制/resources/81F300BD29C72D51D88E5F76FA42789E.jpg">
</li>
</ul>
</li>
</ul>
<h3 id="Sarsa-λ-算法"><a href="#Sarsa-λ-算法" class="headerlink" title="Sarsa(λ)算法"></a>Sarsa(λ)算法</h3><p>可以结合TD(λ)对比理解,这里不做详细介绍</p>
<ul>
<li>在前一章，我们学习了 n-步收获，这里类似的引出一个 n-步 Sarsa 的概念。观察下面一些列的式子<img src="/2019/01/31/rl/不基于模型的控制/resources/67B023AF90EB0427629536CED8B1E447.jpg">
</li>
</ul>
<h3 id="借鉴策略Q学习算法"><a href="#借鉴策略Q学习算法" class="headerlink" title="借鉴策略Q学习算法"></a>借鉴策略Q学习算法</h3><h4 id="背景-2"><a href="#背景-2" class="headerlink" title="背景"></a>背景</h4><p>现时策略学习的特点就是产生实际行为的策略与更新价值 (评价) 所使用的策略是同一个策 略，而借鉴策略学习 (off-policy learning) 中产生指导自身行为的策略 μ(a|s) 与评价策略 π(a|s) 是不同的策略</p>
<ul>
<li>具体地说，个体通过策略 μ(a|s)生成行为与环境发生实际交互，但是在<font color="blue">更新这个状态行为对的价值</font>时使用的是<font color="purple">目标策略</font> π(a|s)</li>
<li>目标策略 π(a|s) 多数是已经具备一定能力的 策略，例如人类已有的经验或其他个体学习到的经验</li>
</ul>
<p>借鉴策略学习相当于站在目标策略 π(a|s)的“肩膀”上学习。</p>
<ul>
<li>借鉴策略学习根据是否经历完整的状态序列可以将其分为基于蒙特卡洛的和 基于 TD 的。<ul>
<li>基于蒙特卡洛的借鉴策略学习目前认为仅有理论上的研究价值，在实际中用处不 大。</li>
<li>这里主要讲解常用借鉴策略 TD 学习</li>
</ul>
</li>
</ul>
<h4 id="借鉴学习TD学习任务"><a href="#借鉴学习TD学习任务" class="headerlink" title="借鉴学习TD学习任务"></a>借鉴学习TD学习任务</h4><ul>
<li>借鉴学习 TD 学习任务就是使用 TD 方法在目标策略 π(a|s) 的基础上更新行为价值，进而优化行为策略<img src="/2019/01/31/rl/不基于模型的控制/resources/8D7E6B60CC00475FD8FF449B15ADEFA3.jpg"></li>
<li>我们可以这样理解:个体处在状态 $S_t$ 中，基于行为策略 μ 产生了一个行为 $A_t$， 执行该行为后进入新的状态 $S_{t+1}$，借鉴策略学习要做的事情就是，<font color="blue">比较借鉴策略和行为策略在状 态 $S_t$ 下产生同样的行为 $A_t$ 的概率的比值</font><font color="purple">(其实就是看此次对该状态价值的更新支持程度)</font>:<ul>
<li>如果这个比值接近 1，说明两个策略在状态 $S_t$ 下采 取的行为 $A_t$ 的概率差不多，此次<font color="blue">对于状态 $S_t$ 价值的更新同时得到两个策略的支持</font></li>
<li>如果这一概率比值很小，则表明借鉴策略 π 在状态 $S_t$ 下选择 $A_t$ 的机会要小一些，此时为了从借鉴策略 学习，我们认为<font color="blue">这一步状态价值的更新不是很符合借鉴策略</font>，因而在更新时打些折扣。</li>
<li>如果这个概率比值大于 1，说明按照借鉴策略，选择行为 $A_t$ 的几率要大于当前行为策略产生 $A_t$的概率，此时应该对<font color="blue">该状态的价值更新就可以大胆些</font></li>
</ul>
</li>
</ul>
<h4 id="Q学习"><a href="#Q学习" class="headerlink" title="Q学习"></a>Q学习</h4><ul>
<li><p>借鉴学习TD学习任务的典型分支</p>
<ul>
<li>行为策略 μ 是基于行为价值函数 Q(s, a) ε-贪婪策略</li>
<li>借鉴 策略 π 则是基于 Q(s, a) 的完全贪婪策略</li>
</ul>
<p>这种学习方法称为 Q 学习 (Q learning)</p>
</li>
<li>Q学习的目标<br>Q 学习的目标是得到最优价值 Q(s,a)</li>
<li>策略μ<br>t 时刻的与环境进行实际交互 的行为$A_t$由策略μ产生:$A_t ∼ μ(·|S_t)$,其中策略 μ 是一个 ε-贪婪策略</li>
<li>策略π<br>t + 1 时刻用来更新 Q 值的行为 $A’<em>{t+1}$ 由下式产生:$ A’</em>{t+1} ∼ π(·|S_{t+1}) $。其中策略 π 是一个完全贪婪策略</li>
<li><p>$Q(S_t, A_t)$ 的按下式更新</p>
<img src="/2019/01/31/rl/不基于模型的控制/resources/98592458FA28992AD024350ED870D682.jpg">
<ul>
<li>其中红色部分的 TD 目标是基于借鉴策略 π 产生的行为 A’ 得到的 Q 值。</li>
<li>根据这种价值更 新的方式<ul>
<li>状态 $S_t$ 依据 ε-贪婪策略得到的行为 $A_t$ 的价值将<font color="blue">朝着</font> $S_{t+1}$ 状态下贪婪策略确定的最 大行为价值的方向做一定比例的更新</li>
<li>这种算法能够使个体的<font color="blue">行为策略策略 μ 更加接近贪婪策 略</font>，同时保证保证个体能持续探索并经历足够丰富的新状态。</li>
</ul>
</li>
</ul>
<p>并最终收敛至最优策略和最优行为价值</p>
<img src="/2019/01/31/rl/不基于模型的控制/resources/91C5C802AF801778C5F7B98E88592A2D.jpg">
<ul>
<li>下图是 Q 学习具体的行为价值更新公式<ul>
<li>注意和上式比较max部分就是基于贪婪策略得到<img src="/2019/01/31/rl/不基于模型的控制/resources/92CE30E338BB6A9D84D5A1FDBFD5CD54.jpg"></li>
</ul>
</li>
</ul>
</li>
<li>伪代码<img src="/2019/01/31/rl/不基于模型的控制/resources/B77A321F5FB67CC55F5C32C100004662.jpg">
</li>
</ul>
<h4 id="Q学习和Sarsa比较"><a href="#Q学习和Sarsa比较" class="headerlink" title="Q学习和Sarsa比较"></a>Q学习和Sarsa比较</h4><p>这里通过悬崖行走的例子 (图) 简要讲解 Sarsa 算法与 Q 学习算法在学习过程中的差别。任 务要求个体从悬崖的一端以尽可能快的速度行走到悬崖的另一端，每多走一步给以 -1 的奖励。 图中悬崖用灰色的长方形表示，在其两端一个是起点，一个是目标终点。个体如果坠入悬崖将得 到一个非常大的负向奖励 (-100) 回到起点。可以看出最优路线是贴着悬崖上方行走。Q 学习算 法可以较快的学习到这个最优策略，但是 Sarsa 算法学到的是与悬崖保持一定的距离安全路线。 在两种学习算法中，由于生成行为的策略依然是 ε 贪婪的，因此都会偶尔发生坠入悬崖的情况， 如果 ε 贪婪策略中的 ε 随经历的增加而逐渐趋于 0，则两种算法都将最后收敛至最优策略<br><img src="/2019/01/31/rl/不基于模型的控制/resources/EB3FAFA56BF37694FF54781C1A854A5A.jpg"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/01/28/rl/不基于模型的预测/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/28/rl/不基于模型的预测/" itemprop="url">不基于模型的预测</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-28T07:00:14+08:00">
                2019-01-28
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/强化学习/" itemprop="url" rel="index">
                    <span itemprop="name">强化学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul>
<li>讲解如何解决一个可以被认为是 MDP、但却不掌握 MDP 具体细节(比如状态转移概率等等)的问题</li>
<li>从本章开始的连续两章内容将讲解如何解决一个可以被认为是 MDP、但却不掌握 MDP 具体细节的问题，<font color="blue">也就是讲述个体如何在没有对环境动力学认识的模型的条件下如何直接 通过个体与环境的实际交互来评估一个策略的好坏或者寻找到最优价值函数和最优策略</font>。其中 本章将聚焦于策略评估，也就是预测问题</li>
</ul>
<h3 id="蒙特卡罗强化学习-Monte-Carlo-reinforcement-learning"><a href="#蒙特卡罗强化学习-Monte-Carlo-reinforcement-learning" class="headerlink" title="蒙特卡罗强化学习 (Monte-Carlo reinforcement learning)"></a>蒙特卡罗强化学习 (Monte-Carlo reinforcement learning)</h3><ul>
<li>MC：<br>指在<font color="blue">不清楚 MDP 状态 转移概率</font>的情况下，直接从<font color="blue">经历完整的状态序列</font> (episode) 来估计状态的真实价值，并认为某状 态的价值等于在多个状态序列中以该状态算得到的<font color="blue">所有收获的平均</font></li>
<li>MC特点：<br>蒙特卡罗强化学习有如下特点:<font color="blue">不依赖状态转移概率，直接从经历过的完整的状态序列中学习</font>，使用的思想就是用<font color="purple">平均收获值代替价值</font>。理论上完整的状态序列越多，结果越准确</li>
<li>完整的状态序列 (complete episode):<br>指从某一个状态开始，个体与环境交互直到终止状态， 环境给出终止状态的奖励为止。<ul>
<li>完整的状态序列不要求起始状态一定是某一个特定的状态，但是 要求个体最终进入环境认可的某一个终止状态</li>
</ul>
</li>
<li>MC含义举例说明:<ul>
<li>基于特定策略 π 的一个 Episode 信息可以表示为如下的一个序列:$S_1,A_1,R_2,S_2,A_2,…,S_t,A_t,R_{t+1},…,S_k ∼π$</li>
<li>t 时刻状态 St 的收获可以表述为:<br>$G_t =R_{t+1} +γR_{t+2} +…+γ(T−1)R_T$</li>
<li>其中 T 为终止时刻。该策略下某一状态 s 的价值:<br>$v_π(s) = E_π[G_t|S_t = s]$</li>
</ul>
</li>
<li>一个序列中出现多次状态的问题<ul>
<li>如果一个完整的状态序列中某一需要计算的状态出现在序列的多个位置， 也就是说个体在与环境交互的过程中从某状态出发后又一次或多次返回过该状态,处理的两种方法：<ul>
<li>首次访问:仅把状态序列中第一次出现该状 态时的收获值纳入到收获平均值的计算中</li>
<li>每次访问:针对一个状态序列中每次出现的该状态，都 计算对应的收获值并纳入到收获平均值的计算中</li>
</ul>
</li>
</ul>
</li>
<li>求解技巧<ul>
<li><font color="purple">累进更新平均值</font>(incremental mean)。而且这种计算平均值的思想也是强化学习的一 个核心思想之一</li>
<li>在求解状态收获的平均值的过程中，我们介绍一种非常实用的不需要存储所有历史收获的 计算方法:累进更新平均值(incremental mean)<img src="/2019/01/28/rl/不基于模型的预测/resources/DA2A5A4C65D5CE54186CE24522BC162E.jpg"></li>
<li>如果把该式中平均值和新数据分别看成是状态的价值和该状态的收获，那么该公式就变成了递增式的蒙特卡罗法更新状态价值。其公式如下:<img src="/2019/01/28/rl/不基于模型的预测/resources/37747F59874E4561D510042CB2C1C389.jpg">
</li>
</ul>
</li>
</ul>
<h3 id="时序差分强化学习"><a href="#时序差分强化学习" class="headerlink" title="时序差分强化学习"></a>时序差分强化学习</h3><ul>
<li>定义：<br>指从采样得到的 <font color="blue">不完整</font>的状态序列学习，该方法通过合理的引导(bootstrapping)，先估计某状态在该状态序列 <font color="blue">完整后可能得到的收获</font>，并在此基础上利用前文所属的<font color="blue">累进更新平均值的方法得到该状态的价 值</font>，再通过不断的采样来持续更新这个价值<ul>
<li>具体地说，在 TD 学习中，算法在估计某一个状态的收获时，用的是离开该状态的即刻奖励 $R_{t+1}$ 与下一时刻状态 $S_{t+1}$ 的预估状态价值乘以衰减系数 γ 组成:<br>$V(S_t) ← V(S_t) + α(R_{t+1} + γV(S_{t+1}) − V(S_t))$<ul>
<li>其中:$R_{t+1} + γV(S_{t+1}) 称为 TD 目标值。R_{t+1} + γV(S_{t+1}) − V(S_t)$称为 TD 误差</li>
<li>引导 (bootstrapping):指的是用 TD 目标值代替收获 $G_t$ 的过程</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="MC-TD-DP比较"><a href="#MC-TD-DP比较" class="headerlink" title="MC,TD,DP比较"></a>MC,TD,DP比较</h3><ul>
<li>MC,TD相同点：<ul>
<li>不依赖于模型 </li>
<li>它们都不再需要清楚某一状态的所有可能的后<br>续状态以及对应的状态转移概率</li>
<li>因此也不再像动态规划算法那样进行全宽度的回溯来更新状 态的价值。</li>
<li>MC 和 TD 学习使用的都是通过个体与环境实际交互生成的一系列状态序列来更新 状态的价值。这在解决大规模问题或者不清楚环境动力学特征的问题时十分有效</li>
</ul>
</li>
<li>DP 算法则是<font color="blue">基于模型</font>的计算状态价值的方法，它通过计算一个状态 S 所 有可能的转移状态 S’ 及其转移概率以及对应的即时奖励来计算这个状态 S 的价值</li>
<li>是否使用引导数据：<ul>
<li>MC 学习并不使用引导数据，它使用实际产生的奖励值来计算状态 价值</li>
<li>TD 和 DP 则都是用后续状态的预估价值作为引导数据来计算当前状态的价值</li>
</ul>
</li>
<li>是否采样：<ul>
<li>MC 和 TD 不依赖模型，使用的都是个体与环境实际交互产生的采样 状态序列来计算状态价值的</li>
<li>DP 则依赖状态转移概率矩阵和奖励函数，全宽度计算状态价 值，没有采样之说。</li>
</ul>
</li>
<li>MC算法<br>深度采样学习。一次学习完整经历，使用实际收获更新状态预估价值<img src="/2019/01/28/rl/不基于模型的预测/resources/D5142FB8A2311F67CFC8FE478F00AE45.jpg"></li>
<li>TD 算法:<br>浅层采样学习。经历可不完整，使用后续状态的预估状态价值预估收获再更新当前状态价值<img src="/2019/01/28/rl/不基于模型的预测/resources/A06E0EBDB69A12A2D646A6341F7D5FF0.jpg"></li>
<li>DP算法：<br>浅层全宽度 (采样) 学习。依据模型，全宽度地使用后续状态预估价值来更新当前<br>状态价值<img src="/2019/01/28/rl/不基于模型的预测/resources/FE5B4742FA8F5659B79D105642A0FF91.jpg"></li>
<li>小结：<ul>
<li>当使用单个采样，同时不经历完整的状态序 列更新价值的算法是 TD 学习;</li>
<li>当使用单个采样，但依赖完整状态序列的算法是 MC 学习;</li>
<li>当考虑全宽度采样，但对每一个采样经历只考虑后续一个状态时的算法是 DP 学习;</li>
<li>如果既考虑所 有状态转移的可能性，同时又依赖完整状态序列的，那么这种算法是穷举 (exhausive search) 法。 </li>
<li>需要说明的是:DP 利用的是整个 MDP 问题的模型，也就是状态转移概率，虽然它并不实际利 用采样经历，但它利用了整个模型的规律，因此也被认为是全宽度 (full width) 采样的</li>
</ul>
</li>
</ul>
<h3 id="n步时序差分学习简介"><a href="#n步时序差分学习简介" class="headerlink" title="n步时序差分学习简介"></a>n步时序差分学习简介</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><ul>
<li>第二节所介绍的 TD 算法实际上都是 TD(0) 算法，括号内的数字 0 表示的是在当前状态下 往前多看 1 步<img src="/2019/01/28/rl/不基于模型的预测/resources/8C3B36B50EF85DAF279B60F3B4D130E2.jpg">
<img src="/2019/01/28/rl/不基于模型的预测/resources/419B67E0D3070AF4AD2D7AA38F9EB4C0.jpg">
</li>
</ul>
<h4 id="λ-收获"><a href="#λ-收获" class="headerlink" title="λ-收获"></a>λ-收获</h4><p>为了能在不增加计算复杂度的情况下<font color="blue">综合考虑所有步数的预测</font>，我们引入了一个新的参数 λ，并定义:λ-收获</p>
<ul>
<li>任意一个 n-步收获的权重被设计为 $(1 − λ)λ^{n−1}$，如图 4.7 所示。通过这样的权重设计，可以得到 λ-收获的计算公式为<img src="/2019/01/28/rl/不基于模型的预测/resources/F95E29C3E3B4152C5012864A49CD722B.jpg"></li>
<li>对应的 TD(λ) 被描述为<img src="/2019/01/28/rl/不基于模型的预测/resources/A586735B46F51B3E57D49C422B48B850.jpg">
<img src="/2019/01/28/rl/不基于模型的预测/resources/E95B81E69FD207077339DECF808D197F.jpg">
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/01/24/rl/动态规划寻找最优策略/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/24/rl/动态规划寻找最优策略/" itemprop="url">动态规划寻找最优策略</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-24T22:00:10+08:00">
                2019-01-24
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/强化学习/" itemprop="url" rel="index">
                    <span itemprop="name">强化学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><h4 id="动态规划理解"><a href="#动态规划理解" class="headerlink" title="动态规划理解"></a>动态规划理解</h4><ul>
<li><p>规划<br>“规划”是在已知环 境动力学的基础上进行评估和控制，具体来说就是在了解包括状态和行为空间、转移概率矩阵、 奖励等信息的基础上<font color="blue">判断一个给定策略的价值函数，或判断一个策略的优劣并最终找到最优的 策略和最优价值函数</font></p>
</li>
<li><p>动态规划算法把求解复杂问题<font color="blue">分解为求解子问题</font>，通过求解子问题进而得到整个问题的解,在解决子问题的时候，其结果<font color="blue">通常需要存储起来被用来解决后续复杂问题</font></p>
<ul>
<li>当问题具有下列两个 性质时，通常可以考虑使用动态规划来求解:<ul>
<li>第一个性质是一个复杂问题的最优解由数个小问题 的最优解构成，可以通过寻找子问题的最优解来得到复杂问题的最优解;</li>
<li>第二个性质是子问题在 复杂问题内重复出现，使得子问题的解可以被存储起来重复利用</li>
</ul>
</li>
</ul>
</li>
<li><p>和马尔科夫决策过程关系<br>马尔科夫决策过程具有上述两 个属性:</p>
<ul>
<li>贝尔曼方程把问题递归为求解子问题</li>
<li>价值函数相当于存储了一些子问题的解，可以复 用。因此可以使用动态规划来求解马尔科夫决策过程</li>
</ul>
</li>
</ul>
<h4 id="预测和控制"><a href="#预测和控制" class="headerlink" title="预测和控制"></a>预测和控制</h4><ul>
<li>预测 (prediction):已知一个马尔科夫决策过程 MDP ⟨S, A, P, R, γ⟩ 和一个策略 π，或者是 给定一个马尔科夫奖励过程 $MRP ⟨S, P_π, R_π, γ⟩$，<font color="blue">求解基于该策略的价值函数 $v_π$</font>。</li>
<li>控制(control):已知一个马尔科夫决策过程MDP⟨S,A,P,R,γ⟩，<font color="blue">求解最优价值函数v∗ 和 最优策略 π∗</font>。</li>
</ul>
<h3 id="策略评估"><a href="#策略评估" class="headerlink" title="策略评估"></a>策略评估</h3><h4 id="含义"><a href="#含义" class="headerlink" title="含义"></a>含义</h4><p>指计算给定策略下状态价值函数的过程<br>对策略评估，我们可 以使用同步迭代联合动态规划的算法:从任意一个状态价值函数开始，依据给定的策略，结合贝 尔曼期望方程、状态转移概率和奖励同步迭代更新状态价值函数，<font color="blue">直至其收敛，得到该策略下最 终的状态价值函数</font></p>
<ul>
<li>贝尔曼期望方程给出了如何根据状态转换关系中的后续状态 s′ 来计算当前状态 s 的价值， 在同步迭代法中，我们使用上一个迭代周期 k 内的后续状态价值来计算更新当前迭代周期 k + 1<br>内某状态 s 的价值<ul>
<li>我们可以对计算得到的新的状态价值函数再次进行迭代，直至状态函数收敛，也就是<font color="blue">迭代计算得到每一个状态的新价值与原价值差别在一个很小的可接受范围内</font><img src="/2019/01/24/rl/动态规划寻找最优策略/resources/E42402554539EC7CBCD4DB4823475972.jpg">
</li>
</ul>
</li>
</ul>
<h4 id="举例-小型方格世界迭代中的价值函数"><a href="#举例-小型方格世界迭代中的价值函数" class="headerlink" title="举例(小型方格世界迭代中的价值函数)"></a>举例(小型方格世界迭代中的价值函数)</h4><ul>
<li>均一概率的随机策略<img src="/2019/01/24/rl/动态规划寻找最优策略/resources/3AE20758C58F796F94B5309148135A30.jpg">
</li>
</ul>
<h3 id="策略迭代"><a href="#策略迭代" class="headerlink" title="策略迭代"></a>策略迭代</h3><h4 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a>背景</h4><p>完成对一个策略的评估，将得到基于该策略下每一个状态的价值。很明显，不同状态对应的 价值一般也不同，那么个体是否可以根据得到的价值状态来调整自己的行动策略呢</p>
<h4 id="迭代过程理解-贪婪策略"><a href="#迭代过程理解-贪婪策略" class="headerlink" title="迭代过程理解(贪婪策略)"></a>迭代过程理解(贪婪策略)</h4><p>个体在某个状态下选择的行为是其能够到达后续所有可能的状态中价值最 大的那个状态。我们以均一随机策略下第 2 次迭代后产生的价值函数为例说明这个贪婪策略</p>
<ul>
<li>新的贪婪策略比之前的均一随机策略要优秀不少，至少在靠近终止<br>状态的几个状态中，个体将有一个明确的行为，而不再是随机行为了。我们从均一随机策略下的 价值函数中产生了新的更优秀的策略，这是一个策略改善的过程<img src="/2019/01/24/rl/动态规划寻找最优策略/resources/8B3A8C37EC6AB3D3C839C7C4FDC684A7.jpg">
</li>
</ul>
<ul>
<li><p>我的理解：<br>上面左图是各个状态的价值，右图是各个状态的策略，选择的贪婪策略由于各个状态不同的价值，导致了右图不同的策略。然后又由于不同的策略，执行下一步行动，继续会导致左图中不同的状态，以此循环</p>
<ul>
<li>依据新的策略 π′ 会得到一个新的价值函数，并产生新的贪婪策略，如此重复循环迭代将最 终得到最优价值函数 v∗ 和最优策略 π∗</li>
</ul>
</li>
<li><p>策略迭代</p>
<ul>
<li>策略在循环迭代中得到更新改善的过程称为策略迭代</li>
<li>从一个初始策略 π 和初始价值函数 V 开始，基于该策略进行完整的价值评估过程得到一个 新的价值函数，随后依据新的价值函数得到新的贪婪策略，随后计算新的贪婪策略下的价值函 数，整个过程反复进行，在这个循环过程中策略和价值函数均得到迭代更新，<font color="blue">并最终收敛值最有 价值函数和最优策略</font><img src="/2019/01/24/rl/动态规划寻找最优策略/resources/94EC8313FD2EAD0383F48AE24FF35931.jpg">
</li>
</ul>
</li>
</ul>
<h3 id="价值迭代"><a href="#价值迭代" class="headerlink" title="价值迭代"></a>价值迭代</h3><h4 id="背景-2"><a href="#背景-2" class="headerlink" title="背景"></a>背景</h4><p>细心的读者可能会发现，如果按照图 3.2 中第三次迭代得到的价值函数采用贪婪选择策略的 话，该策略和最终的最优价值函数对应的贪婪选择策略是一样的，它们都对应于最优策略，如图 3.5，而通过基于均一随机策略的迭代法价值评估要经过数十次迭代才算收敛。这会引出一个问 题:<font color="blue">是否可以提前设置一个迭代终点来减少迭代次数而不影响得到最优策略呢?是否可以每迭代 一次就进行一次策略评估呢</font></p>
<h4 id="最优策略的意义"><a href="#最优策略的意义" class="headerlink" title="最优策略的意义"></a>最优策略的意义</h4><ul>
<li><p>任何一个最优策略可以分为两个阶段:<br>首先该策略要能产生当前状态下的最优行为，其次对 于该最优行为到达的后续状态时该策略仍然是一个最优策略</p>
</li>
<li><p>直观感受(单纯的价值迭代)</p>
<img src="/2019/01/24/rl/动态规划寻找最优策略/resources/954EA9A8B6A3282E4D1EB3705711052F.jpg">
<p>这个公式带给我们的直觉是如果我们能知道最终状态的价值和相关奖励，<font color="blue">可以直接计算得 到最终状态的前一个所有可能状态的最优价值</font>。更乐观的是，即使不知道最终状态是哪一个状 态，<font color="blue">也可以利用上述公式进行纯粹的价值迭代</font>，不停的更新状态价值，最终得到最优价值，而且 这种单纯价值迭代的方法甚至可以允许存在循环的状态转换乃至一些随机的状态转换过程</p>
</li>
</ul>
<h4 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h4><img src="/2019/01/24/rl/动态规划寻找最优策略/resources/6D8A6147FB99EDC0C77B084719D1D679.jpg">
<ul>
<li>首先考虑到个体知道环境的动力学特征的情形。在这种情况下，个体可以直接计算得到与终 止状态直接相邻(斜向不算)的左上角两个状态的最优价值均为 −1。随后个体又可以往右下角 延伸计算得到与之前最优价值为 −1 的两个状态香相邻的 3 个状态的最优价值为 −2。以此类推， 每一次迭代个体将从左上角朝着右下角方向依次直接计算得到一排斜向方格的最优价值，直至 完成最右下角的一个方格最优价值的计算</li>
<li><font color="blue">个人理解</font>：<br>由于上面的直观感受可以知道，前一状态的最优价值可以由后一状态计算得到，所以从最终的终态出发，开始价值为0，然后倒推，得到每一个状态的最优价值<font color="blue">（纯粹的价值迭代，并没有策略的参与）</font></li>
<li>特点(注意并没有策略的参与)<br>价值迭代的目标仍然是寻找到一个最优策略，它通过贝尔曼最优方程从前次迭代 的价值函数中计算得到当次迭代的价值函数，在这个反复迭代的过程中，并没有一个明确的策略 参与<ul>
<li>需要 注意的是，在纯粹的价值迭代寻找最优策略的过程中，迭代过程中产生的状态价值函数不一定对 应一个策略。迭代过程中价值函数更新的公式为:<img src="/2019/01/24/rl/动态规划寻找最优策略/resources/F20EAADAC12F3F252A5FFFCB6B405928.jpg">
</li>
</ul>
</li>
</ul>
<h3 id="同步动态规划算法总结"><a href="#同步动态规划算法总结" class="headerlink" title="同步动态规划算法总结"></a>同步动态规划算法总结</h3><p>使用同步动态规划进行规划基本就讲解完毕了。前文所述的这三类算法<font color="blue">均是基于状态价值函数的</font></p>
<ul>
<li>其中迭代法策略评估属于预测问题，它使用贝尔曼期望方程来进行求解。</li>
<li>策略迭代和价值迭代则属于控制问题<ul>
<li>其中前者使用贝尔曼 期望方程进行一定次数的价值迭代更新，随后在产生的价值函数基础上采取贪婪选择的策略改 善方法形成新的策略，如此交替迭代不断的优化策略;</li>
<li>价值迭代则不依赖任何策略，它使用贝尔 曼最优方程直接对价值函数进行迭代更新</li>
</ul>
</li>
</ul>
<h3 id="异步动态规划算法"><a href="#异步动态规划算法" class="headerlink" title="异步动态规划算法"></a>异步动态规划算法</h3><p>前文所述的系列算法均为同步动态规划算法，<font color="blue">它表示所有的状态更新是同步的</font>。与之对应的 还有异步动态规划算法。在这些算法中，<font color="blue">每一次迭代并不对所有状态的价值进行更新，而是依据 一定的原则有选择性的更新部分状态的价值</font></p>
<ul>
<li>这种算法能显著的节约计算资源，并且只要所有状 态能够得到持续的被访问更新，那么也能确保算法收敛至最优解。</li>
<li>比较常用的异步动态规划思想 有:原位动态规划、优先级动态规划、和实时动态规划</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/01/21/rl/马尔科夫决策过程/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/21/rl/马尔科夫决策过程/" itemprop="url">马尔科夫决策过程</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-21T06:00:22+08:00">
                2019-01-21
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/强化学习/" itemprop="url" rel="index">
                    <span itemprop="name">强化学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ul>
<li>求解强化学习问题可以理解为如何最大化个体在与环境交互过程中获得的累积奖励</li>
<li>当环境状态是完全可观测时，个体可以通过构建马尔科夫决策过程来描述整 个强化学习问题。有时候环境状态并不是完全可观测的，此时个体可以结合自身对于环境的历史 观测数据来构建一个近似的完全可观测环境的描述<ul>
<li>从这个角度来说，<font color="blue">几乎所有的强化学习问题 都可以被认为或可以被转化为马尔科夫决策过程</font></li>
</ul>
</li>
</ul>
<h3 id="马尔科夫过程"><a href="#马尔科夫过程" class="headerlink" title="马尔科夫过程"></a>马尔科夫过程</h3><h4 id="马尔科夫性"><a href="#马尔科夫性" class="headerlink" title="马尔科夫性"></a>马尔科夫性</h4><p>  在一个时序过程中，如果 t + 1 时刻的状态仅取决于 t 时刻的状态 $S_t$ 而与 t 时刻之前的任 何状态都无关时，则认为 t 时刻的状态$S_t$ 具有马尔科夫性 (Markov property)</p>
<h4 id="马尔科夫过程-马尔科夫链"><a href="#马尔科夫过程-马尔科夫链" class="headerlink" title="马尔科夫过程(马尔科夫链)"></a>马尔科夫过程(马尔科夫链)</h4><p>  若过程中的每一 个状态都具有马尔科夫性，则这个过程具备马尔科夫性。具备了马尔科夫性的随机过程称为马尔 科夫过程 (Markov process)，又称马尔科夫链 (Markov chain)</p>
<ul>
<li>描述一个马尔科夫过程的核心是状态转移概率矩阵<br>$P_{ss′} = P [S_{t+1} = s′|S_t = s]$</li>
<li>通常使用一个<font color="blue">元组 ⟨S, P ⟩</font> 来描述马尔科夫过程，其中 S 是有限数量的状态集，P 是状态转 移概率矩阵</li>
</ul>
<h4 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h4><p>  从符合马尔科夫过程给定的状态转移概率矩阵生成一个状态序列的过程称为采样(sample)。</p>
<h4 id="状态序列"><a href="#状态序列" class="headerlink" title="状态序列"></a>状态序列</h4><p>  采样将得到一系列的状态转换过程，本书我们称为状态序列 (episode)</p>
<h4 id="完整状态序列"><a href="#完整状态序列" class="headerlink" title="完整状态序列"></a>完整状态序列</h4><p>  当状态序列的最后一个,状态是终止状态时，该状态序列称为完整的状态序列 (complete episode)</p>
<h3 id="马尔科夫奖励过程"><a href="#马尔科夫奖励过程" class="headerlink" title="马尔科夫奖励过程"></a>马尔科夫奖励过程</h3><h4 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a>背景</h4><p>马尔科夫过程只涉及到状态之间的转移概率，并未触及强化学习问题中伴随着状态转换的 奖励反馈。如果把奖励考虑进马尔科夫过程，则成为马尔科夫奖励过程(Markov reward process, MRP)。它是由 ⟨S, P, R, γ⟩ 构成的一个元组，其中:</p>
<ul>
<li>S 是一个有限状态集</li>
<li>P 是集合中状态转移概率矩阵:$P_{ss′} = P [S_{t+1} = s′|S_t = s]]$</li>
<li>R 是一个奖励函数:$R_s = E [R_{t+1}|S_t = s]$</li>
<li>γ 是一个衰减因子:γ ∈ [0, 1]</li>
</ul>
<h4 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h4><ul>
<li>收获<br>收获(return)是一个马尔科夫奖励过程中从某一个状态 $S_t$ 开始采样<font color="blue">直到终止状态</font>时所有 奖励的有衰减的之和。数学表达式如下<img src="/2019/01/21/rl/马尔科夫决策过程/resources/8A5019829AFB9B5FDCE569CD54825878.jpg"></li>
<li>价值<br>价值(value) 是马尔科夫奖励过程中<font color="blue">状态收获的期望</font>。ta 数学表达式为<br>$v(s) = E [G_t|S_t = s]$</li>
<li><p>价值函数<br>如果存在一个函数，给定一个状态能得到该状态对应的价值，那么该函数就被称为价值函数 (value function)。<font color="blue">价值函数建立了从状态到价值的映射</font></p>
<img src="/2019/01/21/rl/马尔科夫决策过程/resources/63FEBA67EFEAAB1843BCFEFEF3964719.jpg">
</li>
<li><p>马尔科夫奖励过程中的贝尔曼方程</p>
<ul>
<li>它(上图)提示一个状态的价值 由该状态的奖励以及后续状态价值按概率分布求和按一定的衰减比例联合组成</li>
<li>根据马尔科夫奖励过程的定义，$R_{t+1}$ 的期望就是其自身，因为每次离开同一个状 态得到的奖励都是一个固定的值。而下一时刻状态价值的期望，可以根据下一时刻状态的概率分 布得到。如果用 s′ 表示 s 状态下一时刻任一可能的状态:<img src="/2019/01/21/rl/马尔科夫决策过程/resources/B4B06F2BCF4DC9FDD1D3600C3D31715B.jpg">
</li>
</ul>
</li>
</ul>
<h3 id="马尔科夫决策过程-MDP"><a href="#马尔科夫决策过程-MDP" class="headerlink" title="马尔科夫决策过程(MDP)"></a>马尔科夫决策过程(MDP)</h3><h4 id="背景-2"><a href="#背景-2" class="headerlink" title="背景"></a>背景</h4><p>马尔科夫奖励过程并不能直接用来指导解决强化学习问题，因为它不涉及到个体行为的选 择，因此有必要引入马尔科夫决策过程。马尔科夫决策过程(Markov decision process, MDP)是 由 ⟨S, A, P, R, γ⟩ 构成的一个元组，其中:</p>
<ul>
<li>S 是一个有限状态集</li>
<li>A 是一个有限行为集</li>
<li>P 是集合中基于行为的状态转移概率矩阵:$P^a_{ss’} = E [R_{t+1} |S_t = s, A_t = a]$</li>
<li>R 是基于状态和行为的奖励函数:$R_s^a = E [R_{t+1}|S_t = s, A_t = a]$</li>
<li>γ 是一个衰减因子:γ ∈ [0, 1]</li>
</ul>
<h4 id="相关概念-1"><a href="#相关概念-1" class="headerlink" title="相关概念"></a>相关概念</h4><ul>
<li><p>策略<br>个体在给定状 态下从行为集中选择一个行为的依据则称为策略 (policy)，用字母 π 表示。策略 π 是某一状态下基于行为集合的概率分布:<br>$π(a|s)=P[A_t =a|S_t =s]$</p>
<ul>
<li>策略仅通过依靠当前状态就可以产生一个个体的行为，可以说策略 仅与当前状态相关，而与历史状态无关</li>
<li>策略描述的是个体的行 为产生的机制，是不随状态变化而变化的，被认为是静态的</li>
</ul>
</li>
<li><p>随机策略<br>随机策略是一个很常用的策略，当个体使用随机策略时，个体在某一状态下选择的行为并不 确定。借助随机策略，个体可以在同一状态下尝试不同的行为</p>
</li>
<li><p>当给定一个马尔科夫决策过程:M = ⟨S, A, P, R, γ⟩ 和一个策略 π，那么状态序列 $S_1, S_2$, . . . 是一个符合马尔科夫过程 $⟨S, P_π⟩$ 的采样</p>
</li>
<li><p>价值函数<br>价值函数 $v_π(s)$ 是在马尔科夫决策过程下基于策略 π 的状态价值函数，表示从状态 s 开始，遵循当前策略 π 时所获得的收获的期望:$v_π(s) = E [G_t|S_t = s]$</p>
</li>
<li><p>行为价值函数(状态行为对价值函数)<br>一个基于策略 π 的行为价值函数 $q_π(s,a)$，表示在遵循策略 π 时，对当前状态 s 执行某一具体行为 a 所能的到 的收获的期望:$q_π(s,a) = E[G_t|S_t = s,A_t = a]$</p>
</li>
<li>贝尔曼方程<br>同理，可推导出如下两个方程<br>$v_π(s) = E [R_{t+1} + γv_π(S_{t+1})|S_t = s]$<br>$q_π(s, a) = E [R_{t+1} + γq_π(S_{t+1}, A_{t+1})|S_t = s, A_t = a]$</li>
</ul>
<h4 id="状态价值和行为价值转换"><a href="#状态价值和行为价值转换" class="headerlink" title="状态价值和行为价值转换"></a>状态价值和行为价值转换</h4><ul>
<li><p>一个状态的价值可以用该状态下所有行为价值来表达:<br>$v_\pi(s) = \sum \pi(a|s)q_\pi(s,a) (a\in A)$</p>
<img src="/2019/01/21/rl/马尔科夫决策过程/resources/B3E388E69955204B0CBAD8F774E0DCAE.jpg">
</li>
<li><p>一个行为的价值可以用该行为所能到达的后续状态的价值来表达:</p>
<img src="/2019/01/21/rl/马尔科夫决策过程/resources/039915DAC31661A5D043C50B0D7C59E3.jpg">
</li>
<li><p>把上二式组合起来，可以得到下面的结果:</p>
<img src="/2019/01/21/rl/马尔科夫决策过程/resources/E994997187C5D6AD87F3879B0F1BE30E.jpg">
<img src="/2019/01/21/rl/马尔科夫决策过程/resources/4C72981D57DE31EF6A1FF25A7C8F6050.jpg">
</li>
</ul>
<h4 id="最优价值函数"><a href="#最优价值函数" class="headerlink" title="最优价值函数"></a>最优价值函数</h4><ul>
<li>背景：<br>是否存在一个基于某一策略的价值函数， 在该策略下每一个状态的价值都比其它策略下该状态的价值高?如果存在如何找到这样的价值 函数?这样的价值函数对应的策略又是什么策略?</li>
<li>最优状态价值函数<br>是所有策略下产生的众多状态价值函数 中的最大者:$v_∗ = max (v_π(s))$</li>
<li>最优行为价值函数(optimal action-value function)<br>是所有策略下产生的众多行为价 值函数中的最大者:$q_∗(s,a) = max q_π(s,a)$</li>
<li>贝尔曼优化方程<ul>
<li>最优状态行为价值<img src="/2019/01/21/rl/马尔科夫决策过程/resources/AC4326C7287308660F7E7376696F4EF3.jpg">
<ul>
<li>也可以由后续状态行为价值函数得到<img src="/2019/01/21/rl/马尔科夫决策过程/resources/BE707491DE5CCFB8610C8D0958FF7B9E.jpg"></li>
</ul>
</li>
<li>最优状态价值：<img src="/2019/01/21/rl/马尔科夫决策过程/resources/E5EF335F8158C109A4A040F3A6F25DED.jpg">
</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/01/18/gan/GAN小结/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/18/gan/GAN小结/" itemprop="url">GAN小结</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-18T23:11:18+08:00">
                2019-01-18
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/GAN/" itemprop="url" rel="index">
                    <span itemprop="name">GAN</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://blog.csdn.net/qq_25737169/article/details/78857724" target="_blank" rel="noopener">参考文章</a></p>
<h3 id="什么是GAN"><a href="#什么是GAN" class="headerlink" title="什么是GAN"></a>什么是GAN</h3><p>GAN（Generative adversarial nets）,中文是生成对抗网络，他是一种生成式模型，也是一种无监督学习模型。其最大的特点是为深度网络提供了一种对抗训练的方式，此方式有助于解决一些普通训练方式不容易解决的问题</p>
<h3 id="GAN原理"><a href="#GAN原理" class="headerlink" title="GAN原理"></a>GAN原理</h3><p>GAN的主要灵感来源于博弈论中零和博弈的思想，应用到深度学习神经网络上来说，就是通过生成网络G（Generator）和判别网络D（Discriminator）不断博弈，进而使G学习到数据的分布</p>
<ul>
<li>如果用到图片生成上，则训练完成后，G可以从一段随机数中生成逼真的图像。G， D的主要功能是：<ul>
<li>G是一个生成式的网络，它接收一个随机的噪声z（随机数），通过这个噪声生成图像</li>
<li>D是一个判别网络，判别一张图片是不是“真实的”。它的输入参数是x，x代表一张图片，输出D（x）代表x为真实图片的概率，如果为1，就代表100%是真实的图片，而输出为0，就代表不可能是真实的图片</li>
</ul>
</li>
<li>训练过程中，生成网络G的目标就是尽量生成真实的图片去欺骗判别网络D。而D的目标就是尽量辨别出G生成的假图像和真实的图像。这样，G和D构成了一个动态的“博弈过程”，最终的平衡点即纳什均衡点.</li>
</ul>

<h3 id="GAN特点"><a href="#GAN特点" class="headerlink" title="GAN特点"></a>GAN特点</h3><ul>
<li>相比较传统的模型，他存在两个不同的网络，而不是单一的网络，并且训练方式采用的是对抗训练方式</li>
<li>GAN中G的梯度更新信息来自判别器D，而不是来自数据样本</li>
</ul>
<h3 id="GAN优点"><a href="#GAN优点" class="headerlink" title="GAN优点"></a>GAN优点</h3><ul>
<li>GAN是一种生成式模型，相比较其他生成模型（玻尔兹曼机和GSNs）只用到了反向传播,而不需要复杂的马尔科夫链</li>
<li>相比其他所有模型, GAN可以产生更加清晰，真实的样本</li>
<li>GAN采用的是一种无监督的学习方式训练，可以被广泛用在无监督学习和半监督学习领域</li>
<li>相比于变分自编码器, GANs没有引入任何决定性偏置( deterministic bias),变分方法引入决定性偏置,因为他们优化对数似然的下界,而不是似然度本身,这看起来导致了VAEs生成的实例比GANs更模糊</li>
<li>相比VAE, GANs没有变分下界,如果鉴别器训练良好,那么生成器可以完美的学习到训练样本的分布.换句话说,GANs是渐进一致的,但是VAE是有偏差的</li>
<li>GAN应用到一些场景上，比如图片风格迁移，超分辨率，图像补全，去噪，避免了损失函数设计的困难，不管三七二十一，只要有一个的基准，直接上判别器，剩下的就交给对抗训练了</li>
</ul>
<h3 id="GAN缺点"><a href="#GAN缺点" class="headerlink" title="GAN缺点"></a>GAN缺点</h3><h4 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h4><ul>
<li>训练GAN需要达到纳什均衡,有时候可以用梯度下降法做到,有时候做不到.我们还没有找到很好的达到纳什均衡的方法,所以训练GAN相比VAE或者PixelRNN是不稳定的,但我认为在实践中它还是比训练玻尔兹曼机稳定的多</li>
<li>GAN不适合处理离散形式的数据，比如文本</li>
<li>GAN存在训练不稳定、梯度消失、模式崩溃的问题（目前已解决）</li>
</ul>
<h4 id="为什么GAN不适合处理文本数据"><a href="#为什么GAN不适合处理文本数据" class="headerlink" title="为什么GAN不适合处理文本数据"></a>为什么GAN不适合处理文本数据</h4><ul>
<li>文本数据相比较图片数据来说是离散的，因为对于文本来说，通常需要将一个词映射为一个高维的向量，最终预测的输出是一个one-hot向量，假设softmax的输出是（0.2， 0.3， 0.1，0.2，0.15，0.05）那么变为onehot是（0，1，0，0，0，0），如果softmax输出是（0.2， 0.25， 0.2， 0.1，0.15，0.1 ），one-hot仍然是（0， 1， 0， 0， 0， 0），所以对于生成器来说，G输出了不同的结果但是D给出了同样的判别结果，并不能将梯度更新信息很好的传递到G中去，所以D最终输出的判别没有意义。</li>
<li>另外就是GAN的损失函数是JS散度，JS散度不适合衡量不想交分布之间的距离。<br>（WGAN虽然使用wassertein距离代替了JS散度，但是在生成文本上能力还是有限，GAN在生成文本上的应用有seq-GAN,和强化学习结合的产物）</li>
</ul>
<h3 id="GAN的变种"><a href="#GAN的变种" class="headerlink" title="GAN的变种"></a>GAN的变种</h3><p>自从GAN出世后，得到了广泛研究，先后几百篇不同的GANpaper横空出世，国外有大神整理了一个GAN zoo（GAN动物园），链接如下，感兴趣的可以参考一下：<br><a href="https://github.com/hindupuravinash/the-gan-zoo" target="_blank" rel="noopener">GAN zoo</a><br>GitHub上已经1200+star了，顺便附上一张GAN的成果图，可见GAN的研究火热程度：<br><br>由于GAN的变种实在太多，可以学习下DCGAN,, WGAN, improved-WGAN，BEGAN</p>
<h3 id="GAN的广泛应用"><a href="#GAN的广泛应用" class="headerlink" title="GAN的广泛应用"></a>GAN的广泛应用</h3><ul>
<li>GAN本身是一种生成式模型，所以在数据生成上用的是最普遍的，最常见的是图片生成，常用的有DCGAN WGAN，BEGAN，个人感觉在BEGAN的效果最好而且最简单。</li>
<li>GAN本身也是一种无监督学习的典范，因此它在无监督学习，半监督学习领域都有广泛的应用，比较好的论文有<ul>
<li>Improved Techniques for Training GANs</li>
<li>Bayesian GAN（最新）</li>
<li>Good Semi-supervised Learning</li>
</ul>
</li>
<li>不仅在生成领域，GAN在分类领域也占有一席之地，简单来说，就是替换判别器为一个分类器，做多分类任务，而生成器仍然做生成任务，辅助分类器训练。</li>
<li>GAN可以和强化学习结合，目前一个比较好的例子就是seq-GAN</li>
<li>目前比较有意思的应用就是GAN用在图像风格迁移，图像降噪修复，图像超分辨率了，都有比较好的结果，详见pix-2-pix GAN 和cycle GAN。但是GAN目前在视频生成上和预测上还不是很好。</li>
<li>目前也有研究者将GAN用在对抗性攻击上，具体就是训练GAN生成对抗文本，有针对或者无针对的欺骗分类器或者检测系统等等，但是目前没有见到很典范的文章。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/01/18/rl/强化学习基础/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/18/rl/强化学习基础/" itemprop="url">强化学习基础</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-18T06:30:12+08:00">
                2019-01-18
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/强化学习/" itemprop="url" rel="index">
                    <span itemprop="name">强化学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="强化学习基础"><a href="#强化学习基础" class="headerlink" title="强化学习基础"></a>强化学习基础</h3><h4 id="强化学习特点"><a href="#强化学习特点" class="headerlink" title="强化学习特点"></a>强化学习特点</h4><ul>
<li><p>强化学习是机器学习的一个分支：监督学习、无监督学习、强化学习</p>
</li>
<li><p>强化学习的特点：</p>
<ul>
<li>没有监督数据、只有奖励信号</li>
<li>奖励信号不一定是实时的，而很可能是延后的，有时甚至延后很多。</li>
<li>时间（序列）是一个重要因素</li>
<li>当前的行为影响后续接收到的数据</li>
</ul>
</li>
<li><p>强化学习有广泛的应用：像直升机特技飞行、经典游戏、投资管理、发电站控制、让机器人模仿人类行走等</p>
</li>
</ul>
<h3 id="强化学习问题的提出"><a href="#强化学习问题的提出" class="headerlink" title="强化学习问题的提出"></a>强化学习问题的提出</h3><h4 id="奖励Reward"><a href="#奖励Reward" class="headerlink" title="奖励Reward"></a>奖励Reward</h4><ul>
<li><p>是信号的反馈，是一个标量，<font color="blue">它反映个体在t时刻做得怎么样</font>。个体的工作就是最大化累计奖励。</p>
</li>
<li><p>$R_t$强化学习主要基于这样的”奖励假设”：<font color="blue">所有问题解决的目标都可以被描述成最大化累积奖励</font></p>
<img src="/2019/01/18/rl/强化学习基础/resources/FDAE7D078C3300B1B63AAB349135D5EA.jpg">
</li>
</ul>
<h4 id="序列决策-Sequential-Decision-Making"><a href="#序列决策-Sequential-Decision-Making" class="headerlink" title="序列决策 Sequential Decision Making"></a>序列决策 Sequential Decision Making</h4><ul>
<li>目标：<font color="blue">选择一定的行为序列以最大化未来的总体奖励</font></li>
<li>这些行为可能是一个长期的序列</li>
<li>奖励可能而且通常是延迟的</li>
<li>有时候宁愿牺牲即时（短期）的奖励以获取更多的长期奖励</li>
</ul>
<h4 id="个体和环境-Agent-amp-Environment"><a href="#个体和环境-Agent-amp-Environment" class="headerlink" title="个体和环境 Agent &amp; Environment"></a>个体和环境 Agent &amp; Environment</h4><p>  可以从个体和环境两方面来描述强化学习问题。</p>
<ul>
<li><p>在 t 时刻，个体可以：</p>
<ul>
<li>有一个对于环境的观察评估 $O_{t}$ </li>
<li>做出一个行为 $A_{t}$ </li>
<li>从环境得到一个奖励信号 $R_{t+1}$ </li>
</ul>
</li>
<li><p>环境可以：</p>
<ul>
<li>接收个体的动作 $A_{t}$ </li>
<li>更新环境信息，同时使得个体可以得到下一个观测 $O_{t+1}$</li>
<li>给个体一个奖励信号 $R_{t+1} $</li>
</ul>
<img src="/2019/01/18/rl/强化学习基础/resources/A868FA7787B835C43898A76A32B1B469.jpg">
</li>
</ul>
<h4 id="历史和状态-History-amp-State"><a href="#历史和状态-History-amp-State" class="headerlink" title="历史和状态 History &amp; State"></a>历史和状态 History &amp; State</h4><ul>
<li><p>历史</p>
<ul>
<li><font color="blue">历史是观测、行为、奖励的序列</font>：$ H_{t} = O_{1}, R_{1}, A_{1},…, O_{t-1}, R_{t-1}, A_{t-1}, O_{t}, R_{t}, A_{t}$</li>
</ul>
</li>
<li><p>状态</p>
<ul>
<li><font color="blue">状态是所有决定将来的已有的信息</font>，是关于历史的一个函数：$S_{t} = f(H_{t})$</li>
</ul>
</li>
<li><p>环境状态</p>
<ul>
<li>是环境的私有呈现，<font color="blue">包括环境用来决定下一个观测/奖励的所有数据</font>，通常对个体并不完全可见，也就是个体有时候并不知道环境状态的所有细节。即使有时候环境状态对个体可以是完全可见的，这些信息也可能包含着一些无关信息。</li>
</ul>
</li>
<li><p>个体状态</p>
<ul>
<li>是个体的内部呈现，<font color="blue">包括个体可以使用的、决定未来动作的所有信息</font>。个体状态是强化学习算法可以利用的信息，它可以是历史的一个函数： $S^{a}<em>{t} = f(H</em>{t})$</li>
</ul>
</li>
<li><p>信息状态</p>
<ul>
<li>包括历史上所有有用的信息，又称Markov状态</li>
<li>马儿可夫属性 Markov Property<ul>
<li>一个状态St是马尔可夫的，当且仅当：$P[S_{t+1} | S_{t}] = P[S_{t+1} | S_{1}, S_{2},…, S_{t}]$</li>
<li>也就是说，如果信息状态是可知的，那么所有历史信息都可以丢掉，仅需要 t 时刻的信息状态就可以了<ul>
<li>例如：环境状态是Markov的，因为环境状态是环境包含了环境<font color="blue">决定下一个观测/奖励的所有信息</font><img src="/2019/01/18/rl/强化学习基础/resources/3F5DEFC1F69A7F73D61DE50B3B291807.jpg">
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="完全可观测的环境-Fully-Observable-Environments"><a href="#完全可观测的环境-Fully-Observable-Environments" class="headerlink" title="完全可观测的环境 Fully Observable Environments"></a>完全可观测的环境 Fully Observable Environments</h4><ul>
<li>个体能够直接观测到环境状态。在这种条件下:</li>
<li>个体对环境的观测 = 个体状态 = 环境状态</li>
<li>正式地说，这种问题是一个马儿可夫决定过程（Markov Decision Process， MDP）</li>
</ul>
<h4 id="部分可观测的环境-Partially-Observable-Environments"><a href="#部分可观测的环境-Partially-Observable-Environments" class="headerlink" title="部分可观测的环境 Partially Observable Environments"></a>部分可观测的环境 Partially Observable Environments</h4><ul>
<li>个体间接观测环境。举了几个例子：<ul>
<li>一个可拍照的机器人个体对于其周围环境的观测并不能说明其绝度位置，它必须自己去估计自己的绝对位置，而绝对位置则是非常重要的环境状态特征之一；</li>
<li>一个交易员只能看到当前的交易价格；</li>
<li>一个扑克牌玩家只能看到自己的牌和其他已经出过的牌，而不知道整个环境（包括对手的牌）状态。</li>
<li>在这种条件下：<ul>
<li>个体状态 ≠ 环境状态</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="强化学习个体的主要组成部分"><a href="#强化学习个体的主要组成部分" class="headerlink" title="强化学习个体的主要组成部分"></a>强化学习个体的主要组成部分</h3><p>强化学习中的个体可以由以下三个组成部分中的一个或多个组成</p>
<h4 id="策略-Policy"><a href="#策略-Policy" class="headerlink" title="策略 Policy"></a>策略 Policy</h4><ul>
<li>策略是决定个体行为的机制。是<font color="red">从状态到行为的一个映射</font>，可以是确定性的，也可以是不确定性的。</li>
</ul>
<h4 id="价值函数-Value-Function"><a href="#价值函数-Value-Function" class="headerlink" title="价值函数 Value Function"></a>价值函数 Value Function</h4><ul>
<li><font color="red">是一个未来奖励的预测，用来评价当前状态的好坏程度</font><ul>
<li>当面对两个不同的状态时，个体可以用一个Value值来评估这两个状态可能获得的最终奖励区别，继而指导选择不同的行为，即制定不同的策略。</li>
<li><font color="blue">一个价值函数是基于某一个特定策略的</font>，不同的策略下同一状态的价值并不相同。某一策略下的价值函数用下式表示：<img src="/2019/01/18/rl/强化学习基础/resources/1ADEF5845F6145E93F53F33DD65C761D.jpg">
</li>
</ul>
</li>
</ul>
<h4 id="模型-Model"><a href="#模型-Model" class="headerlink" title="模型 Model"></a>模型 Model</h4><p>个体对环境的一个建模，它体现了个体是如何思考环境运行机制的（how the agent think what the environment was.），个体希望模型能模拟环境与个体的交互机制。</p>
<ul>
<li><p>模型至少要解决两个问题：</p>
<ul>
<li><p>一是状态转化概率，即预测下一个可能状态发生的概率：</p>
<img src="/2019/01/18/rl/强化学习基础/resources/A190087077BAA813EE6BBEDDC8685CA2.jpg">
</li>
<li><p>另一项工作是预测可能获得的即时奖励：</p>
<img src="/2019/01/18/rl/强化学习基础/resources/568C86BA012338B66C90B2D68FD23307.jpg">
</li>
</ul>
</li>
<li><p>模型并不是构建一个个体所必需的，很多强化学习算法中个体并不试图（依赖）构建一个模型。</p>
</li>
</ul>
<p>注：模型仅针对个体而言，环境实际运行机制不称为模型，而称为环境动力学(dynamics of environment)，它能够明确确定个体下一个状态和所得的即时奖励</p>
<h3 id="强化学习个体的分类"><a href="#强化学习个体的分类" class="headerlink" title="强化学习个体的分类"></a>强化学习个体的分类</h3><p>解决强化学习问题，个体可以有多种工具组合，<font color="blue">比如通过建立对状态的价值的估计来解决问题，或者通过直接建立对策略的估计来解决问题</font>。这些都是个体可以使用的工具箱里的工具。因此，根据个体内包含的“工具”进行分类，可以把个体分为如下三类：</p>
<ul>
<li><p>仅基于价值函数的 Value Based：在这样的个体中，有对状态的价值估计函数，但是没有直接的策略函数，策略函数由价值函数间接得到。</p>
</li>
<li><p>仅直接基于策略的 Policy Based：这样的个体中行为直接由策略函数产生，个体并不维护一个对各状态价值的估计函数。</p>
</li>
<li><p>演员-评判家形式 Actor-Critic：个体既有价值函数、也有策略函数。两者相互结合解决问题。<br>此外，根据个体在解决强化学习问题时是否建立一个对环境动力学的模型，将其分为两大类：</p>
</li>
</ul>
<p>不基于模型的个体: 这类个体并不视图了解环境如何工作，而仅聚焦于价值和/或策略函数。<br>基于模型的个体：个体尝试建立一个描述环境运作过程的模型，以此来指导价值或策略函数的更新。</p>
<h3 id="学习和规划"><a href="#学习和规划" class="headerlink" title="学习和规划"></a>学习和规划</h3><ul>
<li>学习：环境初始时是未知的，个体不知道环境如何工作，个体通过与环境进行交互，逐渐改善其行为策略。</li>
<li>规划: 环境如何工作对于个体是已知或近似已知的，个体并不与环境发生实际的交互，而是利用其构建的模型进行计算，在此基础上改善其行为策略。</li>
<li>一个常用的强化学习问题解决思路是，<font color="blue">先学习环境如何工作，也就是了解环境工作的方式，即学习得到一个模型，然后利用这个模型进行规划</font></li>
</ul>
<h3 id="预测和控制-Prediction-amp-Control"><a href="#预测和控制-Prediction-amp-Control" class="headerlink" title="预测和控制 Prediction &amp; Control"></a>预测和控制 Prediction &amp; Control</h3><ul>
<li>在强化学习里，我们经常需要先解决关于预测（prediction）的问题，而后在此基础上解决关于控制（Control）的问题。</li>
<li>预测：给定一个策略，评价未来。可以看成是求解在给定策略下的价值函数（value function）的过程。How well will I(an agent) do if I(the agent) follow a specific policy?</li>
<li>控制：找到一个好的策略来最大化未来的奖励。<br>举了一个例子来说明预测和控制的区别</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
	
      	    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/01/16/gan/Conditional GAN/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="雷哥">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/favicon.ico">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="雷哥的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/01/16/gan/Conditional GAN/" itemprop="url">Conditional GAN</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-01-16T07:10:10+08:00">
                2019-01-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/GAN/" itemprop="url" rel="index">
                    <span itemprop="name">GAN</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Conditional-GAN介绍"><a href="#Conditional-GAN介绍" class="headerlink" title="Conditional GAN介绍"></a>Conditional GAN介绍</h3><h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><ul>
<li><p>Traditional supervised approach</p>
<ul>
<li>正面火车和侧面火车都是好的结果，这会导致最后图片很模糊，因为是多张图片的平均<img src="/2019/01/16/gan/Conditional%20GAN/resources/B506E2E85727105810C3FD98DE358329.jpg">
</li>
</ul>
</li>
<li><p>GAN</p>
<ul>
<li>G其实可以很容易忽略D,从而可以无视G的输入(这里是c:train)<ul>
<li>比如如果G发现每次输出猫都能得到高分，那么不管G的输入，只要我我每次都输出是清晰的猫就好了</li>
<li>输出低分基本上有下面三种场景<img src="/2019/01/16/gan/Conditional%20GAN/resources/06288B8EFB718B40410ACFE58F20E136.jpg"></li>
</ul>
</li>
<li>后面的Conditional GAN可以解决这个问题</li>
</ul>
</li>
</ul>
<h4 id="Conditional-GAN"><a href="#Conditional-GAN" class="headerlink" title="Conditional GAN"></a>Conditional GAN</h4><ul>
<li>输入<ul>
<li>G的输出x：图片</li>
<li>G的输入c：train</li>
</ul>
</li>
<li>目标<ul>
<li>x is realistic or not</li>
<li>c ans x are matched or not<img src="/2019/01/16/gan/Conditional%20GAN/resources/71EAA38F9930A85049893063F5F499DD.jpg"></li>
</ul>
</li>
<li>演算法<ul>
<li>核心思想：找到Loss(上面三种情况集合)，然后用梯度提升方法去优化<img src="/2019/01/16/gan/Conditional%20GAN/resources/0FC60DD9F76548EDD7675B99F770ED9E.jpg">
</li>
</ul>
</li>
</ul>
<h4 id="Contitional-GAN-D网络架构"><a href="#Contitional-GAN-D网络架构" class="headerlink" title="Contitional GAN(D网络架构)"></a>Contitional GAN(D网络架构)</h4><ul>
<li>分别将x是否是真实图片和c and x是否match分别输出会比较好一点<img src="/2019/01/16/gan/Conditional%20GAN/resources/3EC8A1DCCF4EDAA9523E2F085EF824F0.jpg">
</li>
</ul>
<h3 id="Stack-GAN"><a href="#Stack-GAN" class="headerlink" title="Stack GAN"></a>Stack GAN</h3><ul>
<li>思想就是先产生小图，然后产生大图<img src="/2019/01/16/gan/Conditional%20GAN/resources/C1400E303DB9A50B6CC45300D159FEB4.jpg">
</li>
</ul>
<h3 id="Image-to-Image"><a href="#Image-to-Image" class="headerlink" title="Image to Image"></a>Image to Image</h3><h4 id="传统方法-Supervised-Learning"><a href="#传统方法-Supervised-Learning" class="headerlink" title="传统方法(Supervised Learning)"></a>传统方法(Supervised Learning)</h4><ul>
<li>也是一样，最后输出会很模糊<ul>
<li>因为一个input对应了多个好图（多张图片），就会把这些图片做平均来输出<img src="/2019/01/16/gan/Conditional%20GAN/resources/9A949394AE46F6D8145151F92CE0611D.jpg">
</li>
</ul>
</li>
</ul>
<h4 id="GAN方法"><a href="#GAN方法" class="headerlink" title="GAN方法"></a>GAN方法</h4><ul>
<li>testing中的close图片，就是supervised learning的结果，比较模糊</li>
<li>GAN比较清晰了，但是会产生一些其他的内容</li>
<li>GAN+close，再加入一些限制条件，就是让G产生的图片更接近真实图片<img src="/2019/01/16/gan/Conditional%20GAN/resources/9EAD3C30D5E435ED7070F5B291B3639A.jpg">
</li>
</ul>
<h4 id="Patch-GAN"><a href="#Patch-GAN" class="headerlink" title="Patch GAN"></a>Patch GAN</h4><ul>
<li>图像太大，容易出现各种问题<img src="/2019/01/16/gan/Conditional%20GAN/resources/3AE30BDB0CD1A217FCFFB482DB274D10.jpg">
</li>
</ul>
<h3 id="GAN用到其他场景"><a href="#GAN用到其他场景" class="headerlink" title="GAN用到其他场景"></a>GAN用到其他场景</h3><h4 id="Speech去杂音"><a href="#Speech去杂音" class="headerlink" title="Speech去杂音"></a>Speech去杂音</h4><ul>
<li>speech生谱图就当做图片，也使用Conditional GAN思想<img src="/2019/01/16/gan/Conditional%20GAN/resources/C11F3C210E44F4A4E9CE5C96C89C7CFF.jpg">
</li>
</ul>
<h4 id="Video"><a href="#Video" class="headerlink" title="Video"></a>Video</h4><ul>
<li>判断是否是正确的连续的影片<ul>
<li>将影片一张张剪辑，形成图片，然后进行训练<img src="/2019/01/16/gan/Conditional%20GAN/resources/E9F52BFDB4677AE62051EA82760CADC3.jpg"></li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


	
    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/images/favicon.ico" alt="雷哥">
          <p class="site-author-name" itemprop="name">雷哥</p>
           
              <p class="site-description motion-element" itemprop="description">不积跬步无以至千里</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">56</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">14</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">12</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/yuancl" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-雷哥"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">雷哥</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Gemini
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
    <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  






  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
